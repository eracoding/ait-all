{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out of Bag Evaluation\n",
    "\n",
    "Each tree only see a subset of the dataset. Any data that a particular tree did not see is called **out of bag** (oob).  We can use them as validation set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random subset of features\n",
    "\n",
    "The $B$ bootstrapped dataset can be correlated.\n",
    "\n",
    "A **random forest** is constructed by bagging + only a random\" subset of $q \\leq n$ features\n",
    "\n",
    "Rule of thumb: $q = \\sqrt{n}$ for classification trees and $q = \\frac{n}{3}$ for regression trees"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                test_size=0.3, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import random, math\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from scipy import stats\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#class\n",
    "class RandomForest:\n",
    "\n",
    "    #init\n",
    "    def __init__(self, B, bootstrap_ratio, with_no_replacement=True):\n",
    "        self.B = B\n",
    "        self.bootstrap_ratio = bootstrap_ratio\n",
    "        self.with_no_replacement = with_no_replacement\n",
    "        self.tree_params = {'max_depth': 2, 'max_features': 'sqrt'} \n",
    "        self.models = [DecisionTreeClassifier(**self.tree_params) for _ in range(B)]\n",
    "    \n",
    "    #fit\n",
    "    def fit(self, X, y):\n",
    "        m, n = X.shape\n",
    "        sample_size = int(self.bootstrap_ratio * len(X))\n",
    "        xsamples = np.zeros((self.B, sample_size, n))\n",
    "        ysamples = np.zeros((self.B, sample_size))\n",
    "        xsamples_oob = []\n",
    "        ysamples_oob = []\n",
    "        \n",
    "        #bootstrapping\n",
    "        for i in range(self.B):\n",
    "            oob_idx = []\n",
    "            idxes   = []\n",
    "            for j in range(sample_size):\n",
    "                idx = random.randrange(m)\n",
    "                if (self.with_no_replacement):\n",
    "                    while idx in idxes:\n",
    "                        idx = random.randrange(m)\n",
    "                idxes.append(idx)\n",
    "                oob_idx.append(idx)\n",
    "                xsamples[i, j, :] = X[idx]\n",
    "                ysamples[i, j] = y[idx]\n",
    "            mask = np.zeros((m), dtype=bool)\n",
    "            mask[oob_idx] = True\n",
    "            xsamples_oob.append(X[~mask])\n",
    "            ysamples_oob.append(y[~mask])\n",
    "        \n",
    "        #fitting\n",
    "        oob_score = 0\n",
    "        print(\"-----Out of bag score for each tree------\")\n",
    "        for i, model in enumerate(self.models):\n",
    "            _X = xsamples[i]\n",
    "            _y = ysamples[i]\n",
    "            model.fit(_X, _y)\n",
    "            \n",
    "            _X_test = np.asarray(xsamples_oob[i])\n",
    "            _y_test = np.asarray(ysamples_oob[i])\n",
    "            yhat = model.predict(_X_test)\n",
    "            oob_score += accuracy_score(_y_test, yhat)\n",
    "            print(f\"Tree {i}\", accuracy_score(_y_test, yhat))\n",
    "        \n",
    "        self.avg_oob_score = oob_score / len(self.models)\n",
    "        print('========Average oob score========')\n",
    "        print(self.avg_oob_score)\n",
    "        \n",
    "            \n",
    "    #predict\n",
    "    def predict(self, X): #<---X_test\n",
    "        predictions = np.zeros((self.B, X.shape[0]))\n",
    "        for i, model in enumerate(self.models):\n",
    "            yhat = model.predict(X)\n",
    "            predictions[i, :] = yhat\n",
    "        return stats.mode(predictions)[0][0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForest(B=5, bootstrap_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Out of bag score for each tree------\n",
      "Tree 0 0.9047619047619048\n",
      "Tree 1 1.0\n",
      "Tree 2 1.0\n",
      "Tree 3 0.9047619047619048\n",
      "Tree 4 0.8571428571428571\n",
      "========Average oob score========\n",
      "0.9333333333333332\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1v/1h7r513n71j7569x87spb4b40000gn/T/ipykernel_70075/1536892778.py:70: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  return stats.mode(predictions)[0][0]\n"
     ]
    }
   ],
   "source": [
    "yhat  = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, yhat))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
