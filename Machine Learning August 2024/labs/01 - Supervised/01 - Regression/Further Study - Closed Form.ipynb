{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Study:  Closed Form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent gives one possible mean for minimizing $J$, which uses iterative approach and may take time.  In the situation where we know that our cost function is strictly concave or convex, we can explicitly take its derivative to zero.  This process of such derivation is called obtaining the **normal equations** or **closed form**. \n",
    "\n",
    "The **closed form** of linear regression can be derived easily.  Let $\\mathbf{X}$ be a matrix of shape $(m, n)$, $\\boldsymbol{\\theta}$ as shape $(n, )$, and $\\mathbf{y}$ as vector of shape $(m, )$.  Instead of writing the cost function as power of square, we shall write it in matrix multiplication as follows:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\boldsymbol{\\theta}} (\\mathbf{X}\\boldsymbol{\\theta} - \\mathbf{y})^T*(\\mathbf{X}\\boldsymbol{\\theta}-\\mathbf{y})$$\n",
    "\n",
    "Recall the following properties:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\mathbf{X}} \\mathbf{X}^T\\mathbf{X}=2\\mathbf{X} \\tag{A}$$\n",
    "$$\\frac{\\partial J}{\\partial \\mathbf{X}} \\mathbf{A}\\mathbf{X}=\\mathbf{A}^T$$\n",
    "$$(\\mathbf{X}\\mathbf{y})^T = \\mathbf{y}^T\\mathbf{X}^T$$\n",
    "\n",
    "Therefore\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial \\boldsymbol{\\theta}} (\\mathbf{X}\\boldsymbol{\\theta} - \\mathbf{y})^T*(\\mathbf{X}\\boldsymbol{\\theta}-\\mathbf{y}) &= \\frac{\\partial J}{\\partial \\boldsymbol{\\theta}} (\\boldsymbol{\\theta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\theta} - \\boldsymbol{\\theta}^T\\mathbf{X}^T\\mathbf{y} - \\mathbf{y}^T\\mathbf{X}\\boldsymbol{\\theta} + \\mathbf{y}^T\\mathbf{y})\\\\\n",
    "&= 2\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\theta} - 2\\mathbf{X}^T\\mathbf{y} \\tag{see note*}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Now, we can set the derivative to 0 to find out the optimal theta\n",
    "\n",
    "$$\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\theta} - \\mathbf{X}^T\\mathbf{y} = 0$$\n",
    "\n",
    "Solving this gives us\n",
    "\n",
    "$$\\boldsymbol{\\theta} =  (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$$\n",
    "\n",
    "\n",
    "Note*: Since $\\mathbf{X}\\boldsymbol{\\theta}$ is a vector, and so is $\\mathbf{y}$, it doesn't matter what the order is, thus we can simply add them to 2.  Also, we got 2 in front of the first part because we have two $\\theta$ (used the property A)\n",
    "\n",
    "\n",
    "**Why not closed form always**.  The answer is simple.  It does not always exists or possible, for example, the cost function is not convex or concave.  But of course, if it exists, we usually prefer closed form given that it is usually faster than gradient descent.  Nevertheless, as you can see, taking inverse of huge number of features can be expensive, thus it is also not always straightforward thing to always prefer closed form.\n",
    "\n",
    "Yes, that's it for most of the theoretical stuff.  Let's start implementing some of these concepts so we can better understand them.\n",
    "\n",
    "The closed form is a normal equations derived from setting the derivatives = 0.  By performing only some inverse operations and matrix multiplication, we will be able to get the theta.\n",
    "\n",
    "$$\\boldsymbol{\\theta} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$$\n",
    "\n",
    "When closed form is available, is doable (can be inversed - can use pseudoinverse), and with not many features (i.e., inverse can be slow), it is recommended to always use closed form.  \n",
    "\n",
    "## Implementation steps:\n",
    "\n",
    "1. Prepare your data\n",
    "    - add intercept\n",
    "    - $\\mathbf{X}$ and $\\mathbf{y}$ and $\\mathbf{w}$ in the right shape\n",
    "        - $\\mathbf{X}$ -> $(m, n)$\n",
    "        - $\\mathbf{y}$ -> $(m, )$\n",
    "        - $\\mathbf{w}$ -> $(n, )$\n",
    "        - where $m$ is number of samples\n",
    "        - where $n$ is number of features\n",
    "    - train-test split\n",
    "    - feature scale\n",
    "    - clean out any missing data\n",
    "    - (optional) feature engineering\n",
    "2. Plug everything into the equation.  Here we shall use X_train to retrieve the $\\boldsymbol{\\theta}$\n",
    "$$\\boldsymbol{\\theta} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$$\n",
    "\n",
    "1. We simply using the $\\boldsymbol{\\theta}$, we can perform a dot product with our X_test which will give us $\\mathbf{\\hat{y}}$.\n",
    "\n",
    "2. We then calculate the errors using mean-squared-error function:\n",
    "\n",
    "$$\\frac{1}{m}\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "Note that it's a bit different from our $J(\\boldsymbol{\\theta})$ because $J(\\boldsymbol{\\theta})$ puts $\\frac{1}{2}$ instead of $\\frac{1}{m}$ for mathematical convenience for derivatives, since we know changing constants do not change the optimization results.\n",
    "\n",
    "\n",
    "Let's implement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "m = X.shape[0]  #number of samples\n",
    "n = X.shape[1]  #number of features\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform(X_test)\n",
    "\n",
    "# actually you can do like this too\n",
    "# X = np.insert(X, 0, 1, axis=1)\n",
    "intercept = np.ones((X_train.shape[0], 1))\n",
    "X_train   = np.concatenate((intercept, X_train), axis=1)\n",
    "intercept = np.ones((X_test.shape[0], 1))\n",
    "X_test    = np.concatenate((intercept, X_test), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fit your algorithm \n",
    "\n",
    "### 1. Define your algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "\n",
    "# order of operation DOES NOT MATTER\n",
    "# But don't flip y before X^T for example\n",
    "def closed_form(X, y):\n",
    "    return inv(X.T @ X) @ X.T @ y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([153.23624595,  -0.51349008, -10.42423616,  25.40910714,\n",
       "        14.53692679, -38.95536033,  26.35053758,   5.38749134,\n",
       "         6.1627566 ,  36.72524689,   1.84754747])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's use the closed_form to find the theta\n",
    "theta = closed_form(X_train, y_train)\n",
    "theta  #<------this is our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compute accuracy/loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the accuracy/loss\n",
    "\n",
    "yhat = X_test @ theta #==> X (m, n+1)  @ (n+1, ) w ==> (m, ) y\n",
    "\n",
    "# if I want to compare yhat and y, I need to make sure they are the same shape\n",
    "assert y_test.shape == yhat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared errors:  2324.9072894171472\n"
     ]
    }
   ],
   "source": [
    "# get the mse\n",
    "mse = ((y_test - yhat)**2).sum() / X_test.shape[0]\n",
    "print(\"Mean squared errors: \", mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 ('teaching_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "becc4c8e5ad229b2591d820334d85e3db0111492344629bf57f272470dce75a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
