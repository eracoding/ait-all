{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "Reinforcement Learning is a subfield of artificial intelligence (AI) where machines learn by experimenting, somewhat like students learning through trial and error. They aim to make the best decisions to maximize rewards in various scenarios. RL is used in applications like robotics, gaming, and autonomous systems.\n",
    "\n",
    "## Proximal Policy Optimization\n",
    "\n",
    "One of the priminent algorithms is **Proximal Policy Optimization (PPO)**, released by OpenAI in 2017, cited more than 10k times.  Instead of making big changes all at once, PPO encourages gradual and more stable improvements, which is crucial for the learning process. PPO finds application in various real-world situations, and we'll explore its principles and significance in more detail.\n",
    "\n",
    "## Terminologies\n",
    "\n",
    "Here are some quick terminologies before we start:\n",
    "\n",
    "- **Agents** - the primary entity interacting with the environment\n",
    "- **Environment** - the primary thing that the agents interact with\n",
    "- **Policy ($\\pi$)**: a policy defines the probability of an action given a state.  The policy guides the agent in selecting actions that maximize its expected cumulative reward over time.\n",
    "- **States / observation** ($s_t$) - a snapshot of the environment at timestep $t$\n",
    "- **Action** ($a_t$) - the possible actions that the agents can perform to the environment at timestep $t$\n",
    "- **Rewards / returns** ($r_t$) - the rewards received each time the agents perform an action at timestep $t$ given state $s$\n",
    "\n",
    "Let's look at a very simple code to understand all these terms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the environment\n",
    "import gymnasium as gym\n",
    "\n",
    "#environment is the simulation / real-world environment\n",
    "env = gym.make('Pendulum-v1', render_mode='human')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple picture of this pendulum thing.\n",
    "\n",
    "<img src = \"figures/pendulum.gif\" height=200>\n",
    "\n",
    "The task is to move left and right such that it goes to the 12 o'clock and stay there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.39593688 -0.91827774 -0.27370557]\n"
     ]
    }
   ],
   "source": [
    "#reset the environment and get  state0\n",
    "state, _ = env.reset()  #cos, sin, velocity\n",
    "done = False\n",
    "\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.326761]\n"
     ]
    }
   ],
   "source": [
    "action = env.action_space.sample()\n",
    "\n",
    "print(action)  #action is swing left or right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.43069062 -0.9024996  -0.7633997 ]\n",
      "-3.921277700930215\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "next_state, reward, done, truncated, info = env.step(action)          \n",
    "print(next_state)\n",
    "print(reward)  #how far it is from the upright position\n",
    "print(done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close() #env.close() won't close the window; just restart the kernel and it will close the window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "## 1. Vanilla Policy Gradient Methods\n",
    "\n",
    "Vanilla policy gradient methods have an objective function to maximize as:\n",
    "\n",
    "$$\\mathbb{E} \\underbrace{(\\log \\pi_\\theta(a_t | s_t)}_{\\text{log prob.}} \\underbrace{\\hat{A}_t}_{\\text{advantage func.}})$$\n",
    "\n",
    "$\\mathbb{E}$ stands for expected value but is simply short hand for empirical average.\n",
    "\n",
    "### 1.1 Advantage function ($\\hat{A}_t$)\n",
    "\n",
    "The right side of the equation - the Advantage $\\hat{A}_t$ function is simply calculating the *advantage* of taking a certain action, compared to the average performance.  If $\\hat{A}$ is positive, it means the action taken is generally better than average, while negative means vice versa.  If it is zero, it means that the action taken is no better than average.\n",
    "\n",
    "The equation is\n",
    "\n",
    "$$\\hat{A}_t = G_t - V(s)$$\n",
    "\n",
    "#### 1.1.1 Discounted sum of rewards $(G_t)$\n",
    "\n",
    "$G_t$ is the discounted sum of rewards at timestep $t$ until the end.\n",
    "\n",
    "The formula is\n",
    "\n",
    "   $$G_t = \\gamma^0 r_{t} + \\gamma^1 r_{t+1} + \\gamma^2 r_{t+2} + \\ldots + \\gamma^{T-t} r_T = \\sum_{k=t}^{T} \\gamma^{k-t} \\cdot r_{k}$$\n",
    "\n",
    "Note that to calculate $G_t$, you have to run many episodes, and then reverse back to calculate $G_t$.\n",
    "\n",
    "The reason to calculate future rewards as well is so that the action taken also accounts for possible future rewards it can get.   \n",
    "\n",
    "The **Discount Factor** ($\\gamma$) is a parameter that determines the importance of future rewards. A value of 0 indicates that only immediate rewards matter, while a value of 1 considers all future rewards equally.  Putting discount factor is a way to constrain the farther the reward in the future, it may be less important.\n",
    "\n",
    "#### 1.1.2 Value function $(V(s))$\n",
    "\n",
    "As for the second part of the advntage function, $V(s)$ is the **state value function** or the **value function** that calculates the average discounted sum of rewards whenever the agent start in this state until the end.  It is commonly calculated using the **critic network** (or sometimes the value network).\n",
    "\n",
    "#### Critic network\n",
    "\n",
    "It is a neural network that takes in states $(s)$ and output its corresponding value $(V(s))$.  The way to train this network is simply:\n",
    "\n",
    "1. Perform rollout and collect a sequences of states, actions, and rewards\n",
    "2. Once you finish many episodes, \n",
    "   1. Go back to each time step, compute $G_t$ for each timestep\n",
    "3. Minimize $\\mathbb{E}(G_t - V(s))^2$\n",
    "\n",
    "In short, critic network learns to estimate $(V(s))$ from many instances, thus it is like a **average** estimates of $(V(s))$\n",
    "\n",
    "### 1.2 Log probablity ($\\log \\pi_\\theta(a_t | s_t)$)\n",
    "\n",
    "The left side $\\log \\pi_\\theta(a_t | s_t)$ returns the log probability of actions given the current state.  Common way to get is through deep neural network.  This network is commonly called the **actor network**.   It takes in states and output the probability distributions of all actions.\n",
    "\n",
    "#### Actor network\n",
    "\n",
    "The way to train this **actor network** is simply:\n",
    "\n",
    "1. Perform rollout and collect a sequences of states, actions, and rewards\n",
    "2. Once you finish many episodes, \n",
    "   1. Go back to each time step, compute $G_t$ for each timestep \n",
    "3. Maximize $\\mathbb{E} (\\log \\pi_\\theta(a_t | s_t) \\hat{A}_t)$\n",
    "\n",
    "### In summary\n",
    "\n",
    "In summary, it's basically maximizing the probability of actions, multiplied by its relative rewards compared to average.\n",
    "\n",
    "$$\\mathbb{E} \\underbrace{(\\log \\pi_\\theta(a_t | s_t)}_{\\text{prob. of actions}} \\underbrace{\\hat{A}_t}_{\\text{relative rewards}})$$\n",
    "\n",
    "## 2. Trust Region Methods\n",
    "\n",
    "The vanilla policy gradient methods often suffer from too large policy updates, causing it fail to find the solution due to the large solution space. \n",
    "\n",
    "**Trust Region Policy Gradient (TRPO)** modifies the objective function to maximize to\n",
    "\n",
    "$$\\mathbb{E}(\\frac{\\log \\pi_\\theta(a_t | s_t)}{\\log \\pi_{\\theta_{\\text{old}}}(a_t | s_t)} \\hat{A}_t) - \\beta \\text{KL}(\\pi_{\\theta_{\\text{old}}}( \\cdot | s_t), \\pi_{\\theta}( \\cdot | s_t))$$\n",
    "\n",
    "The first term calculates the **ratio** instead of the probability, which would help the model to find meaningful updates.  Larger difference, the better.  However, too large updates can cause a lot of problem, thus we constrain the difference between old policy and new policy using KL divergence.\n",
    "\n",
    "Note: For those who don't know what is KL divergence, you can easily search up the equation, but it is a very simple equation of $P \\log \\frac{P}{Q}$ which simply measures how different is two distributions $P$ and $Q$.  Higher value means more difference.\n",
    "\n",
    "## 3. Proximal Policy Methods\n",
    "\n",
    "TRPO still suffers from choosing the right $\\beta$ which varies from task to task.  \n",
    "\n",
    "**Proximal Policy Policy Gradient (PPO)** modifies the objective function to maximize to\n",
    "\n",
    "$$\\mathbb{E}( \n",
    "   \\min(\n",
    "   \\frac{\\log \\pi_\\theta(a_t | s_t)}{\\log \\pi_{\\theta_{\\text{old}}}(a_t | s_t)} \\hat{A}_t , \n",
    "   \\text{clip}(\\frac{\\log \\pi_\\theta(a_t | s_t)}{\\log \\pi_{\\theta_{\\text{old}}}(a_t | s_t)}, 1 - \\epsilon, 1 + \\epsilon)\\hat{A}_t)           ) $$\n",
    "\n",
    "Although this looks very difficult, it simply bounds the updates within $\\epsilon$ which is specified commonly as 0.2. \n",
    "\n",
    "Let's look at the effect closely via this picture:\n",
    "\n",
    "<img src = \"figures/clip.png\" height=\"300\">\n",
    "\n",
    "Here, the x-axis is the policy ratio, and the y-axis is simply the clipped objective function we just defined.  \n",
    "When $A > 0$, it restricts the rewards to $1 + \\epsilon$.  When $A < 0$, it restricts the rewards to $1 - \\epsilon$.\n",
    "\n",
    "### 3.1 Actor loss function\n",
    "\n",
    "Based on what we learn, the **actor** loss function of PPO is simply\n",
    "\n",
    "$$J(\\theta) = -\\min(\n",
    "   \\frac{\\log \\pi_\\theta(a_t | s_t)}{\\log \\pi_{\\theta_{\\text{old}}}(a_t | s_t)} \\hat{A}_t , \n",
    "   \\text{clip}(\\frac{\\log \\pi_\\theta(a_t | s_t)}{\\log \\pi_{\\theta_{\\text{old}}}(a_t | s_t)}, 1 - \\epsilon, 1 + \\epsilon)\\hat{A}_t) $$\n",
    "\n",
    "To encourage exploration, it adds the **entropy** term\n",
    "\n",
    "$$J(\\theta) = -\\min(\n",
    "   \\frac{\\log \\pi_\\theta(a_t | s_t)}{\\log \\pi_{\\theta_{\\text{old}}}(a_t | s_t)} \\hat{A}_t , \n",
    "   \\text{clip}(\\frac{\\log \\pi_\\theta(a_t | s_t)}{\\log \\pi_{\\theta_{\\text{old}}}(a_t | s_t)}, 1 - \\epsilon, 1 + \\epsilon)\\hat{A}_t) - \\lambda S[\\pi_\\theta](s_t)$$\n",
    "\n",
    "Here, we want *high* entropy, thus we put minus in front for a minimization problem.  $\\lambda$ is simply a coefficient to control this entropy bonus.\n",
    "\n",
    "#### Entropy\n",
    "\n",
    "To understand how entropy helps exploration, a simple example of **entropy** is:\n",
    "\n",
    "Suppose you have three actions, A1, A2, and A3. The probabilities to each action as follows:\n",
    "\n",
    "- Probability of selecting A1: $P(A1) = 0.9$\n",
    "- Probability of selecting A2: $P(A2) = 0.05$\n",
    "- Probability of selecting A3: $P(A3) = 0.05$\n",
    "\n",
    "The entropy of this policy can be calculated as:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "S &= -\\sum_{i=1}^{3} P(A_i) \\ln(P(A_i)) \\\\\n",
    "S &= -(0.9 \\ln(0.9) + 0.05 \\ln(0.05) + 0.05 \\ln(0.05)) \\\\\n",
    "S &\\approx 0.394\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Let's change the probabilities to \n",
    "\n",
    "- Probability of selecting A1: $P(A1) = 0.4$\n",
    "- Probability of selecting A2: $P(A2) = 0.3$\n",
    "- Probability of selecting A3: $P(A3) = 0.3$\n",
    "\n",
    "The entropy $S$ will be 1.08.\n",
    "\n",
    "Thus higher entropy encourages the model to explore more actions, instead of just making one action very prominent.  However, it is important to note that we should balance exploration and exploitation by putting a coefficient in front of this entropy to control how much we want to explore.\n",
    "\n",
    "### 3.2 Critic loss function\n",
    "\n",
    "The critic loss function is simply\n",
    "\n",
    "$$J(\\theta) = (G_t - V(s))^2$$\n",
    "\n",
    "That's it!  Now let's look at the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms and code\n",
    "\n",
    "1. Initialize actor and critic network\n",
    "2. Collect data\n",
    "   1. Initialize environment and its states\n",
    "   2. Let the agent interact with the environment\n",
    "      1. Store the state, rewards, actions, next states, and log probability of the action into a list\n",
    "   3. Once you finish collecting these states, rewards, etc.\n",
    "      1. Reverse the time and compute $G_t$\n",
    "3. Calculate probability\n",
    "   1. $\\displaystyle\\frac{\\pi}{\\pi_\\text{old}} \\hat{A}_t$\n",
    "   2. $\\text{clip}(\\displaystyle\\frac{\\pi}{\\pi_\\text{old}}, 1 - \\epsilon, 1 + \\epsilon) \\hat{A}_t$\n",
    "4. Calculate loss\n",
    "   1. Actor loss = - minimum of the 3.1 and 3.2 - entropy bonus\n",
    "   2. Critic loss = $(G_t - V(s))^2$\n",
    "5. Backpropagate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backbone neural network for actor and critic networks\n",
    "\n",
    "Note the actor and critic networks can be feedforward, RNN or CNN depending on the states.  Here I think using a feedforward is good for a beginner lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(in_dim, 64)\n",
    "        self.layer2 = nn.Linear(64, 64)\n",
    "        self.layer3 = nn.Linear(64, out_dim)\n",
    "        \n",
    "    def forward(self, states):\n",
    "        if isinstance(states, np.ndarray):\n",
    "            states = torch.tensor(states, dtype = torch.float)\n",
    "                    \n",
    "        activation1 = F.relu(self.layer1(states))\n",
    "        activation2 = F.relu(self.layer2(activation1))\n",
    "        out         = self.layer3(activation2)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO class\n",
    "\n",
    "Here, we gonna define several functions:\n",
    "- `collect_data` - collect data and put into a list\n",
    "- `fit` - called `collect_data`, and then compute `G_t` and then learn using PPO loss function\n",
    "- `get_action` - asked the `actor network` to give you the action and its prob given the states\n",
    "- `compute_discounted_rewards` - compute `G_t`\n",
    "- `predict` - ask the `actor network` and `critic network` to give you the action, prob and `V(s)`, and entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.distributions import MultivariateNormal\n",
    "from torch.optim import Adam\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, env):\n",
    "        self._init_params()\n",
    "        \n",
    "        #extract info from environment\n",
    "        self.env = env\n",
    "        self.states_dim = env.observation_space.shape[0]\n",
    "        self.act_dim    = env.action_space.shape[0]\n",
    "        \n",
    "        ## STEP 1\n",
    "        #input is state for both actor and critic networks\n",
    "        #output is a value for critic networks, and action distribution for actor networks \n",
    "        self.actor  = FeedForwardNN(self.states_dim, self.act_dim) \n",
    "        self.critic = FeedForwardNN(self.states_dim, 1)\n",
    "        \n",
    "        ##this is for sampling actions when collecting data\n",
    "        self.cov_var = torch.full(size = (self.act_dim, ), fill_value=0.5)\n",
    "        self.cov_mat = torch.diag(self.cov_var)  #basically every action has a variance of 0.5\n",
    "        \n",
    "        self.actor_optim = Adam(self.actor.parameters(), lr=self.lr)\n",
    "        self.critic_optim = Adam(self.critic.parameters(), lr=self.lr)\n",
    "    \n",
    "    def _init_params(self):\n",
    "        torch.manual_seed(999)  #just for reproducibility\n",
    "        self.timesteps_per_batch = 4800\n",
    "        self.max_timesteps_per_episode = 1600\n",
    "        self.gamma = 0.95\n",
    "        self.n_updates_per_iteration = 5\n",
    "        self.clip = 0.2\n",
    "        self.lr = 0.005\n",
    "        self.entropy_weight = 0.05 #higher means more exploration; we can set it very low for pendulum because it's a very simple problem\n",
    "    \n",
    "    ## STEP 2\n",
    "    def collect_data(self):\n",
    "        #rollout\n",
    "        batch_states    = [] #shape: (number of timesteps per batch, states_dim)\n",
    "        batch_acts      = [] #shape: (number of timesteps per batch, act_dim)\n",
    "        batch_log_probs = [] #(number of timesteps per batch, )\n",
    "        batch_rewards   = [] #(number of episodes, number of timesteps per episode)\n",
    "        batch_discounted_rewards = [] #(number of timesteps per batch, )\n",
    "        batch_lens      = [] #(number of episodes, )\n",
    "        \n",
    "        #Number of timesteps run so far this batch\n",
    "        t = 0\n",
    "        ep_rewards = []\n",
    "        \n",
    "        #batch means one batch of data we collect, which can span multiple episodes\n",
    "        #one episode means you start the env, until you reach the terminal state\n",
    "        \n",
    "        while t < self.timesteps_per_batch:  #30\n",
    "            \n",
    "            #Rewards this episode\n",
    "            ep_rewards = []\n",
    "            \n",
    "            states = self.env.reset()[0]  ## STEP 2.1\n",
    "            done   = False\n",
    "            \n",
    "            ## STEP 2.2\n",
    "            for ep_t in range(self.max_timesteps_per_episode):\n",
    "                t += 1\n",
    "                \n",
    "                #collect states\n",
    "                batch_states.append(states)\n",
    "                \n",
    "                action, log_prob = self.get_action(states)    \n",
    "                states, rewards, done, _, _ = self.env.step(action)\n",
    "                \n",
    "                #collect reward, action, and log prob\n",
    "                ep_rewards.append(rewards)                \n",
    "                batch_acts.append(action)\n",
    "                batch_log_probs.append(log_prob)\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                \n",
    "            batch_lens.append(ep_t + 1)           \n",
    "            batch_rewards.append(ep_rewards)\n",
    "        \n",
    "        #convert to tensor; note that converting the list first to np array then to tensor is much faster\n",
    "        batch_states    = torch.tensor(np.array(batch_states), dtype=torch.float)\n",
    "        batch_acts      = torch.tensor(np.array(batch_acts), dtype=torch.float)\n",
    "        batch_log_probs = torch.tensor(np.array(batch_log_probs), dtype=torch.float)\n",
    "\n",
    "        ## STEP 2.3\n",
    "        #compute G_t\n",
    "        batch_discounted_rewards = self.compute_discounted_rewards(batch_rewards)\n",
    "        \n",
    "        return batch_states, batch_acts, batch_log_probs, batch_discounted_rewards, batch_lens\n",
    "                \n",
    "    def fit(self, total_timesteps):\n",
    "        t = 0 # Timesteps simulated until now\n",
    "        i = 0\n",
    "        actor_losses  = [] #for reporting\n",
    "        critic_losses = []\n",
    "        discounted_rewards = []\n",
    "        \n",
    "        while t < total_timesteps:\n",
    "                        \n",
    "            batch_states, batch_acts, batch_log_probs, batch_discounted_rewards, batch_lens = self.collect_data()\n",
    "                        \n",
    "            t += np.sum(batch_lens)\n",
    "            i += 1\n",
    "                    \n",
    "            # Calculate V\n",
    "            V, _ , _ = self.predict(batch_states, batch_acts)\n",
    "\n",
    "            # Calculate advantage\n",
    "            A_k = batch_discounted_rewards - V.detach()\n",
    "            \n",
    "            # For faster convergence\n",
    "            A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-10)\n",
    "            \n",
    "            for _ in range(self.n_updates_per_iteration):\n",
    "                V, curr_log_probs, entropy = self.predict(batch_states, batch_acts)\n",
    "                ratios = torch.exp(curr_log_probs - batch_log_probs) #log ratio become minus\n",
    "                \n",
    "                # Calculate surrogate losses\n",
    "                surr1 = ratios * A_k\n",
    "                surr2 = torch.clamp(ratios, 1 - self.clip, 1 + self.clip) * A_k\n",
    "                actor_loss = (-torch.min(surr1, surr2)).mean()\n",
    "                entropy_loss = entropy.mean()\n",
    "                actor_loss = actor_loss - self.entropy_weight * entropy_loss\n",
    "                critic_loss = nn.MSELoss()(batch_discounted_rewards, V)\n",
    "                \n",
    "                actor_losses.append(actor_loss.detach())\n",
    "                critic_losses.append(critic_loss.detach())\n",
    "                \n",
    "                # Backprop\n",
    "                self.actor_optim.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                self.actor_optim.step()\n",
    "                \n",
    "                self.critic_optim.zero_grad()    \n",
    "                critic_loss.backward()    \n",
    "                self.critic_optim.step()\n",
    "                \n",
    "                #just for plotting\n",
    "                discounted_rewards.append(batch_discounted_rewards.mean())\n",
    "                \n",
    "            self.print_summary(i, t, discounted_rewards, critic_losses, actor_losses)\n",
    "                \n",
    "    def get_action(self, states):\n",
    "        mean = self.actor(states)\n",
    "        dist = MultivariateNormal(mean, self.cov_mat)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        \n",
    "        #detach from computational graph\n",
    "        return action.detach().numpy(), log_prob.detach()\n",
    "    \n",
    "    def compute_discounted_rewards(self, batch_rewards):\n",
    "        # batch_rewards: shape (number of episodes, number of timesteps per episode)\n",
    "        batch_discounted_rewards = []  #shape: (num of timesteps in batch)\n",
    "                        \n",
    "        # Iterate through each episode backwards to maintain same order in batch_discounted_rewards\n",
    "        for episode_reward in reversed(batch_rewards):\n",
    "        \n",
    "            discounted_reward = 0\n",
    "            for reward in reversed(episode_reward):\n",
    "                discounted_reward = reward + discounted_reward * self.gamma\n",
    "                batch_discounted_rewards.insert(0, discounted_reward)\n",
    "                \n",
    "        batch_discounted_rewards = torch.tensor(batch_discounted_rewards, dtype=torch.float)\n",
    "        \n",
    "        return batch_discounted_rewards\n",
    "    \n",
    "    def predict(self, batch_states, batch_acts):\n",
    "        # Query critic network for a value V for each state in batch_states.\n",
    "        V = self.critic(batch_states).squeeze()\n",
    "        \n",
    "        mean = self.actor(batch_states)\n",
    "        dist = MultivariateNormal(mean, self.cov_mat)\n",
    "                \n",
    "        log_probs = dist.log_prob(batch_acts)\n",
    "        \n",
    "        return V, log_probs, dist.entropy()\n",
    "    \n",
    "    def print_summary(self, i, t, discounted_rewards, critic_losses, actor_losses):\n",
    "        avg_discounted_rewards  = np.mean([rewards.float().mean() for rewards in discounted_rewards])\n",
    "        avg_actor_loss  = np.mean([losses.float().mean() for losses in actor_losses])\n",
    "        avg_critic_loss = np.mean([losses.float().mean() for losses in critic_losses])\n",
    "        \n",
    "        if(i+1) % 10 == 0:\n",
    "            print(f\"#{i+1:3.0f} | Timesteps: {t:7.0f} |  Critic Loss: {avg_critic_loss:10.3f} | Actor Loss: {avg_actor_loss:10.6f} | Dis. Rewards: {avg_discounted_rewards:5.3f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 10 | Timesteps:   43200 |  Critic Loss:   6536.713 | Actor Loss:  -0.049997 | Dis. Rewards: -114.061\n",
      "# 20 | Timesteps:   91200 |  Critic Loss:   3647.361 | Actor Loss:  -0.052533 | Dis. Rewards: -102.112\n",
      "# 30 | Timesteps:  139200 |  Critic Loss:   2615.570 | Actor Loss:  -0.053370 | Dis. Rewards: -93.388\n",
      "# 40 | Timesteps:  187200 |  Critic Loss:   2072.689 | Actor Loss:  -0.054115 | Dis. Rewards: -83.478\n",
      "# 50 | Timesteps:  235200 |  Critic Loss:   1822.857 | Actor Loss:  -0.053837 | Dis. Rewards: -73.711\n",
      "# 60 | Timesteps:  283200 |  Critic Loss:   1557.916 | Actor Loss:  -0.054006 | Dis. Rewards: -63.065\n",
      "# 70 | Timesteps:  331200 |  Critic Loss:   1378.605 | Actor Loss:  -0.054071 | Dis. Rewards: -54.818\n",
      "# 80 | Timesteps:  379200 |  Critic Loss:   1219.411 | Actor Loss:  -0.054183 | Dis. Rewards: -48.356\n",
      "# 90 | Timesteps:  427200 |  Critic Loss:   1092.476 | Actor Loss:  -0.054234 | Dis. Rewards: -43.259\n",
      "#100 | Timesteps:  475200 |  Critic Loss:    991.444 | Actor Loss:  -0.054301 | Dis. Rewards: -39.277\n"
     ]
    }
   ],
   "source": [
    "#pip install gymnasium\n",
    "#brew install swig\n",
    "#pip install box2d-py\n",
    "\n",
    "import gymnasium as gym\n",
    "import pickle\n",
    "\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "\n",
    "model = PPO(env)\n",
    "model.fit(500000)\n",
    "\n",
    "filename = 'model/pendulumv1'\n",
    "with open(f'{filename}.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    1 | Angle:  0.351 | Reward: -0.14191\n",
      "Iteration    2 | Angle:  0.374 | Reward: -0.15256\n",
      "Iteration    3 | Angle:  0.396 | Reward: -0.16520\n",
      "Iteration    4 | Angle:  0.417 | Reward: -0.17961\n",
      "Iteration    5 | Angle:  0.438 | Reward: -0.19589\n",
      "Iteration    6 | Angle:  0.461 | Reward: -0.21448\n",
      "Iteration    7 | Angle:  0.485 | Reward: -0.23611\n",
      "Iteration    8 | Angle:  0.511 | Reward: -0.26184\n",
      "Iteration    9 | Angle:  0.541 | Reward: -0.29313\n",
      "Iteration   10 | Angle:  0.575 | Reward: -0.33194\n",
      "Iteration   11 | Angle:  0.614 | Reward: -0.38094\n",
      "Iteration   12 | Angle:  0.660 | Reward: -0.44372\n",
      "Iteration   13 | Angle:  0.715 | Reward: -0.52510\n",
      "Iteration   14 | Angle:  0.778 | Reward: -0.63152\n",
      "Iteration   15 | Angle:  0.853 | Reward: -0.77161\n",
      "Iteration   16 | Angle:  0.941 | Reward: -0.95671\n",
      "Iteration   17 | Angle:  1.045 | Reward: -1.20153\n",
      "Iteration   18 | Angle:  1.168 | Reward: -1.52367\n",
      "Iteration   19 | Angle:  1.310 | Reward: -1.97250\n",
      "Iteration   20 | Angle:  1.483 | Reward: -2.52903\n",
      "Iteration   21 | Angle:  1.692 | Reward: -3.39709\n",
      "Iteration   22 | Angle:  1.931 | Reward: -4.61625\n",
      "Iteration   23 | Angle:  2.206 | Reward: -6.00968\n",
      "Iteration   24 | Angle:  2.514 | Reward: -7.89187\n",
      "Iteration   25 | Angle:  2.845 | Reward: -10.09926\n",
      "Iteration   26 | Angle: -3.083 | Reward: -12.49588\n",
      "Iteration   27 | Angle: -2.721 | Reward: -14.54299\n",
      "Iteration   28 | Angle: -2.368 | Reward: -12.65589\n",
      "Iteration   29 | Angle: -2.028 | Reward: -10.58337\n",
      "Iteration   30 | Angle: -1.726 | Reward: -8.73594\n",
      "Iteration   31 | Angle: -1.448 | Reward: -6.63187\n",
      "Iteration   32 | Angle: -1.215 | Reward: -5.20141\n",
      "Iteration   33 | Angle: -1.027 | Reward: -3.64223\n",
      "Iteration   34 | Angle: -0.877 | Reward: -2.47449\n",
      "Iteration   35 | Angle: -0.763 | Reward: -1.66899\n",
      "Iteration   36 | Angle: -0.679 | Reward: -1.09829\n",
      "Iteration   37 | Angle: -0.618 | Reward: -0.74511\n",
      "Iteration   38 | Angle: -0.563 | Reward: -0.53590\n",
      "Iteration   39 | Angle: -0.517 | Reward: -0.43878\n",
      "Iteration   40 | Angle: -0.474 | Reward: -0.35713\n",
      "Iteration   41 | Angle: -0.438 | Reward: -0.29980\n",
      "Iteration   42 | Angle: -0.403 | Reward: -0.24686\n",
      "Iteration   43 | Angle: -0.368 | Reward: -0.21504\n",
      "Iteration   44 | Angle: -0.344 | Reward: -0.18490\n",
      "Iteration   45 | Angle: -0.318 | Reward: -0.14575\n",
      "Iteration   46 | Angle: -0.294 | Reward: -0.13058\n",
      "Iteration   47 | Angle: -0.273 | Reward: -0.11052\n",
      "Iteration   48 | Angle: -0.249 | Reward: -0.09453\n",
      "Iteration   49 | Angle: -0.233 | Reward: -0.08498\n",
      "Iteration   50 | Angle: -0.215 | Reward: -0.06680\n",
      "Iteration   51 | Angle: -0.194 | Reward: -0.06101\n",
      "Iteration   52 | Angle: -0.169 | Reward: -0.05727\n",
      "Iteration   53 | Angle: -0.143 | Reward: -0.05520\n",
      "Iteration   54 | Angle: -0.131 | Reward: -0.04841\n",
      "Iteration   55 | Angle: -0.120 | Reward: -0.02327\n",
      "Iteration   56 | Angle: -0.108 | Reward: -0.01987\n",
      "Iteration   57 | Angle: -0.102 | Reward: -0.01834\n",
      "Iteration   58 | Angle: -0.095 | Reward: -0.01220\n",
      "Iteration   59 | Angle: -0.083 | Reward: -0.01213\n",
      "Iteration   60 | Angle: -0.078 | Reward: -0.01234\n",
      "Iteration   61 | Angle: -0.064 | Reward: -0.00947\n",
      "Iteration   62 | Angle: -0.054 | Reward: -0.01162\n",
      "Iteration   63 | Angle: -0.051 | Reward: -0.00802\n",
      "Iteration   64 | Angle: -0.035 | Reward: -0.00689\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import pickle\n",
    "\n",
    "filename = 'model/pendulumv1'\n",
    "\n",
    "with open(f'{filename}.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "\n",
    "env = gym.make('Pendulum-v1', render_mode='human')\n",
    "num_episodes = 1\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    i = 0\n",
    "    while not done:\n",
    "        env.render()\n",
    "\n",
    "        action, log_probabilities = model.get_action(state)\n",
    "        next_state, reward, done, truncated, info = env.step(action)    \n",
    "        \n",
    "        angle = np.arctan2(next_state[1], next_state[0])\n",
    "        angle_threshold = 0.05\n",
    "        if abs(angle) < angle_threshold:\n",
    "            done = True\n",
    "                       \n",
    "        state = next_state\n",
    "        \n",
    "        i += 1\n",
    "        #the more negative the rewards, the farther it is from the upright position.\n",
    "        print(f\"Iteration {i: 4.0f} | Angle: {angle:6.3f} | Reward: {reward:3.5f}\")\n",
    "\n",
    "env.close() #env.close() won't close the window; just restart the kernel and it will close the window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workshop\n",
    "\n",
    "1. What is Reinforcement Learning?\n",
    "2. Be familiar what is agents, environment, policy, states, action, and rewards.\n",
    "3. What is $G_t$?  How do we obtain it? \n",
    "4. How is the critic network relates to $G_t$?\n",
    "5. In $G_t$, there is some discount factor $\\gamma$, what it it?  What is the range?  0 means what? and 1 means what?\n",
    "6. What is $V(s)$?  How do we obtain it?\n",
    "7. So the advantage function $\\hat{A}_t = G_t - V(s)$.  But what it really means?\n",
    "8. How do we obtain the log probability of actions given the current state?  How it relates to the actor network? What is the size of this?\n",
    "9.  In summary, what does $$\\mathbb{E} \\underbrace{(\\log \\pi_\\theta(a_t | s_t)}_{\\text{log prob.}} \\underbrace{\\hat{A}_t}_{\\text{advantage func.}})$$ mean?\n",
    "10. In Trust Region Methods, we introduce (1) proability ratio, and (2) KL divergence.  What is it for?\n",
    "11. But what remains the limitation of the Trust Region Methods?\n",
    "12. In Proximal Policy Methods, it adds some clip function, how does that fix the Trust Region Methods limitation?\n",
    "13. In PPO, there is a entropy term.  What is it?  Bigger lambda in front of the entropy means what?  How about smaller term?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
