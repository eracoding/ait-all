{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cat Dogs classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Basic image processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic image opening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image #!pip install pillow\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Image.open('../case_studies/data/cat_dog/train/cat/1.jpg') as my_cat:\n",
    "    # display(my_cat)\n",
    "    nparray_cat = np.array(my_cat)\n",
    "    print(nparray_cat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to loop through all the files in folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../case_studies/data/cat_dog/'\n",
    "img_names = []  #get all the image names --> os.walk\n",
    "\n",
    "for folder, subfolders, filenames in os.walk(path):\n",
    "    # print(f\"{folder} | {subfolders} | {filenames}\")\n",
    "    #write whatever filtering you want - to get the file you want\n",
    "    for name in filenames:\n",
    "        img_names.append(folder + '/' + name)\n",
    "        \n",
    "len(img_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation\n",
    "\n",
    "- In deep learning for computer vision, we often transform our image as either (1) preprocessing step, or for (2) data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog = Image.open('../case_studies/data/cat_dog/train/dog/14.jpg')\n",
    "# dog.size  #h = 500, w = 386\n",
    "# display(dog)\n",
    "# dog.getpixel??\n",
    "# r, g, b = dog.getpixel((0, 0))\n",
    "# print(r, g, b)  #this will be useful for filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#most basic transformation, which is making 0-255 to 0-1\n",
    "transform = transforms.Compose([\n",
    "    # transforms.RandomRotation(30),\n",
    "    # transforms.Resize((50)),\n",
    "    # transforms.CenterCrop(200), ## this is for data augmentation\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "norm_dog = transform(dog)\n",
    "print(norm_dog.shape)\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.imshow(np.transpose(norm_dog, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models # add models to the list\n",
    "from torchvision.utils import make_grid\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# online data augmentation - dynamically transform our image \n",
    "# randomly during batching (through dataloader)\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224), #this will help focus on the faces\n",
    "    transforms.ToTensor(), #normalize\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224), #this will help focus on the faces\n",
    "    transforms.ToTensor(), #normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '../case_studies/data/cat_dog/'\n",
    "\n",
    "train_data = datasets.ImageFolder(os.path.join(root, 'train'), transform=train_transform)\n",
    "test_data  = datasets.ImageFolder(os.path.join(root, 'test'),  transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6998"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data) #18002 images of cats and dogs\n",
    "len(test_data)  #6998 images of cats and dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#is their any imbalance\n",
    "np_targets = np.array(test_data.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3499"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np_targets[np_targets==1]) #9001 cats, #9001 dogs | #3499 cats, 3499 dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat', 'dog']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6998"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(999)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64,  shuffle=True)\n",
    "test_loader  = DataLoader(test_data, batch_size=6998, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 224, 224])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for image, label in train_loader:\n",
    "    print(image.shape) #(bs, ch, h, w)\n",
    "    print(label.shape) #(bs,         )\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Define our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape #(bs, ch, h, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = nn.Conv2d(3, 6, 3, 1, 1)  #(in_c, out_c, k, s, p)\n",
    "l2 = nn.Conv2d(6, 16, 3, 1, 1) \n",
    "\n",
    "l2(l1(image)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class chaky_cnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Conv2d(3, 6, 3, 1, 1)  #(in_c, out_c, k, s, p)\n",
    "        self.l2 = nn.Conv2d(6, 16, 3, 1, 1) \n",
    "        self.fc1 = nn.Linear(16*224*224, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)\n",
    "    def forward(self, image):\n",
    "        out = F.relu(self.l1(image))\n",
    "        out = F.relu(self.l2(out))\n",
    "        out = out.reshape((-1, 16*224*224))\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test case\n",
    "model = chaky_cnn()\n",
    "out   = model(image)\n",
    "assert out.shape[1] == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optim     = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Training!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda0') #for GPU\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "for i in range(num_epochs):\n",
    "    train_correct = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        # images.to(device) #for GPU\n",
    "        # labels.to(device) #for GPU\n",
    "        \n",
    "        y_hat = model(images) #yhat: (bs, classes)\n",
    "        loss  = criterion(y_hat, labels)\n",
    "        \n",
    "        real_pred = torch.max(y_hat, 1)[1]\n",
    "        train_correct  += (real_pred == labels).sum()/images.shape[0]\n",
    "                \n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "    print(f\"Epoch: {i} | Train acc: {train_correct/len(train_loader):3.2f} | Loss: {loss.item():3.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_sample_image = test_data[256][0]\n",
    "some_sample_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(some_sample_image)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(output, 1)[1]\n",
    "# train_data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[256][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Using pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models #actually pretrained models are offered by many vendors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet = models.alexnet(weights=models.AlexNet_Weights.DEFAULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freeze the parameters\n",
    "#you don't need to train the parameters\n",
    "for param in alexnet.parameters():\n",
    "    param.requires_grad = False  #you don't allow any parameter in AlexNet to learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet.classifier = nn.Sequential(\n",
    "    nn.Linear(in_features=9216, out_features=1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(1024, 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(alexnet.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Train acc: 0.89 | Loss: 0.2527\n",
      "Epoch: 1 | Train acc: 0.91 | Loss: 0.1178\n",
      "Epoch: 2 | Train acc: 0.92 | Loss: 0.1719\n",
      "Epoch: 3 | Train acc: 0.92 | Loss: 0.1969\n",
      "Epoch: 4 | Train acc: 0.93 | Loss: 0.1429\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "for i in range(num_epochs):\n",
    "    train_correct = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        y_hat = alexnet(images) #yhat: (bs, classes)\n",
    "        loss  = criterion(y_hat, labels)\n",
    "        \n",
    "        real_pred = torch.max(y_hat, 1)[1]\n",
    "        train_correct  += (real_pred == labels).sum()/images.shape[0]\n",
    "                \n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "    print(f\"Epoch: {i} | Train acc: {train_correct/len(train_loader):3.2f} | Loss: {loss.item():3.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
