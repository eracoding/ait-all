{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch - CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn #defining neural network\n",
    "import torch.nn.functional as F #helper\n",
    "from torch.utils.data import DataLoader #batching\n",
    "from torchvision import datasets, transforms #loading datasets\n",
    "from torchvision.utils import make_grid  #for visualization\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.16.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install torchvision\n",
    "import torchvision\n",
    "torchvision.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10afe2150>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to help serve as \"alternative\" cross-validation in deep learning\n",
    "torch.manual_seed(22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define some transforms (convert 0-255 to 0-1)\n",
    "transform = transforms.ToTensor()\n",
    "#if you want to crop the image, rotate the image; they are all done in this transform\n",
    "\n",
    "#use pytorch datasets to load MNIST - dataset of digit images\n",
    "#train\n",
    "train_data = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
    "\n",
    "#validation set\n",
    "train_set, val_set = torch.utils.data.random_split(train_data, [50000, 10000])\n",
    "\n",
    "#test\n",
    "test_data  = datasets.MNIST(root='data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x157c57910>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CUDA out of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)  #this shuffle will depend on our seed\n",
    "val_loader   = DataLoader(val_set,   batch_size=64, shuffle=True)\n",
    "test_loader  = DataLoader(test_data, batch_size=10000, shuffle=False) #because no training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "782"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)  #782 * 64 = 50,048 = 50000 + 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for images, labels in train_loader:  #taking 64 images, corresponding 64 labels\n",
    "    break\n",
    "\n",
    "images.shape #(64 images, 1 channel, height, width)\n",
    "labels.shape #(class for the 64 images)\n",
    "\n",
    "the_image = images[0]\n",
    "the_image = np.transpose(the_image, (1, 2, 0))\n",
    "the_image.shape\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.imshow(the_image) #(h, w, c)\n",
    "\n",
    "labels[0].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape #input (bs, ch, h, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#understanding the relationship between the input and convolutional layer -> output\n",
    "#conv1d = time series, language, signal\n",
    "#conv2d = images, spectrograms\n",
    "# layer1 = nn.Con\n",
    "# nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3, stride=1, padding=1)\n",
    "layer1 = nn.Conv2d(1, 1, 3, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$O = \\frac{I-F+2P}{S} + 1$$\n",
    "\n",
    "I = image width/height, F = filter size, P = padding, S = stride, O = output width/height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how to pass something into this layer\n",
    "output = layer1(images)\n",
    "output.shape #(bs, out_channel, h, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 120])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1 = nn.Conv2d(1, 5, 3, 1, 1)\n",
    "layer2 = nn.Conv2d(5, 10, 3, 1, 1)\n",
    "layer3 = nn.Linear(10 * 28 * 28, 120)\n",
    "out = layer2(layer1(images))\n",
    "out = out.reshape((-1, 10*28*28))\n",
    "out2 = layer3(out)\n",
    "out2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chaky_CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self): #init all hyperparameters and layers\n",
    "        super().__init__() #inherit everything from nn.Module\n",
    "        self.layer1 = nn.Conv2d(1, 5, 3, 1, 1) \n",
    "        self.layer2 = nn.Conv2d(5, 10, 3, 1, 1)\n",
    "        self.layer3 = nn.Linear(10 * 28 * 28, 120)\n",
    "        self.layer4 = nn.Linear(120, 84) #why 120, 84 - chaky examples\n",
    "        self.layer5 = nn.Linear(84, 10)  #why 10, 10 classes\n",
    "        \n",
    "    def forward(self, images):  #performing the forward pass through all layers\n",
    "        out = F.relu(self.layer1(images))\n",
    "        out = F.relu(self.layer2(out))\n",
    "        out = out.reshape((-1, 10*28*28))\n",
    "        out = F.relu(self.layer3(out))\n",
    "        out = F.relu(self.layer4(out))\n",
    "        out = self.layer5(out)\n",
    "        return out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test first\n",
    "chaky_model = Chaky_CNN()\n",
    "output      = chaky_model(images)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skip counting the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define the loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(chaky_model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "for i in range(num_epochs):\n",
    "    for images, labels in train_loader:\n",
    "        y_hat = chaky_model(images)\n",
    "        loss  = criterion(y_hat, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01282444130629301"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entrophy Loss:  0.08785514533519745\n"
     ]
    }
   ],
   "source": [
    "chaky_model.eval() #this will turn off dropout, batch norm\n",
    "with torch.no_grad():  #this will turn off gradient calculations\n",
    "    for images, labels in test_loader:\n",
    "        y_hat = chaky_model(images)\n",
    "        loss = criterion(y_hat, labels)\n",
    "        \n",
    "print(\"Cross Entrophy Loss: \", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_pred = torch.max(y_hat, 1)[1]\n",
    "y_hat_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 969,    0,   14,    1,    0,    3,    8,    1,    4,    6],\n",
       "       [   0, 1127,    1,    0,    0,    0,    6,    5,    1,    1],\n",
       "       [   3,    1,  995,    2,    1,    0,    0,    6,    6,    0],\n",
       "       [   0,    2,    4,  997,    0,   11,    1,    2,    1,    1],\n",
       "       [   1,    1,    2,    0,  974,    0,   17,    0,    9,   24],\n",
       "       [   3,    0,    0,    0,    0,  867,    6,    0,    2,    2],\n",
       "       [   1,    0,    0,    0,    0,    5,  918,    0,    0,    0],\n",
       "       [   2,    2,   15,    6,    3,    4,    0, 1013,    8,   22],\n",
       "       [   1,    2,    1,    3,    1,    0,    2,    0,  933,    0],\n",
       "       [   0,    0,    0,    1,    3,    2,    0,    1,   10,  953]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#confusion matrix\n",
    "confusion_matrix(y_hat_pred, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0][0].shape\n",
    "test_data[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(np.transpose(test_data[0][0], (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = chaky_model(test_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-11.9412,  -2.8596,   3.3668,  -1.0450,  -6.3315,  -8.5397, -27.3065,\n",
       "          18.4977,  -8.6600,  -0.6668]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
