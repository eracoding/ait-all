{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.0, 1.0)\n",
    "])\n",
    "\n",
    "mnist_train = datasets.FashionMNIST('./data', train=True, download=True, transform=transform_train)\n",
    "mnist_test = datasets.FashionMNIST('./data', train=False, download=True, transform=transform_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label of image above: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label of image above: 8\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label = mnist_train[12][1]\n",
    "print('Label of image above:', label)\n",
    "img = mnist_train[12][0].numpy()\n",
    "plt.imshow(img.reshape(28, 28), cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "label = mnist_train[100][1]\n",
    "print('Label of image above:', label)\n",
    "img = mnist_train[100][0].numpy()\n",
    "plt.imshow(img.reshape(28, 28), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "mnist_train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "mnist_valid_loader = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 1, 28, 28])\n",
      "torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "for data, label in mnist_train_loader:\n",
    "    print(data.shape)\n",
    "    print(label.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_custom(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Input: 28x28x1\n",
    "        self.conv1 = nn.Conv2d(1, 12, kernel_size=5) # (28 - 5 + 0)/ 1 + 1 = 24\n",
    "        self.conv2 = nn.Conv2d(12, 24, kernel_size=5) # (22 - 5) + 1 = 20\n",
    "        \n",
    "        self.mxpool = nn.MaxPool2d(2) # 12x12x32\n",
    "\n",
    "        # Maxpooling 4x4x64\n",
    "\n",
    "        self.fc1 = nn.Linear(10 * 10 * 24, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.mxpool(F.relu(self.conv2(x)))\n",
    "\n",
    "        x = x.view(-1, 10 * 10 * 24)\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNIST_custom()\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MNIST_custom(\n",
       "  (conv1): Conv2d(1, 12, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(12, 24, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (mxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=2400, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Tr Loss: 0.7926, Tr Acc: 71.7983, Val Loss: 0.4162, Val Acc: 84.9500\n",
      "Epoch 2/20, Tr Loss: 0.3466, Tr Acc: 87.2117, Val Loss: 0.3394, Val Acc: 87.1800\n",
      "Epoch 3/20, Tr Loss: 0.2848, Tr Acc: 89.3433, Val Loss: 0.3177, Val Acc: 88.2300\n",
      "Epoch 4/20, Tr Loss: 0.2529, Tr Acc: 90.5850, Val Loss: 0.3027, Val Acc: 88.9700\n",
      "Epoch 5/20, Tr Loss: 0.2206, Tr Acc: 91.7767, Val Loss: 0.2759, Val Acc: 90.0500\n",
      "Epoch 6/20, Tr Loss: 0.1974, Tr Acc: 92.6717, Val Loss: 0.2859, Val Acc: 89.9000\n",
      "Epoch 7/20, Tr Loss: 0.1776, Tr Acc: 93.3717, Val Loss: 0.2873, Val Acc: 89.9300\n",
      "Epoch 8/20, Tr Loss: 0.1513, Tr Acc: 94.3717, Val Loss: 0.3094, Val Acc: 89.8600\n",
      "Epoch 9/20, Tr Loss: 0.1361, Tr Acc: 94.7983, Val Loss: 0.3197, Val Acc: 90.3400\n",
      "Epoch 10/20, Tr Loss: 0.1183, Tr Acc: 95.5933, Val Loss: 0.3339, Val Acc: 90.7400\n",
      "Epoch 11/20, Tr Loss: 0.1044, Tr Acc: 96.0833, Val Loss: 0.3341, Val Acc: 90.3900\n",
      "Epoch 12/20, Tr Loss: 0.0851, Tr Acc: 96.8283, Val Loss: 0.3831, Val Acc: 90.2700\n",
      "Epoch 13/20, Tr Loss: 0.0783, Tr Acc: 97.1217, Val Loss: 0.3799, Val Acc: 90.5400\n",
      "Epoch 14/20, Tr Loss: 0.0629, Tr Acc: 97.6767, Val Loss: 0.4122, Val Acc: 91.0000\n",
      "Epoch 15/20, Tr Loss: 0.0579, Tr Acc: 97.8083, Val Loss: 0.4405, Val Acc: 90.6800\n",
      "Epoch 16/20, Tr Loss: 0.0511, Tr Acc: 98.1750, Val Loss: 0.4898, Val Acc: 90.6900\n",
      "Epoch 17/20, Tr Loss: 0.0452, Tr Acc: 98.3367, Val Loss: 0.5122, Val Acc: 90.2800\n",
      "Epoch 18/20, Tr Loss: 0.0480, Tr Acc: 98.2417, Val Loss: 0.5158, Val Acc: 90.4800\n",
      "Epoch 19/20, Tr Loss: 0.0436, Tr Acc: 98.3350, Val Loss: 0.5513, Val Acc: 89.9100\n",
      "Epoch 20/20, Tr Loss: 0.0454, Tr Acc: 98.3600, Val Loss: 0.5804, Val Acc: 89.8600\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "train_accuracy = []\n",
    "valid_accuracy = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    ############################\n",
    "    # Train\n",
    "    ############################\n",
    "    \n",
    "    iter_loss = 0.0\n",
    "    correct = 0\n",
    "    iterations = 0\n",
    "    \n",
    "    model.train()                   # Put the network into training mode\n",
    "    \n",
    "    for i, (items, classes) in enumerate(mnist_train_loader):\n",
    "        \n",
    "        # Convert torch tensor to Variable\n",
    "        items = Variable(items)\n",
    "        classes = Variable(classes)\n",
    "        \n",
    "        # If we have GPU, shift the data to GPU\n",
    "        if torch.cuda.is_available():\n",
    "            items = items.cuda()\n",
    "            classes = classes.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()     # Clear off the gradients from any past operation\n",
    "        outputs = model(items)      # Do the forward pass\n",
    "        loss = criterion(outputs, classes) # Calculate the loss\n",
    "        iter_loss += loss.item() # Accumulate the loss\n",
    "        loss.backward()           # Calculate the gradients with help of back propagation\n",
    "        optimizer.step()          # Ask the optimizer to adjust the parameters based on the gradients\n",
    "        \n",
    "        # Record the correct predictions for training data \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == classes.data).sum()\n",
    "        iterations += 1\n",
    "\n",
    "    # Record the training loss\n",
    "    train_loss.append(iter_loss/iterations)\n",
    "    # Record the training accuracy\n",
    "    train_accuracy.append(100 * correct.cpu() / float(len(mnist_train_loader.dataset)))\n",
    "   \n",
    "\n",
    "    ############################\n",
    "    # Validate - How did we do on the unseen dataset?\n",
    "    ############################\n",
    "    \n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    iterations = 0\n",
    "\n",
    "    model.eval()                    # Put the network into evaluate mode\n",
    "    \n",
    "    for i, (items, classes) in enumerate(mnist_valid_loader):\n",
    "        \n",
    "        # Convert torch tensor to Variable\n",
    "        items = Variable(items)\n",
    "        classes = Variable(classes)\n",
    "        \n",
    "        # If we have GPU, shift the data to GPU\n",
    "        if torch.cuda.is_available():\n",
    "            items = items.cuda()\n",
    "            classes = classes.cuda()\n",
    "        \n",
    "        outputs = model(items)      # Do the forward pass\n",
    "        loss += criterion(outputs, classes).item() # Calculate the loss\n",
    "        \n",
    "        # Record the correct predictions for training data\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == classes.data).sum()\n",
    "        \n",
    "        iterations += 1\n",
    "\n",
    "    # Record the validation loss\n",
    "    valid_loss.append(loss/iterations)\n",
    "    # Record the validation accuracy\n",
    "    correct_scalar = np.array([correct.clone().cpu()])[0]\n",
    "    valid_accuracy.append(correct_scalar / len(mnist_valid_loader.dataset) * 100.0)\n",
    "\n",
    "    print ('Epoch %d/%d, Tr Loss: %.4f, Tr Acc: %.4f, Val Loss: %.4f, Val Acc: %.4f'\n",
    "           %(epoch+1, num_epochs, train_loss[-1], train_accuracy[-1], \n",
    "             valid_loss[-1], valid_accuracy[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNIST_custom()\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Tr Loss: 1.9821, Tr Acc: 36.6967, Val Loss: 1.5405, Val Acc: 45.7200\n",
      "Epoch 2/20, Tr Loss: 0.8779, Tr Acc: 67.6633, Val Loss: 0.7150, Val Acc: 73.8500\n",
      "Epoch 3/20, Tr Loss: 0.6489, Tr Acc: 75.3800, Val Loss: 0.6254, Val Acc: 76.3500\n",
      "Epoch 4/20, Tr Loss: 0.5755, Tr Acc: 78.3417, Val Loss: 0.5673, Val Acc: 78.9800\n",
      "Epoch 5/20, Tr Loss: 0.5337, Tr Acc: 80.1683, Val Loss: 0.5380, Val Acc: 80.2500\n",
      "Epoch 6/20, Tr Loss: 0.4947, Tr Acc: 81.8167, Val Loss: 0.4991, Val Acc: 82.2000\n",
      "Epoch 7/20, Tr Loss: 0.4732, Tr Acc: 82.8050, Val Loss: 0.4830, Val Acc: 83.1700\n",
      "Epoch 8/20, Tr Loss: 0.4441, Tr Acc: 84.2350, Val Loss: 0.4635, Val Acc: 83.6600\n",
      "Epoch 9/20, Tr Loss: 0.4216, Tr Acc: 85.0367, Val Loss: 0.4395, Val Acc: 84.6100\n",
      "Epoch 10/20, Tr Loss: 0.4077, Tr Acc: 85.5000, Val Loss: 0.4317, Val Acc: 85.0000\n",
      "Epoch 11/20, Tr Loss: 0.3948, Tr Acc: 85.9367, Val Loss: 0.4419, Val Acc: 83.6900\n",
      "Epoch 12/20, Tr Loss: 0.3800, Tr Acc: 86.4350, Val Loss: 0.4095, Val Acc: 85.5100\n",
      "Epoch 13/20, Tr Loss: 0.3680, Tr Acc: 86.8183, Val Loss: 0.4047, Val Acc: 85.0300\n",
      "Epoch 14/20, Tr Loss: 0.3630, Tr Acc: 86.8583, Val Loss: 0.3981, Val Acc: 86.0400\n",
      "Epoch 15/20, Tr Loss: 0.3514, Tr Acc: 87.2983, Val Loss: 0.3788, Val Acc: 86.7900\n",
      "Epoch 16/20, Tr Loss: 0.3356, Tr Acc: 87.8417, Val Loss: 0.3785, Val Acc: 86.2000\n",
      "Epoch 17/20, Tr Loss: 0.3253, Tr Acc: 88.3400, Val Loss: 0.3590, Val Acc: 87.1600\n",
      "Epoch 18/20, Tr Loss: 0.3238, Tr Acc: 88.3900, Val Loss: 0.3706, Val Acc: 86.8400\n",
      "Epoch 19/20, Tr Loss: 0.3260, Tr Acc: 88.1300, Val Loss: 0.3623, Val Acc: 87.1700\n",
      "Epoch 20/20, Tr Loss: 0.3127, Tr Acc: 88.7067, Val Loss: 0.3586, Val Acc: 87.3400\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "train_accuracy = []\n",
    "valid_accuracy = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    ############################\n",
    "    # Train\n",
    "    ############################\n",
    "    \n",
    "    iter_loss = 0.0\n",
    "    correct = 0\n",
    "    iterations = 0\n",
    "    \n",
    "    model.train()                   # Put the network into training mode\n",
    "    \n",
    "    for i, (items, classes) in enumerate(mnist_train_loader):\n",
    "        \n",
    "        # Convert torch tensor to Variable\n",
    "        items = Variable(items)\n",
    "        classes = Variable(classes)\n",
    "        \n",
    "        # If we have GPU, shift the data to GPU\n",
    "        if torch.cuda.is_available():\n",
    "            items = items.cuda()\n",
    "            classes = classes.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()     # Clear off the gradients from any past operation\n",
    "        outputs = model(items)      # Do the forward pass\n",
    "        loss = criterion(outputs, classes) # Calculate the loss\n",
    "        iter_loss += loss.item() # Accumulate the loss\n",
    "        loss.backward()           # Calculate the gradients with help of back propagation\n",
    "        optimizer.step()          # Ask the optimizer to adjust the parameters based on the gradients\n",
    "        \n",
    "        # Record the correct predictions for training data \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == classes.data).sum()\n",
    "        iterations += 1\n",
    "\n",
    "    # Record the training loss\n",
    "    train_loss.append(iter_loss/iterations)\n",
    "    # Record the training accuracy\n",
    "    train_accuracy.append(100 * correct.cpu() / float(len(mnist_train_loader.dataset)))\n",
    "   \n",
    "\n",
    "    ############################\n",
    "    # Validate - How did we do on the unseen dataset?\n",
    "    ############################\n",
    "    \n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    iterations = 0\n",
    "\n",
    "    model.eval()                    # Put the network into evaluate mode\n",
    "    \n",
    "    for i, (items, classes) in enumerate(mnist_valid_loader):\n",
    "        \n",
    "        # Convert torch tensor to Variable\n",
    "        items = Variable(items)\n",
    "        classes = Variable(classes)\n",
    "        \n",
    "        # If we have GPU, shift the data to GPU\n",
    "        if torch.cuda.is_available():\n",
    "            items = items.cuda()\n",
    "            classes = classes.cuda()\n",
    "        \n",
    "        outputs = model(items)      # Do the forward pass\n",
    "        loss += criterion(outputs, classes).item() # Calculate the loss\n",
    "        \n",
    "        # Record the correct predictions for training data\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == classes.data).sum()\n",
    "        \n",
    "        iterations += 1\n",
    "\n",
    "    # Record the validation loss\n",
    "    valid_loss.append(loss/iterations)\n",
    "    # Record the validation accuracy\n",
    "    correct_scalar = np.array([correct.clone().cpu()])[0]\n",
    "    valid_accuracy.append(correct_scalar / len(mnist_valid_loader.dataset) * 100.0)\n",
    "\n",
    "    print ('Epoch %d/%d, Tr Loss: %.4f, Tr Acc: %.4f, Val Loss: %.4f, Val Acc: %.4f'\n",
    "           %(epoch+1, num_epochs, train_loss[-1], train_accuracy[-1], \n",
    "             valid_loss[-1], valid_accuracy[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNIST_custom()\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Tr Loss: 91.2199, Tr Acc: 12.4567, Val Loss: 1.9677, Val Acc: 29.4300\n",
      "Epoch 2/20, Tr Loss: 1.3175, Tr Acc: 52.7917, Val Loss: 0.8371, Val Acc: 68.4800\n",
      "Epoch 3/20, Tr Loss: 0.7707, Tr Acc: 69.9850, Val Loss: 0.7374, Val Acc: 71.6200\n",
      "Epoch 4/20, Tr Loss: 0.6863, Tr Acc: 72.5900, Val Loss: 0.7115, Val Acc: 72.7200\n",
      "Epoch 5/20, Tr Loss: 0.6302, Tr Acc: 75.2233, Val Loss: 0.6588, Val Acc: 75.2300\n",
      "Epoch 6/20, Tr Loss: 0.5911, Tr Acc: 76.9583, Val Loss: 0.6747, Val Acc: 74.0500\n",
      "Epoch 7/20, Tr Loss: 0.6938, Tr Acc: 74.4333, Val Loss: 0.5876, Val Acc: 78.5500\n",
      "Epoch 8/20, Tr Loss: 0.5144, Tr Acc: 79.9533, Val Loss: 0.6008, Val Acc: 77.8600\n",
      "Epoch 9/20, Tr Loss: 0.5061, Tr Acc: 80.7350, Val Loss: 0.5942, Val Acc: 78.1200\n",
      "Epoch 10/20, Tr Loss: 0.4943, Tr Acc: 81.2533, Val Loss: 0.4995, Val Acc: 81.0900\n",
      "Epoch 11/20, Tr Loss: 0.4999, Tr Acc: 81.0900, Val Loss: 0.4704, Val Acc: 82.4100\n",
      "Epoch 12/20, Tr Loss: 0.4652, Tr Acc: 82.6783, Val Loss: 0.4886, Val Acc: 81.9800\n",
      "Epoch 13/20, Tr Loss: 1.1857, Tr Acc: 63.9500, Val Loss: 0.9649, Val Acc: 65.4100\n",
      "Epoch 14/20, Tr Loss: 0.7407, Tr Acc: 73.1950, Val Loss: 0.6489, Val Acc: 76.9000\n",
      "Epoch 15/20, Tr Loss: 0.5264, Tr Acc: 80.0667, Val Loss: 0.5375, Val Acc: 80.2400\n",
      "Epoch 16/20, Tr Loss: 0.4846, Tr Acc: 81.5933, Val Loss: 0.4964, Val Acc: 81.4100\n",
      "Epoch 17/20, Tr Loss: 0.4375, Tr Acc: 83.3717, Val Loss: 0.4770, Val Acc: 82.0900\n",
      "Epoch 18/20, Tr Loss: 0.4386, Tr Acc: 83.6567, Val Loss: 0.7177, Val Acc: 74.7500\n",
      "Epoch 19/20, Tr Loss: 0.6280, Tr Acc: 79.0550, Val Loss: 0.4417, Val Acc: 83.9500\n",
      "Epoch 20/20, Tr Loss: 0.3974, Tr Acc: 85.0583, Val Loss: 0.4263, Val Acc: 84.4200\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "train_accuracy = []\n",
    "valid_accuracy = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    ############################\n",
    "    # Train\n",
    "    ############################\n",
    "    \n",
    "    iter_loss = 0.0\n",
    "    correct = 0\n",
    "    iterations = 0\n",
    "    \n",
    "    model.train()                   # Put the network into training mode\n",
    "    \n",
    "    for i, (items, classes) in enumerate(mnist_train_loader):\n",
    "        \n",
    "        # Convert torch tensor to Variable\n",
    "        items = Variable(items)\n",
    "        classes = Variable(classes)\n",
    "        \n",
    "        # If we have GPU, shift the data to GPU\n",
    "        if torch.cuda.is_available():\n",
    "            items = items.cuda()\n",
    "            classes = classes.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()     # Clear off the gradients from any past operation\n",
    "        outputs = model(items)      # Do the forward pass\n",
    "        loss = criterion(outputs, classes) # Calculate the loss\n",
    "        iter_loss += loss.item() # Accumulate the loss\n",
    "        loss.backward()           # Calculate the gradients with help of back propagation\n",
    "        optimizer.step()          # Ask the optimizer to adjust the parameters based on the gradients\n",
    "        \n",
    "        # Record the correct predictions for training data \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == classes.data).sum()\n",
    "        iterations += 1\n",
    "\n",
    "    # Record the training loss\n",
    "    train_loss.append(iter_loss/iterations)\n",
    "    # Record the training accuracy\n",
    "    train_accuracy.append(100 * correct.cpu() / float(len(mnist_train_loader.dataset)))\n",
    "   \n",
    "\n",
    "    ############################\n",
    "    # Validate - How did we do on the unseen dataset?\n",
    "    ############################\n",
    "    \n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    iterations = 0\n",
    "\n",
    "    model.eval()                    # Put the network into evaluate mode\n",
    "    \n",
    "    for i, (items, classes) in enumerate(mnist_valid_loader):\n",
    "        \n",
    "        # Convert torch tensor to Variable\n",
    "        items = Variable(items)\n",
    "        classes = Variable(classes)\n",
    "        \n",
    "        # If we have GPU, shift the data to GPU\n",
    "        if torch.cuda.is_available():\n",
    "            items = items.cuda()\n",
    "            classes = classes.cuda()\n",
    "        \n",
    "        outputs = model(items)      # Do the forward pass\n",
    "        loss += criterion(outputs, classes).item() # Calculate the loss\n",
    "        \n",
    "        # Record the correct predictions for training data\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == classes.data).sum()\n",
    "        \n",
    "        iterations += 1\n",
    "\n",
    "    # Record the validation loss\n",
    "    valid_loss.append(loss/iterations)\n",
    "    # Record the validation accuracy\n",
    "    correct_scalar = np.array([correct.clone().cpu()])[0]\n",
    "    valid_accuracy.append(correct_scalar / len(mnist_valid_loader.dataset) * 100.0)\n",
    "\n",
    "    print ('Epoch %d/%d, Tr Loss: %.4f, Tr Acc: %.4f, Val Loss: %.4f, Val Acc: %.4f'\n",
    "           %(epoch+1, num_epochs, train_loss[-1], train_accuracy[-1], \n",
    "             valid_loss[-1], valid_accuracy[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "I tried to show the same architecture but with different optimization methods which clearly indicates the one that performing better - Adam.\n",
    "\n",
    "1. Adam accuracy - 98.3% training\n",
    "2. SGD accuracy - 87.8% training\n",
    "3. RMSprop accuracy - 85.1% training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl4cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
