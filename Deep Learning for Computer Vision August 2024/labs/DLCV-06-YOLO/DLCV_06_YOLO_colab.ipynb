{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sICtOB9_xcaY"
      },
      "source": [
        "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
        "\n",
        "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3d3XqjrCxcac"
      },
      "outputs": [],
      "source": [
        "NAME = \"Ulugbek Shernazarov\"\n",
        "ID = \"st125457\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6S8IsP9lxcae"
      },
      "source": [
        "# 06-YOLO\n",
        "\n",
        "In this lab, we'll explore a fascinating use of image classification deep neural networks to peform a different\n",
        "task: object detection.\n",
        "\n",
        "Credits: parts of this lab are based on other authors' code and blog posts:\n",
        "\n",
        "- YOLO v3 in PyTorch: [tutorial code](https://github.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch),\n",
        "  [blog](https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/)\n",
        "\n",
        "\n",
        "## Object Detection\n",
        "\n",
        "If you could go back in time to the 1990s, there were no cameras that could find faces in a photograph, and no\n",
        "researcher had a way to count dogs in a video in real time. Everyone had to count the dogs manually.\n",
        "Times were very tough.\n",
        "\n",
        "The Holy Grail of computer vision research at the time was real time face detection. If we could find faces\n",
        "in images fast enough, we could build systems that interact more naturally with human beings. But nobody had\n",
        "a solution.\n",
        "\n",
        "Things changed when Viola and Jones introduced the first real time face detector, the Haar-like cascade, at\n",
        "the end of the 1990s.\n",
        "This technique swept a detection window over the input image at multiple sizes, and subjected each local patch\n",
        "to a cascade of simple rough classifiers. Each patch that made it to the end of the cascade of classifiers was\n",
        "treated as a positive detection. After a set of candidate patches were identified, there would be a cleanup\n",
        "stage when neighboring detections are clustered into isolated detections.\n",
        "\n",
        "This method and one cousin, the HOG detector, which was slower but a little more accurate, dominated during the 2000s\n",
        "and on into the 2010s. These methods worked well enough when trained carefully on the specific environment they were\n",
        "used in, but usually couldn't be transfer to a new environment.\n",
        "\n",
        "With the introduction of AlexNet and the amazing advances in image classification, we could follow the direction\n",
        "of R-CNN, to use a region proposal algorithm followed by a deep learning classifier to do object detection VERY slowly\n",
        "but much more accurately than the old real time methods.\n",
        "\n",
        "## What is YOLO?\n",
        "\n",
        "However, it wasn't until YOLO that we had a deep learning model for object detection that could run in real time.\n",
        "It took some clever insight to realize that everything, from feature extraction to bounding box estimation, could\n",
        "actually be done with a single neural network that could be trained end-to-end to detect objects.\n",
        "\n",
        "YOLO (You Only Look Once) uses only convolutional layers. This makes it a **\"fully convolutional network\" or FCN**.\n",
        "\n",
        "YOLOv3 has 75 convolutional layers, with skip connections and upsampling layers. No pooling is used, but there is a convolutional\n",
        "layer with stride 2 used for downsampling. **Strided convoution** rather than pooling was used to prevent loss of fine-grained detailed\n",
        "information about the precise location of low-level features that would otherwise occur with pooling.\n",
        "\n",
        "Normally, the output of a convolutional layer is a feature map. Applying convolutional layers to a possible detection window or\n",
        "region of interest (ROI) in the image then classifying the ROI's feature map is a reasonable method for detection prediction that\n",
        "is used in Fast R-CNN and Faster R-CNN.\n",
        "However, the innovation of YOLO was to use the feature map directly to predict bounding boxes and, for each bounding box, to\n",
        "predict whether or not an object is at the center of the bounding box. Finally, a classifier is used for each bounding box\n",
        "to indicate the content of the bounding box.\n",
        "\n",
        "## YOLO v3 from \"scratch\"\n",
        "\n",
        "Early versions of YOLO were very fast but not nearly as accurate as their slower cousins. YOLO v3 included many of the\n",
        "tricks and techniques used by other models, such as multiscale analysis, and it achieved both high accuracy and fast inference.\n",
        "\n",
        "Here we'll experiment with building up the YOLO v3 model in PyTorch. However, we won't train it ourselves, as that would\n",
        "require days of training; instead, we'll\n",
        "grab the weights for our PyTorch YOLO v3 from the original Darknet model by Joseph Redmon and friends.\n",
        "\n",
        "### Ground Truth Bounding Boxes\n",
        "\n",
        "Here is how we present example images and corresponding object bounding boxes to the model.\n",
        "\n",
        "The input image is divided into grid cells. The number of cells depends on the number of convolutional layers\n",
        "and the stride of each of those convolutional layers. For example, if we use a 416 $\\times$ 416 input image size,\n",
        "and we apply 5 conv layers with a stride of 2 each (for a total downsampling factor of 32), we end up with a 13$\\times$13\n",
        "feature map, each corresponding to a region in the original image of size 32 $\\times$ 32 pixels.\n",
        "\n",
        "A ground truth box has a center (x and y position), a width, and a height. Normally the ground truth boxes would be\n",
        "provided by a human annotator.\n",
        "\n",
        "Each ground truth box's center must lie in some grid cell in the original image. Consider this example from the YOLO paper:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tJtYxfVxcae"
      },
      "source": [
        "<img src=\"img/yolo05.png\" title=\"GroundTruthBox\" style=\"width: 400px;\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBRjYB_5xcae"
      },
      "source": [
        "The grid is represented by the black lines. The ground truth bounding box for the object is the yellow rectangle. The center\n",
        "of this bounding box happens to be within the red-outlined grid cell.\n",
        "\n",
        "The grid cell containing the center of a ground truth bounding box is given the responsibility during training to try to predict\n",
        "the presence of the object.\n",
        "\n",
        "In order to indicate the presence of the given object, the model outputs several parameters for a given candidate object:\n",
        " - $(t_x, t_y, t_w, t_h)$ indicate the box's location and size. During training, the targets for these outputs are the actual ground truth box parameters.\n",
        " - $p_o$ is an \"objectness\" score that indicates the likelihood that an object exists in the given bounding box. This output uses a sigmoid function.\n",
        "   During training, the target for $p_o$ is set to 1 for the center grid cell (the red grid cell), and it is set to 0 for the the neighboring grid cells.\n",
        " - $(p_1, p_2, \\ldots, p_n)$ are class confidence scores. They indicate the probability of the detected object belonging to a particular class. The targets,\n",
        "   obviously, are set to 1 for the ground truth object class and 0 for other classes during training.\n",
        "\n",
        "### Anchor Boxes\n",
        "\n",
        "One problem that would occur in YOLO if you tried to directly learn the parameters mentioned above is the problem of unstable gradients during training.\n",
        "In a way that is sort of analagous to how a residual block begins with an identity map and learns differences from identity, YOLO v3 uses the idea of\n",
        "anchor boxes originally introduced by the R-CNN team. Instead of predicting $(t_x, t_y, t_w, t_h)$ directly, we predict how those parameters are *different from\n",
        "the parameters of a typical bounding box, an anchor box*.\n",
        "YOLO v3 uses three bounding boxes per cell. At training time, once ground truth bounding box's center is mapped to a grid cell, we find which of the anchors for\n",
        "that cell has the highest IoU with the ground truth box.\n",
        "\n",
        "### So What Does YOLO Actually Predict?\n",
        "\n",
        "First, let's understand that all predictions are relative to the grid cell. YOLO predicts the following:\n",
        "- Offsets $(t_x, t_y)$ are specified relative to the top left corner of the grid cell, as a ratio between 0 and 1, using a sigmoid to limit the values.\n",
        "- Height, and width $(t_w, t_h)$ are specified relative to the dimensions of an anchor box.\n",
        "\n",
        "Thus, YOLO does not predict absolute coordinates -- it predicts values that can then be used to compute the box's position and size in absolute coordinates.\n",
        "This diagram gives the idea. We see that the absolute $t_x$ is the grid cell's $(c_x, c_y)$ plus $\\sigma(t_x)$ times the grid cell width. Similarly for $t_y$.\n",
        "The absolute width of the predicted bounding box is the width of the anchor box times $e^{tw}$. Similarly for the height."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zO0xjcxxcae"
      },
      "source": [
        "<img src=\"img/yolo06.png\" title=\"GroundTruthBox\" style=\"width: 640px;\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrF-Rf8rxcae"
      },
      "source": [
        "Hopefully you can see the difference between the YOLO v3 bounding box predictions and the Faster R-CNN bounding box predictions. The offset of the center is\n",
        "encoded relative to the grid cell containing the anchor box rather than the anchor box itself. The dimensions of the bounding box, however, similar to Faster R-CNN,\n",
        "are predicted relative to the anchor box size.\n",
        "\n",
        "### Multi-scale prediction\n",
        "\n",
        "Rather than a single grid size and grid cell size,\n",
        "YOLO v3 detects objects at multiple sizes with downsampling factors of 32, 16, and 8. The largest objects are detected at the\n",
        "first, coarsest scale, whereas mid-sized objects are detected at the intermediate scale, and small objects are detected at the finest\n",
        "scale. The example below shows the three grid sizes relative to the image and an object:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Re-7tVEexcaf"
      },
      "source": [
        "<img src=\"img/yolo_Scales.png\" title=\"GroundTruthBox\" style=\"width: 640px;\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKPCel6hxcaf"
      },
      "source": [
        "### YOLO dataset format\n",
        "\n",
        "A YOLO dataset contains two sets of files, each pair with the same name: image files (in any supported format)\n",
        "and label files (in TXT, JSON, or XML format). A label file contains data in the format\n",
        "\n",
        "$$i, C_x, C_y, L_x, L_y$$\n",
        "\n",
        "- $i$: Label index\n",
        "- $C_x$: Center position in the horizontal ($x$-axis) direction, encoded in the range 0-1, where 0 means the left edge of the image and 1 means the right edge.\n",
        "- $C_y$: Center position in the vertical ($y$-axis) direction, encoded in the range 0-1, where 0 means the top edge of the image and 1 means the bottom edge.\n",
        "- $L_x$: Object width, encoded in the range 0-1, where 1 means the width of the image.\n",
        "- $L_y$: Object height, encoded in the range 0-1, where 1 means the height of the image.\n",
        "\n",
        "To calculate these values for an object, suppose $(W,H)$ is the actual size of a particular image in pixels,\n",
        "$(O_x, O_y)$ is the actual position of an object in that image, in pixels, and $(l_x,l_y)$ is the actual size of the object, again in pixels.\n",
        "The object label elements would be calculated as\n",
        "\n",
        "$$C_x = \\frac{O_x}{W}, \\; C_y = \\frac{O_y}{H} \\\\\n",
        "L_x = \\frac{l_x}{W}, \\; L_y = \\frac{l_y}{H}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HV2ATfN8xcaf"
      },
      "source": [
        "### YOLOv3 Architecture\n",
        "\n",
        "<img src=\"img/YOLOv3_Architecture.png\" title=\"YOLOv3\" style=\"width: 960px;\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5D8NLp0xcaf"
      },
      "source": [
        "### Preparation for Building YOLO in PyTorch\n",
        "\n",
        "First of all, we will need OpenCV:\n",
        "\n",
        "    pip3 install --upgrade pip\n",
        "    pip install matplotlib opencv-python\n",
        "\n",
        "Create a directory where the code for your detector will live.\n",
        "\n",
        "In that directory, download util.py and darknet.py from https://github.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch.\n",
        "\n",
        "In Jupyter you would download thusly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuplOw95xcaf",
        "outputId": "3245d34d-32c5-4787-bb79-bb27f078cb32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-19 09:46:02--  https://raw.githubusercontent.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/master/darknet.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11533 (11K) [text/plain]\n",
            "Saving to: ‘darknet.py.1’\n",
            "\n",
            "\rdarknet.py.1          0%[                    ]       0  --.-KB/s               \rdarknet.py.1        100%[===================>]  11.26K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-10-19 09:46:02 (124 MB/s) - ‘darknet.py.1’ saved [11533/11533]\n",
            "\n",
            "--2024-10-19 09:46:03--  https://raw.githubusercontent.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/master/util.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7432 (7.3K) [text/plain]\n",
            "Saving to: ‘util.py.1’\n",
            "\n",
            "util.py.1           100%[===================>]   7.26K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-10-19 09:46:03 (98.4 MB/s) - ‘util.py.1’ saved [7432/7432]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/master/darknet.py\n",
        "!wget https://raw.githubusercontent.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/master/util.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tafs4OELxcaf"
      },
      "source": [
        "If you're in a Docker container, just run the `wget` commands at the command line. Make sure your proxy environment variables are set correctly.\n",
        "\n",
        "### Take a Look at the YOLO Darknet Configuration File\n",
        "\n",
        "Next, let's download the `yolov3.cfg` configuration file and take a look. You could grab it from the canonical Darknet github repository\n",
        "or any other place it's stored."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SOBXafGxcaf",
        "outputId": "c80ee3c6-2246-4aa1-9718-6292052ffee7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-19 09:46:03--  https://raw.githubusercontent.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/master/cfg/yolov3.cfg\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8346 (8.2K) [text/plain]\n",
            "Saving to: ‘yolov3.cfg’\n",
            "\n",
            "\ryolov3.cfg            0%[                    ]       0  --.-KB/s               \ryolov3.cfg          100%[===================>]   8.15K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-10-19 09:46:03 (97.5 MB/s) - ‘yolov3.cfg’ saved [8346/8346]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p cfg\n",
        "!wget https://raw.githubusercontent.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/master/cfg/yolov3.cfg\n",
        "!mv yolov3.cfg cfg/yolov3.cfg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKJgCP2_xcag"
      },
      "source": [
        "The configuration file looks like this:\n",
        "\n",
        "    [net]\n",
        "    # Testing\n",
        "    batch=1\n",
        "    subdivisions=1\n",
        "    # Training\n",
        "    # batch=64\n",
        "    # subdivisions=16\n",
        "    width= 416\n",
        "\n",
        "    height = 416\n",
        "    channels=3\n",
        "    momentum=0.9\n",
        "    decay=0.0005\n",
        "    angle=0\n",
        "    saturation = 1.5\n",
        "    exposure = 1.5\n",
        "    hue=.1\n",
        "\n",
        "    learning_rate=0.001\n",
        "    burn_in=1000\n",
        "    max_batches = 500200\n",
        "    policy=steps\n",
        "    steps=400000,450000\n",
        "    scales=.1,.1\n",
        "\n",
        "    [convolutional]\n",
        "    batch_normalize=1\n",
        "    filters=32\n",
        "    size=3\n",
        "    stride=1\n",
        "    pad=1\n",
        "    activation=leaky\n",
        "\n",
        "    ...\n",
        "\n",
        "    [shortcut]\n",
        "    from=-3\n",
        "    activation=linear\n",
        "\n",
        "    ...\n",
        "\n",
        "    [yolo]\n",
        "    mask = 6,7,8\n",
        "    anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326\n",
        "    classes=80\n",
        "    num=9\n",
        "    jitter=.3\n",
        "    ignore_thresh = .7\n",
        "    truth_thresh = 1\n",
        "    random=1\n",
        "\n",
        "    [route]\n",
        "    layers = -4\n",
        "\n",
        "    [convolutional]\n",
        "    batch_normalize=1\n",
        "    filters=256\n",
        "    size=1\n",
        "    stride=1\n",
        "    pad=1\n",
        "    activation=leaky\n",
        "\n",
        "    [upsample]\n",
        "    stride=2\n",
        "\n",
        "    [route]\n",
        "    layers = -1, 61\n",
        "\n",
        "    ...\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Uy37U_Rxcah"
      },
      "source": [
        "### Overview of the Configuration Blocks\n",
        "\n",
        "The configuration blocks fall into a few cateogies:\n",
        "\n",
        "- Net: the global configuration at the top of the configuration file. It declares the size of input images, batch size, learning rate, and so on.\n",
        "\n",
        "      batch=64\n",
        "      subdivisions=16\n",
        "      width=608\n",
        "      height=608\n",
        "      channels=3\n",
        "      momentum=0.9\n",
        "      decay=0.0005\n",
        "      angle=0\n",
        "      saturation = 1.5\n",
        "      exposure = 1.5\n",
        "      hue=.1\n",
        "\n",
        "\n",
        "- Convolutional: convolutional layer. Note that this specfication is a little more powerful than the PyTorch way of doing things, as options\n",
        "  for batch normalization and the activation function (leaky ReLU in this case) are built in.\n",
        "\n",
        "      [convolutional]\n",
        "      batch_normalize=1\n",
        "      filters=32\n",
        "      size=3\n",
        "      stride=1\n",
        "      pad=1\n",
        "      activation=leaky\n",
        "        \n",
        "\n",
        "- Shortcut: skip connections that implement residual blocks. -3 means to add the feature maps output by the previous layer to those output by the layer three layers\n",
        "  back. Linear activation means identity (no nonlinear activation of the result).\n",
        "  \n",
        "      [shortcut]\n",
        "      from=-3           # Connect the layer three layers back to here.\n",
        "      activation=linear\n",
        "\n",
        "\n",
        "- Upsample: Bilinear upsampling of the previous layer using a particular stride\n",
        "\n",
        "      [upsample]\n",
        "      stride=2\n",
        "\n",
        "\n",
        "- Route: The route layer deserves a bit of explanation. It has an attribute `layers`, which can have either one or two values.\n",
        "  \n",
        "      [route]\n",
        "      layers = -4\n",
        "\n",
        "      [route]\n",
        "      layers = -1, 61    \n",
        "  \n",
        "  When the layers attribute has only one value, it outputs the feature maps of the layer indexed by the value. In our example, it is -4, so the layer will output\n",
        "  the feature maps from the 4th layer backwards from the route layer.\n",
        "\n",
        "  When layers has two values, it returns the concatenated feature maps of the layers indexed by its values. In our example it is -1, 61, so the layer will output\n",
        "  feature maps from the previous layer (-1) and the 61st layer, concatenated along the channels (depth) dimension.\n",
        "   \n",
        "- YOLO:\n",
        "\n",
        "      [yolo]\n",
        "      mask = 0,1,2\n",
        "      anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326\n",
        "      classes=80\n",
        "      num=9\n",
        "      jitter=.3\n",
        "      ignore_thresh = .5\n",
        "      truth_thresh = 1\n",
        "      random=1\n",
        "  \n",
        "  Here we have a few important attributes:\n",
        "  \n",
        "  - anchors: describes the anchor boxes. The model contains 9 anchors, but only those in the `mask` are used.\n",
        "\n",
        "  - mask: which anchor indices will be used in this YOLO layer\n",
        "     \n",
        "  - classes: number of object classes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "em6ey3bJxcah"
      },
      "source": [
        "### Create a network from the config file\n",
        "\n",
        "We are going to follow the general approach of some of the GitHub contributors who have developed PyTorch tools\n",
        "to deal with Darknet models. In the file `darknet.py`, there's a `parse_cfg` function. The function will read\n",
        "the Darknet configuration file and store the blocks in a dictionary.\n",
        "\n",
        "<img src=\"img/configfunc.JPG\" title=\"configfunc\" style=\"width: 640px;\" />\n",
        "\n",
        "We'll then create PyTorch NN Modules for each of the blocks in the Darknet configuration as implemented in the\n",
        "`create_modules` function. Take a look at this function for more understanding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXuG7R22xcah"
      },
      "source": [
        "### Convolutional block\n",
        "\n",
        "<img src=\"img/convolutionalblock.JPG\" title=\"covolutionalblock\" style=\"width: 600px;\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjVL0CNexcah"
      },
      "source": [
        "### Shortcut block\n",
        "\n",
        "<img src=\"img/shortcutblock.JPG\" title=\"shortcutblock\" style=\"width: 600px;\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5_4o3dFxcah"
      },
      "source": [
        "### Upsample block\n",
        "\n",
        "<img src=\"img/upsampleblock.JPG\" title=\"upsampleblock\" style=\"width: 600px;\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3i10ORPcxcah"
      },
      "source": [
        "### Route block\n",
        "\n",
        "Why does it use an empty layer? The actual work will be done in the `forward()` function.\n",
        "\n",
        "<img src=\"img/routeblock.JPG\" title=\"routeblock\" style=\"width: 600px;\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IU_mV9ptxcai"
      },
      "source": [
        "### YOLO block\n",
        "\n",
        "<img src=\"img/yoloblock.JPG\" title=\"yoloblock\" style=\"width: 600px;\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxrsR5Mdxcai"
      },
      "source": [
        "### Using the code\n",
        "\n",
        "OK, let's try it out. Depending on what you already have installed, you may need to run\n",
        "\n",
        "    # apt install libgl1-mesa-glx\n",
        "\n",
        "for the next step to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "tags": [
          "outputPrepend"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmDlUnPexcai",
        "outputId": "b78211bf-8a9e-40fd-d990-d42ec23b711b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "({'type': 'net', 'batch': '1', 'subdivisions': '1', 'width': '416', 'height': '416', 'channels': '3', 'momentum': '0.9', 'decay': '0.0005', 'angle': '0', 'saturation': '1.5', 'exposure': '1.5', 'hue': '.1', 'learning_rate': '0.001', 'burn_in': '1000', 'max_batches': '500200', 'policy': 'steps', 'steps': '400000,450000', 'scales': '.1,.1'}, ModuleList(\n",
            "  (0): Sequential(\n",
            "    (conv_0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_0): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (1): Sequential(\n",
            "    (conv_1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (batch_norm_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (2): Sequential(\n",
            "    (conv_2): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (3): Sequential(\n",
            "    (conv_3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_3): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (4): Sequential(\n",
            "    (shortcut_4): EmptyLayer()\n",
            "  )\n",
            "  (5): Sequential(\n",
            "    (conv_5): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (batch_norm_5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (6): Sequential(\n",
            "    (conv_6): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_6): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (7): Sequential(\n",
            "    (conv_7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_7): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (8): Sequential(\n",
            "    (shortcut_8): EmptyLayer()\n",
            "  )\n",
            "  (9): Sequential(\n",
            "    (conv_9): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_9): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (10): Sequential(\n",
            "    (conv_10): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_10): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (11): Sequential(\n",
            "    (shortcut_11): EmptyLayer()\n",
            "  )\n",
            "  (12): Sequential(\n",
            "    (conv_12): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (batch_norm_12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_12): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (13): Sequential(\n",
            "    (conv_13): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_13): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (14): Sequential(\n",
            "    (conv_14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_14): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_14): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (15): Sequential(\n",
            "    (shortcut_15): EmptyLayer()\n",
            "  )\n",
            "  (16): Sequential(\n",
            "    (conv_16): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_16): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_16): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (17): Sequential(\n",
            "    (conv_17): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_17): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_17): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (18): Sequential(\n",
            "    (shortcut_18): EmptyLayer()\n",
            "  )\n",
            "  (19): Sequential(\n",
            "    (conv_19): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_19): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_19): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (20): Sequential(\n",
            "    (conv_20): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_20): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_20): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (21): Sequential(\n",
            "    (shortcut_21): EmptyLayer()\n",
            "  )\n",
            "  (22): Sequential(\n",
            "    (conv_22): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_22): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_22): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (23): Sequential(\n",
            "    (conv_23): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_23): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_23): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (24): Sequential(\n",
            "    (shortcut_24): EmptyLayer()\n",
            "  )\n",
            "  (25): Sequential(\n",
            "    (conv_25): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_25): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_25): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (26): Sequential(\n",
            "    (conv_26): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_26): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_26): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (27): Sequential(\n",
            "    (shortcut_27): EmptyLayer()\n",
            "  )\n",
            "  (28): Sequential(\n",
            "    (conv_28): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_28): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_28): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (29): Sequential(\n",
            "    (conv_29): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_29): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_29): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (30): Sequential(\n",
            "    (shortcut_30): EmptyLayer()\n",
            "  )\n",
            "  (31): Sequential(\n",
            "    (conv_31): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_31): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_31): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (32): Sequential(\n",
            "    (conv_32): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_32): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_32): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (33): Sequential(\n",
            "    (shortcut_33): EmptyLayer()\n",
            "  )\n",
            "  (34): Sequential(\n",
            "    (conv_34): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_34): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_34): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (35): Sequential(\n",
            "    (conv_35): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_35): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_35): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (36): Sequential(\n",
            "    (shortcut_36): EmptyLayer()\n",
            "  )\n",
            "  (37): Sequential(\n",
            "    (conv_37): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (batch_norm_37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_37): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (38): Sequential(\n",
            "    (conv_38): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_38): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_38): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (39): Sequential(\n",
            "    (conv_39): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_39): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_39): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (40): Sequential(\n",
            "    (shortcut_40): EmptyLayer()\n",
            "  )\n",
            "  (41): Sequential(\n",
            "    (conv_41): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_41): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_41): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (42): Sequential(\n",
            "    (conv_42): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_42): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_42): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (43): Sequential(\n",
            "    (shortcut_43): EmptyLayer()\n",
            "  )\n",
            "  (44): Sequential(\n",
            "    (conv_44): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_44): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_44): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (45): Sequential(\n",
            "    (conv_45): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_45): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_45): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (46): Sequential(\n",
            "    (shortcut_46): EmptyLayer()\n",
            "  )\n",
            "  (47): Sequential(\n",
            "    (conv_47): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_47): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_47): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (48): Sequential(\n",
            "    (conv_48): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_48): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_48): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (49): Sequential(\n",
            "    (shortcut_49): EmptyLayer()\n",
            "  )\n",
            "  (50): Sequential(\n",
            "    (conv_50): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_50): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_50): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (51): Sequential(\n",
            "    (conv_51): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_51): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_51): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (52): Sequential(\n",
            "    (shortcut_52): EmptyLayer()\n",
            "  )\n",
            "  (53): Sequential(\n",
            "    (conv_53): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_53): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_53): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (54): Sequential(\n",
            "    (conv_54): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_54): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_54): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (55): Sequential(\n",
            "    (shortcut_55): EmptyLayer()\n",
            "  )\n",
            "  (56): Sequential(\n",
            "    (conv_56): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_56): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_56): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (57): Sequential(\n",
            "    (conv_57): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_57): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_57): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (58): Sequential(\n",
            "    (shortcut_58): EmptyLayer()\n",
            "  )\n",
            "  (59): Sequential(\n",
            "    (conv_59): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_59): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_59): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (60): Sequential(\n",
            "    (conv_60): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_60): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_60): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (61): Sequential(\n",
            "    (shortcut_61): EmptyLayer()\n",
            "  )\n",
            "  (62): Sequential(\n",
            "    (conv_62): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (batch_norm_62): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_62): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (63): Sequential(\n",
            "    (conv_63): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_63): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_63): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (64): Sequential(\n",
            "    (conv_64): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_64): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_64): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (65): Sequential(\n",
            "    (shortcut_65): EmptyLayer()\n",
            "  )\n",
            "  (66): Sequential(\n",
            "    (conv_66): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_66): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_66): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (67): Sequential(\n",
            "    (conv_67): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_67): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_67): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (68): Sequential(\n",
            "    (shortcut_68): EmptyLayer()\n",
            "  )\n",
            "  (69): Sequential(\n",
            "    (conv_69): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_69): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_69): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (70): Sequential(\n",
            "    (conv_70): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_70): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_70): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (71): Sequential(\n",
            "    (shortcut_71): EmptyLayer()\n",
            "  )\n",
            "  (72): Sequential(\n",
            "    (conv_72): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_72): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_72): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (73): Sequential(\n",
            "    (conv_73): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_73): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_73): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (74): Sequential(\n",
            "    (shortcut_74): EmptyLayer()\n",
            "  )\n",
            "  (75): Sequential(\n",
            "    (conv_75): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_75): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_75): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (76): Sequential(\n",
            "    (conv_76): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_76): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_76): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (77): Sequential(\n",
            "    (conv_77): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_77): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_77): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (78): Sequential(\n",
            "    (conv_78): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_78): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_78): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (79): Sequential(\n",
            "    (conv_79): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_79): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_79): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (80): Sequential(\n",
            "    (conv_80): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_80): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_80): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (81): Sequential(\n",
            "    (conv_81): Conv2d(1024, 255, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (82): Sequential(\n",
            "    (Detection_82): DetectionLayer()\n",
            "  )\n",
            "  (83): Sequential(\n",
            "    (route_83): EmptyLayer()\n",
            "  )\n",
            "  (84): Sequential(\n",
            "    (conv_84): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_84): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_84): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (85): Sequential(\n",
            "    (upsample_85): Upsample(scale_factor=2.0, mode='nearest')\n",
            "  )\n",
            "  (86): Sequential(\n",
            "    (route_86): EmptyLayer()\n",
            "  )\n",
            "  (87): Sequential(\n",
            "    (conv_87): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_87): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_87): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (88): Sequential(\n",
            "    (conv_88): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_88): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_88): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (89): Sequential(\n",
            "    (conv_89): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_89): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_89): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (90): Sequential(\n",
            "    (conv_90): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_90): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_90): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (91): Sequential(\n",
            "    (conv_91): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_91): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_91): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (92): Sequential(\n",
            "    (conv_92): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_92): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_92): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (93): Sequential(\n",
            "    (conv_93): Conv2d(512, 255, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (94): Sequential(\n",
            "    (Detection_94): DetectionLayer()\n",
            "  )\n",
            "  (95): Sequential(\n",
            "    (route_95): EmptyLayer()\n",
            "  )\n",
            "  (96): Sequential(\n",
            "    (conv_96): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_96): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_96): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (97): Sequential(\n",
            "    (upsample_97): Upsample(scale_factor=2.0, mode='nearest')\n",
            "  )\n",
            "  (98): Sequential(\n",
            "    (route_98): EmptyLayer()\n",
            "  )\n",
            "  (99): Sequential(\n",
            "    (conv_99): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_99): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_99): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (100): Sequential(\n",
            "    (conv_100): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_100): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_100): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (101): Sequential(\n",
            "    (conv_101): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_101): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_101): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (102): Sequential(\n",
            "    (conv_102): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_102): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_102): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (103): Sequential(\n",
            "    (conv_103): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_103): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_103): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (104): Sequential(\n",
            "    (conv_104): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_104): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_104): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (105): Sequential(\n",
            "    (conv_105): Conv2d(256, 255, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (106): Sequential(\n",
            "    (Detection_106): DetectionLayer()\n",
            "  )\n",
            "))\n"
          ]
        }
      ],
      "source": [
        "import darknet\n",
        "\n",
        "blocks = darknet.parse_cfg(\"cfg/yolov3.cfg\")\n",
        "print(darknet.create_modules(blocks))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z50xZJawxcai"
      },
      "source": [
        "### Darknet class\n",
        "\n",
        "Let's make our own version of the `Darknet` class in `darknet.py`.\n",
        "\n",
        "The class has two main functions:\n",
        "\n",
        "1. `forward()`: forward propagation, following the instructions in the dictionary modules\n",
        "2. `load_weights()`: load a set of pretrained weights into the network\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YDq_4JRIxcai"
      },
      "outputs": [],
      "source": [
        "from util import *\n",
        "\n",
        "class MyDarknet(nn.Module):\n",
        "    def __init__(self, cfgfile):\n",
        "        super(MyDarknet, self).__init__()\n",
        "        # load the config file and create our model\n",
        "        self.blocks = darknet.parse_cfg(cfgfile)\n",
        "        self.net_info, self.module_list = darknet.create_modules(self.blocks)\n",
        "\n",
        "    def forward(self, x, CUDA:bool):\n",
        "        modules = self.blocks[1:]\n",
        "        outputs = {}   #We cache the outputs for the route layer\n",
        "\n",
        "        write = 0\n",
        "        # run forward propagation. Follow the instruction from dictionary modules\n",
        "        for i, module in enumerate(modules):\n",
        "            module_type = (module[\"type\"])\n",
        "\n",
        "            if module_type == \"convolutional\" or module_type == \"upsample\":\n",
        "                # do convolutional network\n",
        "                x = self.module_list[i](x)\n",
        "\n",
        "            elif module_type == \"route\":\n",
        "                # concat layers\n",
        "                layers = module[\"layers\"]\n",
        "                layers = [int(a) for a in layers]\n",
        "\n",
        "                if (layers[0]) > 0:\n",
        "                    layers[0] = layers[0] - i\n",
        "\n",
        "                if len(layers) == 1:\n",
        "                    x = outputs[i + (layers[0])]\n",
        "\n",
        "                else:\n",
        "                    if (layers[1]) > 0:\n",
        "                        layers[1] = layers[1] - i\n",
        "\n",
        "                    map1 = outputs[i + layers[0]]\n",
        "                    map2 = outputs[i + layers[1]]\n",
        "                    x = torch.cat((map1, map2), 1)\n",
        "\n",
        "\n",
        "            elif  module_type == \"shortcut\":\n",
        "                from_ = int(module[\"from\"])\n",
        "                # residual network\n",
        "                x = outputs[i-1] + outputs[i+from_]\n",
        "\n",
        "            elif module_type == 'yolo':\n",
        "                anchors = self.module_list[i][0].anchors\n",
        "                #Get the input dimensions\n",
        "                inp_dim = int (self.net_info[\"height\"])\n",
        "\n",
        "                #Get the number of classes\n",
        "                num_classes = int (module[\"classes\"])\n",
        "\n",
        "                #Transform\n",
        "                # predict_transform is in util.py\n",
        "                batch_size = x.size(0)\n",
        "                stride =  inp_dim // x.size(2)\n",
        "                grid_size = inp_dim // stride\n",
        "                bbox_attrs = 5 + num_classes\n",
        "                num_anchors = len(anchors)\n",
        "\n",
        "                x = predict_transform(x, inp_dim, anchors, num_classes, CUDA)\n",
        "                if not write:              #if no collector has been intialised.\n",
        "                    detections = x\n",
        "                    write = 1\n",
        "\n",
        "                else:\n",
        "                    detections = torch.cat((detections, x), 1)\n",
        "\n",
        "            outputs[i] = x\n",
        "\n",
        "        return detections\n",
        "\n",
        "\n",
        "    def load_weights(self, weightfile):\n",
        "        '''\n",
        "        Load pretrained weight\n",
        "        '''\n",
        "        #Open the weights file\n",
        "        fp = open(weightfile, \"rb\")\n",
        "\n",
        "        #The first 5 values are header information\n",
        "        # 1. Major version number\n",
        "        # 2. Minor Version Number\n",
        "        # 3. Subversion number\n",
        "        # 4,5. Images seen by the network (during training)\n",
        "        header = np.fromfile(fp, dtype = np.int32, count = 5)\n",
        "        self.header = torch.from_numpy(header)\n",
        "        self.seen = self.header[3]\n",
        "\n",
        "        weights = np.fromfile(fp, dtype = np.float32)\n",
        "\n",
        "        ptr = 0\n",
        "        for i in range(len(self.module_list)):\n",
        "            module_type = self.blocks[i + 1][\"type\"]\n",
        "\n",
        "            #If module_type is convolutional load weights\n",
        "            #Otherwise ignore.\n",
        "\n",
        "            if module_type == \"convolutional\":\n",
        "                model = self.module_list[i]\n",
        "                try:\n",
        "                    batch_normalize = int(self.blocks[i+1][\"batch_normalize\"])\n",
        "                except:\n",
        "                    batch_normalize = 0\n",
        "\n",
        "                conv = model[0]\n",
        "\n",
        "\n",
        "                if (batch_normalize):\n",
        "                    bn = model[1]\n",
        "\n",
        "                    #Get the number of weights of Batch Norm Layer\n",
        "                    num_bn_biases = bn.bias.numel()\n",
        "\n",
        "                    #Load the weights\n",
        "                    bn_biases = torch.from_numpy(weights[ptr:ptr + num_bn_biases])\n",
        "                    ptr += num_bn_biases\n",
        "\n",
        "                    bn_weights = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n",
        "                    ptr  += num_bn_biases\n",
        "\n",
        "                    bn_running_mean = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n",
        "                    ptr  += num_bn_biases\n",
        "\n",
        "                    bn_running_var = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n",
        "                    ptr  += num_bn_biases\n",
        "\n",
        "                    #Cast the loaded weights into dims of model weights.\n",
        "                    bn_biases = bn_biases.view_as(bn.bias.data)\n",
        "                    bn_weights = bn_weights.view_as(bn.weight.data)\n",
        "                    bn_running_mean = bn_running_mean.view_as(bn.running_mean)\n",
        "                    bn_running_var = bn_running_var.view_as(bn.running_var)\n",
        "\n",
        "                    #Copy the data to model\n",
        "                    bn.bias.data.copy_(bn_biases)\n",
        "                    bn.weight.data.copy_(bn_weights)\n",
        "                    bn.running_mean.copy_(bn_running_mean)\n",
        "                    bn.running_var.copy_(bn_running_var)\n",
        "\n",
        "                else:\n",
        "                    #Number of biases\n",
        "                    num_biases = conv.bias.numel()\n",
        "\n",
        "                    #Load the weights\n",
        "                    conv_biases = torch.from_numpy(weights[ptr: ptr + num_biases])\n",
        "                    ptr = ptr + num_biases\n",
        "\n",
        "                    #reshape the loaded weights according to the dims of the model weights\n",
        "                    conv_biases = conv_biases.view_as(conv.bias.data)\n",
        "\n",
        "                    #Finally copy the data\n",
        "                    conv.bias.data.copy_(conv_biases)\n",
        "\n",
        "                #Let us load the weights for the Convolutional layers\n",
        "                num_weights = conv.weight.numel()\n",
        "\n",
        "                #Do the same as above for weights\n",
        "                conv_weights = torch.from_numpy(weights[ptr:ptr+num_weights])\n",
        "                ptr = ptr + num_weights\n",
        "\n",
        "                conv_weights = conv_weights.view_as(conv.weight.data)\n",
        "                conv.weight.data.copy_(conv_weights)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P24rLyq3xcai"
      },
      "source": [
        "### Test Forward Propagation\n",
        "\n",
        "Let's propagate a single image through the network and see what we get."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yf8CluQHxcaj",
        "outputId": "b9084b01-c883-44c7-b1f9-4fadcd974ff9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-19 09:46:06--  https://github.com/ayooshkathuria/pytorch-yolo-v3/raw/master/dog-cycle-car.png\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/ayooshkathuria/pytorch-yolo-v3/master/dog-cycle-car.png [following]\n",
            "--2024-10-19 09:46:06--  https://raw.githubusercontent.com/ayooshkathuria/pytorch-yolo-v3/master/dog-cycle-car.png\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 347445 (339K) [image/png]\n",
            "Saving to: ‘dog-cycle-car.png.1’\n",
            "\n",
            "dog-cycle-car.png.1 100%[===================>] 339.30K  --.-KB/s    in 0.005s  \n",
            "\n",
            "2024-10-19 09:46:07 (65.0 MB/s) - ‘dog-cycle-car.png.1’ saved [347445/347445]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/ayooshkathuria/pytorch-yolo-v3/raw/master/dog-cycle-car.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7uonLDkxcaj"
      },
      "source": [
        "Here's code to load the image into memory and push it through the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Fuu4fNoWxcaj"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "\n",
        "def get_test_input():\n",
        "    img = cv2.imread(\"dog-cycle-car.png\")\n",
        "    img = cv2.resize(img, (416,416))          #Resize to the input dimension\n",
        "    img_ =  img[:,:,::-1].transpose((2,0,1))  # BGR -> RGB | H X W C -> C X H X W\n",
        "    img_ = img_[np.newaxis,:,:,:]/255.0       #Add a channel at 0 (for batch) | Normalise\n",
        "    img_ = torch.from_numpy(img_).float()     #Convert to float\n",
        "    img_ = Variable(img_)                     # Convert to Variable\n",
        "    return img_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znCjGV3Dxcaj"
      },
      "source": [
        "Go ahead and try it (noting that the model hasn't been trained so we don't expect any correct result):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5mkc1T0xcaj",
        "outputId": "1dd22bf5-fff0-4b58-f547-122d8f5db1bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[1.3927e+01, 1.8489e+01, 1.1397e+02,  ..., 4.8350e-01,\n",
            "          5.6331e-01, 5.1292e-01],\n",
            "         [1.7388e+01, 1.5664e+01, 1.2931e+02,  ..., 5.3444e-01,\n",
            "          5.0698e-01, 5.6196e-01],\n",
            "         [1.3788e+01, 1.6116e+01, 3.4173e+02,  ..., 4.0188e-01,\n",
            "          5.7769e-01, 4.2387e-01],\n",
            "         ...,\n",
            "         [4.1091e+02, 4.1140e+02, 1.6161e+01,  ..., 4.9643e-01,\n",
            "          5.2532e-01, 5.4997e-01],\n",
            "         [4.1225e+02, 4.1185e+02, 2.2585e+01,  ..., 4.9896e-01,\n",
            "          5.1473e-01, 5.5245e-01],\n",
            "         [4.1195e+02, 4.1205e+02, 3.0434e+01,  ..., 4.3810e-01,\n",
            "          4.7809e-01, 4.7495e-01]]], grad_fn=<CatBackward0>)\n"
          ]
        }
      ],
      "source": [
        "from util import *\n",
        "\n",
        "model = MyDarknet(\"cfg/yolov3.cfg\")\n",
        "inp = get_test_input()\n",
        "pred = model(inp, False)\n",
        "print (pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtb_14hBxcaj"
      },
      "source": [
        "### Understanding the output result\n",
        "\n",
        "The result from prediction model will be $B(13\\cdot 13 + 26\\cdot 26 + 52 \\cdot 52)3\\cdot85$. Why? We have\n",
        "- $B$: the number of images in the batch\n",
        "- $13\\cdot 13$: number of elements (grid cells) in the coarsest feature map\n",
        "- $26\\cdot 16$: number of elements (grid cells) in the medium scale feature map\n",
        "- $52\\cdot 52$: number of elements (grid cells) in the finest cale feature map\n",
        "- $3$: the number of anchor boxes per grid cell\n",
        "- $85$: number of bounding box attributes (4 for bounding box, 1 for objectness, 80 for the COCO classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s73Z51GYxcaj"
      },
      "source": [
        "### Download a pretrained weight file\n",
        "\n",
        "Darknet stores weights as in this diagram:\n",
        "\n",
        "<img src=\"img/weights.png\" title=\"weight\" style=\"width: 600px;\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JchEKyOAxcak",
        "outputId": "3634c269-1e7f-47f8-cb56-c9bd17271b15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-19 09:46:08--  https://pjreddie.com/media/files/yolov3.weights\n",
            "Resolving pjreddie.com (pjreddie.com)... 162.0.215.52\n",
            "Connecting to pjreddie.com (pjreddie.com)|162.0.215.52|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248007048 (237M) [application/octet-stream]\n",
            "Saving to: ‘yolov3.weights.1’\n",
            "\n",
            "yolov3.weights.1    100%[===================>] 236.52M  16.4MB/s    in 15s     \n",
            "\n",
            "2024-10-19 09:46:25 (15.5 MB/s) - ‘yolov3.weights.1’ saved [248007048/248007048]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://pjreddie.com/media/files/yolov3.weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJ1CyBqWxcak"
      },
      "source": [
        "You can download the yolov3 weights from [here](https://drive.google.com/file/d/1T_rTXbBx8pO-2UeehUSP4R3UN7vGqcAG/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UgbYPvSoxcak"
      },
      "outputs": [],
      "source": [
        "model.load_weights(\"yolov3.weights\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2K3wYtKYxcak"
      },
      "source": [
        "### Test with the sample image again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndJS_GIrxcao",
        "outputId": "23fbedda-65ec-4de9-9dfc-9611051a5882"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[8.5426e+00, 1.9015e+01, 1.1130e+02,  ..., 1.7305e-03,\n",
            "          1.3874e-03, 9.2979e-04],\n",
            "         [1.4105e+01, 1.8868e+01, 9.4014e+01,  ..., 5.9498e-04,\n",
            "          9.2469e-04, 1.3084e-03],\n",
            "         [2.1125e+01, 1.5269e+01, 3.5793e+02,  ..., 8.3604e-03,\n",
            "          5.1065e-03, 5.8559e-03],\n",
            "         ...,\n",
            "         [4.1268e+02, 4.1069e+02, 3.7159e+00,  ..., 1.7188e-06,\n",
            "          4.0959e-06, 6.5904e-07],\n",
            "         [4.1132e+02, 4.1023e+02, 8.0354e+00,  ..., 1.3928e-05,\n",
            "          3.2255e-05, 1.2078e-05],\n",
            "         [4.1076e+02, 4.1318e+02, 4.9634e+01,  ..., 4.2181e-06,\n",
            "          1.0795e-05, 1.8107e-05]]], grad_fn=<CatBackward0>)\n"
          ]
        }
      ],
      "source": [
        "inp = get_test_input()\n",
        "pred = model(inp, False)\n",
        "print (pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SKDSBM1xcao"
      },
      "source": [
        "### From YOLO output tensor to *true* detections\n",
        "\n",
        "In the prediction result, there are many results. We need to threshold them using the objectness score\n",
        "output for each bounding box prediction. The `write_results` function in `util.py` does just that.\n",
        "\n",
        "    def write_results(prediction, confidence, num_classes, nms_conf = 0.4)\n",
        "\n",
        "- prediction: prediction result tensor returned from the YOLO model\n",
        "- confidence: objectness score threshold to apply to the set of detections\n",
        "- num_classes: number of classes to expect\n",
        "- nms_conf: NMS IoU threshold\n",
        "\n",
        "NMS stands for \"non-maxima suppression.\" The basic idea is that if you have two predicted bounding\n",
        "boxes that overlap each other significantly, you should throw away the box with the lower confidence\n",
        "score. Overlap is measured by IoU (Intersection over Union), wich is just the ratio of the the area\n",
        "of intersection of the two regions with the area of the union of the two regions:\n",
        "\n",
        "$$ IoU(R_1,R_2) = \\frac{|R_1 \\cap R_2|}{|R_1 \\cup R_2|}. $$\n",
        "\n",
        "The default of 0.4 means if the intersection is 40% or more of the union, the two bounding boxes\n",
        "are overlapping enough that only one of the detections should survive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCuZ80j0xcao",
        "outputId": "5709c9bd-aa0d-45ef-cd24-12f4776956a4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  0.0000,  61.5397, 100.8604, 307.2721, 303.1128,   0.9469,   0.9985,\n",
              "           1.0000],\n",
              "        [  0.0000, 253.8480,  66.1098, 378.0398, 118.0090,   0.9992,   0.8164,\n",
              "           7.0000],\n",
              "        [  0.0000,  71.0339, 163.2243, 175.7470, 382.2701,   0.9999,   0.9936,\n",
              "          16.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "write_results(pred.detach(), 0.5, 80, nms_conf = 0.4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oayzyIdUxcap"
      },
      "source": [
        "### Show the resulting detections on top of an image\n",
        "\n",
        "The model was trained on the COCO dataset, so download the class label file `coco.names`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODCkGYRXxcap",
        "outputId": "3422a814-db60-4c62-c81f-26e3c28047df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-19 09:46:26--  https://raw.githubusercontent.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/master/data/coco.names\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 625 [text/plain]\n",
            "Saving to: ‘coco.names’\n",
            "\n",
            "coco.names          100%[===================>]     625  --.-KB/s    in 0s      \n",
            "\n",
            "2024-10-19 09:46:26 (46.3 MB/s) - ‘coco.names’ saved [625/625]\n",
            "\n",
            "mkdir: cannot create directory ‘data’: File exists\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/master/data/coco.names\n",
        "!mkdir data\n",
        "!mv coco.names data/coco.names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "EDSlaAuKxcap"
      },
      "outputs": [],
      "source": [
        "def load_classes(namesfile):\n",
        "    fp = open(namesfile, \"r\")\n",
        "    names = fp.read().split(\"\\n\")[:-1]\n",
        "    return names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bffjugcfxcap",
        "outputId": "da183168-749b-4270-b814-bf6576462d69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['person', 'bicycle', 'car', 'motorbike', 'aeroplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'sofa', 'pottedplant', 'bed', 'diningtable', 'toilet', 'tvmonitor', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n"
          ]
        }
      ],
      "source": [
        "num_classes = 80\n",
        "classes = load_classes(\"data/coco.names\")\n",
        "print(classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wh0InEiExcap"
      },
      "source": [
        "So we see that the three surviving bounding boxes above, outputting object types 1, 7, and 16, indicate a bicycle, a truck, and a dog.\n",
        "Let's draw the detections on top of the input image for better visualization.\n",
        "\n",
        "We'll use some code based on Kathuria's `detect.py`. You can download the original as"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxGODhhXxcap",
        "outputId": "9d5bccad-c40d-48ce-db86-b37e4283327d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-19 09:46:27--  https://raw.githubusercontent.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/master/detect.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7273 (7.1K) [text/plain]\n",
            "Saving to: ‘detect.py.1’\n",
            "\n",
            "detect.py.1         100%[===================>]   7.10K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-10-19 09:46:27 (93.1 MB/s) - ‘detect.py.1’ saved [7273/7273]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/master/detect.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNzTCKj4xcap"
      },
      "source": [
        "Here's our version. It will process the images in subdirectory `cocoimages` so let's make it and put our sample there:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "4KNFaMbLxcap"
      },
      "outputs": [],
      "source": [
        "!mkdir -p cocoimages\n",
        "!cp dog-cycle-car.png cocoimages/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQcJtwGexcap",
        "outputId": "7cd9d0db-5692-4bed-b981-39e315e5aae0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading network.....\n",
            "Network successfully loaded\n",
            "dog-cycle-car.png    predicted in  0.087 seconds\n",
            "Objects Detected:    bicycle truck dog\n",
            "----------------------------------------------------------\n",
            "SUMMARY\n",
            "----------------------------------------------------------\n",
            "Task                     : Time Taken (in seconds)\n",
            "\n",
            "Reading addresses        : 0.000\n",
            "Loading batch            : 0.016\n",
            "Detection (1 images)     : 0.809\n",
            "Output Processing        : 0.000\n",
            "Drawing Boxes            : 0.016\n",
            "Average time_per_img     : 0.841\n",
            "----------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from __future__ import division\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import cv2\n",
        "from util import *\n",
        "import argparse\n",
        "import os\n",
        "import os.path as osp\n",
        "from darknet import Darknet\n",
        "import pickle as pkl\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "images = \"cocoimages\"\n",
        "batch_size = 4\n",
        "confidence = 0.5\n",
        "nms_thesh = 0.4\n",
        "start = 0\n",
        "CUDA = torch.cuda.is_available()\n",
        "\n",
        "num_classes = 80\n",
        "classes = load_classes(\"data/coco.names\")\n",
        "\n",
        "#Set up the neural network\n",
        "\n",
        "print(\"Loading network.....\")\n",
        "model = MyDarknet(\"cfg/yolov3.cfg\")\n",
        "model.load_weights(\"yolov3.weights\")\n",
        "print(\"Network successfully loaded\")\n",
        "\n",
        "model.net_info[\"height\"] = 416\n",
        "inp_dim = int(model.net_info[\"height\"])\n",
        "assert inp_dim % 32 == 0\n",
        "assert inp_dim > 32\n",
        "\n",
        "#If there's a GPU availible, put the model on GPU\n",
        "\n",
        "if CUDA:\n",
        "    model.cuda()\n",
        "\n",
        "# Set the model in evaluation mode\n",
        "\n",
        "model.eval()\n",
        "\n",
        "read_dir = time.time()\n",
        "\n",
        "# Detection phase\n",
        "\n",
        "try:\n",
        "    imlist = [osp.join(osp.realpath('.'), images, img) for img in os.listdir(images)]\n",
        "except NotADirectoryError:\n",
        "    imlist = []\n",
        "    imlist.append(osp.join(osp.realpath('.'), images))\n",
        "except FileNotFoundError:\n",
        "    print (\"No file or directory with the name {}\".format(images))\n",
        "    exit()\n",
        "\n",
        "if not os.path.exists(\"des\"):\n",
        "    os.makedirs(\"des\")\n",
        "\n",
        "load_batch = time.time()\n",
        "loaded_ims = [cv2.imread(x) for x in imlist]\n",
        "\n",
        "im_batches = list(map(prep_image, loaded_ims, [inp_dim for x in range(len(imlist))]))\n",
        "im_dim_list = [(x.shape[1], x.shape[0]) for x in loaded_ims]\n",
        "im_dim_list = torch.FloatTensor(im_dim_list).repeat(1,2)\n",
        "\n",
        "\n",
        "leftover = 0\n",
        "if (len(im_dim_list) % batch_size):\n",
        "    leftover = 1\n",
        "\n",
        "if batch_size != 1:\n",
        "    num_batches = len(imlist) // batch_size + leftover\n",
        "    im_batches = [torch.cat((im_batches[i*batch_size : min((i +  1)*batch_size,\n",
        "                        len(im_batches))]))  for i in range(num_batches)]\n",
        "\n",
        "write = 0\n",
        "\n",
        "if CUDA:\n",
        "    im_dim_list = im_dim_list.cuda()\n",
        "\n",
        "start_det_loop = time.time()\n",
        "for i, batch in enumerate(im_batches):\n",
        "    # Load the image\n",
        "    start = time.time()\n",
        "    if CUDA:\n",
        "        batch = batch.cuda()\n",
        "    with torch.no_grad():\n",
        "        prediction = model(Variable(batch), CUDA)\n",
        "\n",
        "    prediction = write_results(prediction, confidence, num_classes, nms_conf = nms_thesh)\n",
        "\n",
        "    end = time.time()\n",
        "\n",
        "    if type(prediction) == int:\n",
        "\n",
        "        for im_num, image in enumerate(imlist[i*batch_size: min((i +  1)*batch_size, len(imlist))]):\n",
        "            im_id = i*batch_size + im_num\n",
        "            print(\"{0:20s} predicted in {1:6.3f} seconds\".format(image.split(\"/\")[-1], (end - start)/batch_size))\n",
        "            print(\"{0:20s} {1:s}\".format(\"Objects Detected:\", \"\"))\n",
        "            print(\"----------------------------------------------------------\")\n",
        "        continue\n",
        "\n",
        "    prediction[:,0] += i*batch_size    #transform the atribute from index in batch to index in imlist\n",
        "\n",
        "    if not write:                      #If we have't initialised output\n",
        "        output = prediction\n",
        "        write = 1\n",
        "    else:\n",
        "        output = torch.cat((output,prediction))\n",
        "\n",
        "    for im_num, image in enumerate(imlist[i*batch_size: min((i +  1)*batch_size, len(imlist))]):\n",
        "        im_id = i*batch_size + im_num\n",
        "        objs = [classes[int(x[-1])] for x in output if int(x[0]) == im_id]\n",
        "        print(\"{0:20s} predicted in {1:6.3f} seconds\".format(image.split(\"/\")[-1], (end - start)/batch_size))\n",
        "        print(\"{0:20s} {1:s}\".format(\"Objects Detected:\", \" \".join(objs)))\n",
        "        print(\"----------------------------------------------------------\")\n",
        "\n",
        "    if CUDA:\n",
        "        torch.cuda.synchronize()\n",
        "try:\n",
        "    output\n",
        "except NameError:\n",
        "    print (\"No detections were made\")\n",
        "    exit()\n",
        "\n",
        "im_dim_list = torch.index_select(im_dim_list, 0, output[:,0].long())\n",
        "\n",
        "scaling_factor = torch.min(416/im_dim_list,1)[0].view(-1,1)\n",
        "\n",
        "output[:,[1,3]] -= (inp_dim - scaling_factor*im_dim_list[:,0].view(-1,1))/2\n",
        "output[:,[2,4]] -= (inp_dim - scaling_factor*im_dim_list[:,1].view(-1,1))/2\n",
        "\n",
        "output[:,1:5] /= scaling_factor\n",
        "\n",
        "for i in range(output.shape[0]):\n",
        "    output[i, [1,3]] = torch.clamp(output[i, [1,3]], 0.0, im_dim_list[i,0])\n",
        "    output[i, [2,4]] = torch.clamp(output[i, [2,4]], 0.0, im_dim_list[i,1])\n",
        "\n",
        "output_recast = time.time()\n",
        "class_load = time.time()\n",
        "colors = [[255, 0, 0], [255, 0, 0], [255, 255, 0], [0, 255, 0], [0, 255, 255], [0, 0, 255], [255, 0, 255]]\n",
        "\n",
        "draw = time.time()\n",
        "\n",
        "def write(x, results):\n",
        "    c1 = tuple(x[1:3].int())\n",
        "    c2 = tuple(x[3:5].int())\n",
        "    img = results[int(x[0])]\n",
        "    cls = int(x[-1])\n",
        "    color = random.choice(colors)\n",
        "    label = \"{0}\".format(classes[cls])\n",
        "    cv2.rectangle(img, c1, c2,color, 1)\n",
        "    t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 1 , 1)[0]\n",
        "    c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4\n",
        "    cv2.rectangle(img, c1, c2,color, -1)\n",
        "    cv2.putText(img, label, (c1[0], c1[1] + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1, [225,255,255], 1);\n",
        "    return img\n",
        "\n",
        "\n",
        "# list(map(lambda x: write(x, loaded_ims), output))\n",
        "\n",
        "det_names = pd.Series(imlist).apply(lambda x: \"{}/det_{}\".format(\"des\",x.split(\"/\")[-1]))\n",
        "\n",
        "list(map(cv2.imwrite, det_names, loaded_ims))\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print(\"SUMMARY\")\n",
        "print(\"----------------------------------------------------------\")\n",
        "print(\"{:25s}: {}\".format(\"Task\", \"Time Taken (in seconds)\"))\n",
        "print()\n",
        "print(\"{:25s}: {:2.3f}\".format(\"Reading addresses\", load_batch - read_dir))\n",
        "print(\"{:25s}: {:2.3f}\".format(\"Loading batch\", start_det_loop - load_batch))\n",
        "print(\"{:25s}: {:2.3f}\".format(\"Detection (\" + str(len(imlist)) +  \" images)\", output_recast - start_det_loop))\n",
        "print(\"{:25s}: {:2.3f}\".format(\"Output Processing\", class_load - output_recast))\n",
        "print(\"{:25s}: {:2.3f}\".format(\"Drawing Boxes\", end - draw))\n",
        "print(\"{:25s}: {:2.3f}\".format(\"Average time_per_img\", (end - load_batch)/len(imlist)))\n",
        "print(\"----------------------------------------------------------\")\n",
        "\n",
        "\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHDCyzf6xcaq"
      },
      "source": [
        "Voila! You got the YOLO result in folder \"des\"\n",
        "\n",
        "<img src=\"img/dogresult.png\" title=\"weight\" style=\"width: 600px;\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6hIplgixcaq"
      },
      "source": [
        "### Training in YOLOv3\n",
        "\n",
        "Now let's create training function. There are 2 parts:\n",
        " - Dataset\n",
        " - Training\n",
        "\n",
        "#### Dataset\n",
        "\n",
        "You can download COCO dataset from [COCO dataset websit](https://cocodataset.org/#home).\n",
        "If you want to create dataset, you must create an images folder and put annotation file in json coco format in another position.\n",
        "\n",
        "For your customdataset, there are many application can create the annotation file in coco format. For example: [makesense](https://www.makesense.ai/)\n",
        "\n",
        "Put the folder name in code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3OpnLsDxcaq"
      },
      "outputs": [],
      "source": [
        "path2data_train=\"/path/to/train/images/\"\n",
        "path2json_train=\"/path/to/train/coco_annotation.json\"\n",
        "\n",
        "path2data_val=\"/path/to/validate/images/\"\n",
        "path2json_val=\"/path/to/validate/coco_annotation.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VX_PtJCxcaq"
      },
      "outputs": [],
      "source": [
        "# path2data_train=\"/home/alisa/fiftyone/coco-2017/train2017/data/\"\n",
        "# path2json_train=\"/home/alisa/fiftyone/coco-2017/annotations/instances_train2017.json\"\n",
        "\n",
        "# path2data_val=\"/home/alisa/fiftyone/coco-2017/val2017/data\"\n",
        "# path2json_val=\"/home/alisa/fiftyone/coco-2017/annotations/instances_val2017.json\"\n",
        "\n",
        "\n",
        "path2data_train=\"/home/alisa/fiftyone/coco-2017/train2017/data/\"\n",
        "path2json_train=\"/home/alisa/fiftyone/coco-2017/annotations/instances_train2017.json\"\n",
        "\n",
        "path2data_val=\"/home/alisa/fiftyone/coco-2017/val2017/data\"\n",
        "path2json_val=\"/home/alisa/fiftyone/coco-2017/annotations/instances_val2017.json\"\n",
        "\n",
        "img_size = 416"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBToA9pAxcaq"
      },
      "source": [
        "Create custom dataset in coco format. The important thing of the dataset is annotation position. You need to create annotation in each image into yolo format. Thus, in class CustomCoco, it contains *create_label* function for convert bounding box in each class into yolo label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "Z6mYyd_Zxcaq",
        "outputId": "c47c45f4-a484-4feb-b84c-38fc2a020a8d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'img_size' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-d6c8814dbc83>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mSTRIDES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mIP_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mNUM_ANCHORS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mNUM_CLASSES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'img_size' is not defined"
          ]
        }
      ],
      "source": [
        "# coding=utf-8\n",
        "import os\n",
        "import sys\n",
        "from PIL import Image\n",
        "import torch\n",
        "import math\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "from torchvision.datasets import CocoDetection\n",
        "\n",
        "from typing import Any, Callable, Optional, Tuple\n",
        "import json\n",
        "\n",
        "ANCHORS = [\n",
        "            [[12, 16], [19, 36], [40, 28]],\n",
        "            [[36, 75], [76, 55], [72, 146]],\n",
        "            [[142, 110], [192, 243], [459, 401]]\n",
        "]\n",
        "\n",
        "STRIDES = [8, 16, 32]\n",
        "\n",
        "IP_SIZE = img_size\n",
        "NUM_ANCHORS = 3\n",
        "NUM_CLASSES = 80\n",
        "\n",
        "with open('coco_cats.json') as js:\n",
        "    data = json.load(js)[\"categories\"]\n",
        "\n",
        "cats_dict = {}\n",
        "for i in range(0, 80):\n",
        "    cats_dict[str(data[i]['id'])] = i\n",
        "\n",
        "\n",
        "\n",
        "class CustomCoco(CocoDetection):\n",
        "    def __init__(\n",
        "            self,\n",
        "            root: str,\n",
        "            annFile: str,\n",
        "            transform: Optional[Callable] = None,\n",
        "            target_transform: Optional[Callable] = None,\n",
        "            transforms: Optional[Callable] = None,\n",
        "    ) -> None:\n",
        "        super(CocoDetection, self).__init__(root, transforms, transform, target_transform)\n",
        "        from pycocotools.coco import COCO\n",
        "        self.coco = COCO(annFile)\n",
        "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
        "\n",
        "\n",
        "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "\n",
        "        Returns:\n",
        "            tuple: Tuple (image, target). target is the object returned by ``coco.loadAnns``.\n",
        "        \"\"\"\n",
        "        coco = self.coco\n",
        "        img_id = self.ids[index]\n",
        "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
        "        target = coco.loadAnns(ann_ids)\n",
        "\n",
        "        path = coco.loadImgs(img_id)[0]['file_name']\n",
        "\n",
        "        img = Image.open(os.path.join(self.root, path)).convert('RGB')\n",
        "        img = np.array(img)\n",
        "\n",
        "        labels = list(obj['category_id'] for obj in target)\n",
        "        bboxes = list(obj['bbox'] for obj in target)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            bboxes = list(obj['bbox'] for obj in target)\n",
        "            category_ids = list(obj['category_id'] for obj in target)\n",
        "            transformed = self.transform(image=img, bboxes=bboxes, category_ids=category_ids)\n",
        "            img = transformed['image'],\n",
        "            bboxes = torch.Tensor(transformed['bboxes'])\n",
        "            cat_ids = torch.Tensor(transformed['category_ids'])\n",
        "            labels, bboxes = self.__create_label(bboxes, cat_ids.type(torch.IntTensor))\n",
        "\n",
        "        return img, labels, bboxes\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __create_label(self, bboxes, class_inds):\n",
        "        \"\"\"\n",
        "        Label assignment. For a single picture all GT box bboxes are assigned anchor.\n",
        "        1、Select a bbox in order, convert its coordinates(\"xyxy\") to \"xywh\"; and scale bbox'\n",
        "           xywh by the strides.\n",
        "        2、Calculate the iou between the each detection layer'anchors and the bbox in turn, and select the largest\n",
        "            anchor to predict the bbox.If the ious of all detection layers are smaller than 0.3, select the largest\n",
        "            of all detection layers' anchors to predict the bbox.\n",
        "        Note :\n",
        "        1、The same GT may be assigned to multiple anchors. And the anchors may be on the same or different layer.\n",
        "        2、The total number of bboxes may be more than it is, because the same GT may be assigned to multiple layers\n",
        "        of detection.\n",
        "        \"\"\"\n",
        "        # print(\"Class indices: \", class_inds)\n",
        "        bboxes = np.array(bboxes)\n",
        "        class_inds = np.array(class_inds)\n",
        "        anchors = ANCHORS # all the anchors\n",
        "        strides = np.array(STRIDES) # list of strides\n",
        "        train_output_size = IP_SIZE / strides # image with different scales\n",
        "        anchors_per_scale = NUM_ANCHORS # anchor per scale\n",
        "\n",
        "        label = [\n",
        "            np.zeros(\n",
        "                (\n",
        "                    int(train_output_size[i]),\n",
        "                    int(train_output_size[i]),\n",
        "                    anchors_per_scale,\n",
        "                    5 + NUM_CLASSES,\n",
        "                )\n",
        "            )\n",
        "            for i in range(3)\n",
        "        ]\n",
        "        # 150 bounding box ground truths per scale\n",
        "        bboxes_xywh = [\n",
        "            np.zeros((150, 4)) for _ in range(3)\n",
        "        ]  # Darknet the max_num is 30\n",
        "        bbox_count = np.zeros((3,))\n",
        "\n",
        "        for i in range(len(bboxes)):\n",
        "            bbox_coor = bboxes[i][:4]\n",
        "            bbox_class_ind = cats_dict[str(class_inds[i])]\n",
        "\n",
        "            # onehot\n",
        "            one_hot = np.zeros(NUM_CLASSES, dtype=np.float32)\n",
        "            one_hot[bbox_class_ind] = 1.0\n",
        "            # one_hot_smooth = dataAug.LabelSmooth()(one_hot, self.num_classes)\n",
        "\n",
        "            # convert \"xyxy\" to \"xywh\"\n",
        "            bbox_xywh = np.concatenate(\n",
        "                [\n",
        "                    (0.5 * bbox_coor[2:] + bbox_coor[:2]) ,\n",
        "                    bbox_coor[2:],\n",
        "                ],\n",
        "                axis=-1,\n",
        "            )\n",
        "\n",
        "            bbox_xywh_scaled = (\n",
        "                1.0 * bbox_xywh[np.newaxis, :] / strides[:, np.newaxis]\n",
        "            )\n",
        "\n",
        "            iou = []\n",
        "            exist_positive = False\n",
        "            for i in range(3):\n",
        "                anchors_xywh = np.zeros((anchors_per_scale, 4))\n",
        "                anchors_xywh[:, 0:2] = (\n",
        "                    np.floor(bbox_xywh_scaled[i, 0:2]).astype(np.int32) + 0.5\n",
        "                )  # 0.5 for compensation\n",
        "\n",
        "                # assign all anchors\n",
        "                anchors_xywh[:, 2:4] = anchors[i]\n",
        "\n",
        "                iou_scale = iou_xywh_numpy(\n",
        "                    bbox_xywh_scaled[i][np.newaxis, :], anchors_xywh\n",
        "                )\n",
        "                iou.append(iou_scale)\n",
        "                iou_mask = iou_scale > 0.3\n",
        "\n",
        "                if np.any(iou_mask):\n",
        "                    xind, yind = np.floor(bbox_xywh_scaled[i, 0:2]).astype(\n",
        "                        np.int32\n",
        "                    )\n",
        "\n",
        "                    label[i][yind, xind, iou_mask, 0:4] = bbox_xywh * strides[i]\n",
        "                    label[i][yind, xind, iou_mask, 4:5] = 1.0\n",
        "                    label[i][yind, xind, iou_mask, 5:] = one_hot\n",
        "\n",
        "                    bbox_ind = int(bbox_count[i] % 150)  # BUG : 150为一个先验值,内存消耗大\n",
        "                    bboxes_xywh[i][bbox_ind, :4] = bbox_xywh * strides[i]\n",
        "                    bbox_count[i] += 1\n",
        "\n",
        "                    exist_positive = True\n",
        "\n",
        "            if not exist_positive:\n",
        "                # check if a ground truth bb have the best anchor with any scale\n",
        "                best_anchor_ind = np.argmax(np.array(iou).reshape(-1), axis=-1)\n",
        "                best_detect = int(best_anchor_ind / anchors_per_scale)\n",
        "                best_anchor = int(best_anchor_ind % anchors_per_scale)\n",
        "\n",
        "                xind, yind = np.floor(\n",
        "                    bbox_xywh_scaled[best_detect, 0:2]\n",
        "                ).astype(np.int32)\n",
        "\n",
        "                label[best_detect][yind, xind, best_anchor, 0:4] = bbox_xywh * strides[best_detect]\n",
        "                label[best_detect][yind, xind, best_anchor, 4:5] = 1.0\n",
        "                # label[best_detect][yind, xind, best_anchor, 5:6] = bbox_mix\n",
        "                label[best_detect][yind, xind, best_anchor, 5:] = one_hot\n",
        "\n",
        "                bbox_ind = int(bbox_count[best_detect] % 150)\n",
        "                bboxes_xywh[best_detect][bbox_ind, :4] = bbox_xywh * strides[best_detect]\n",
        "                bbox_count[best_detect] += 1\n",
        "\n",
        "        flatten_size_s = int(train_output_size[2]) * int(train_output_size[2]) * anchors_per_scale\n",
        "        flatten_size_m = int(train_output_size[1]) * int(train_output_size[1]) * anchors_per_scale\n",
        "        flatten_size_l = int(train_output_size[0]) * int(train_output_size[0]) * anchors_per_scale\n",
        "\n",
        "        label_s = torch.Tensor(label[2]).view(1, flatten_size_s, 5 + NUM_CLASSES).squeeze(0)\n",
        "        label_m = torch.Tensor(label[1]).view(1, flatten_size_m, 5 + NUM_CLASSES).squeeze(0)\n",
        "        label_l = torch.Tensor(label[0]).view(1, flatten_size_l, 5 + NUM_CLASSES).squeeze(0)\n",
        "\n",
        "        bboxes_s = torch.Tensor(bboxes_xywh[2])\n",
        "        bboxes_m = torch.Tensor(bboxes_xywh[1])\n",
        "        bboxes_l = torch.Tensor(bboxes_xywh[0])\n",
        "\n",
        "        # label_sbbox, label_mbbox, label_lbbox = label\n",
        "        sbboxes, mbboxes, lbboxes = bboxes_xywh\n",
        "        # print(\"label\")\n",
        "        labels = torch.cat([label_l, label_m, label_s], 0)\n",
        "        bboxes = torch.cat([bboxes_l, bboxes_m, bboxes_s], 0)\n",
        "        return labels, bboxes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6Q2Qa1Lxcar"
      },
      "source": [
        "### There still are two important functions for training:\n",
        "- iou_xywh_numpy: calculate the iou (intersect over union) boxes between boxes1 and boxes2. The function has been used in CustomCoco dataset\n",
        "- CIOU_xywh_torch: calculate IOU from prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "3pBJ5kFaxcar"
      },
      "outputs": [],
      "source": [
        "def iou_xywh_numpy(boxes1, boxes2):\n",
        "    boxes1 = np.array(boxes1)\n",
        "    boxes2 = np.array(boxes2)\n",
        "    # print(boxes1, boxes2)\n",
        "\n",
        "    boxes1_area = boxes1[..., 2] * boxes1[..., 3]\n",
        "    boxes2_area = boxes2[..., 2] * boxes2[..., 3]\n",
        "\n",
        "    boxes1 = np.concatenate([boxes1[..., :2] - boxes1[..., 2:] * 0.5,\n",
        "                                boxes1[..., :2] + boxes1[..., 2:] * 0.5], axis=-1)\n",
        "    boxes2 = np.concatenate([boxes2[..., :2] - boxes2[..., 2:] * 0.5,\n",
        "                                boxes2[..., :2] + boxes2[..., 2:] * 0.5], axis=-1)\n",
        "\n",
        "    left_up = np.maximum(boxes1[..., :2], boxes2[..., :2])\n",
        "    right_down = np.minimum(boxes1[..., 2:], boxes2[..., 2:])\n",
        "\n",
        "    inter_section = np.maximum(right_down - left_up, 0.0)\n",
        "    inter_area = inter_section[..., 0] * inter_section[..., 1]\n",
        "    union_area = boxes1_area + boxes2_area - inter_area\n",
        "    IOU = 1.0 * inter_area / union_area\n",
        "    return IOU\n",
        "\n",
        "\n",
        "def CIOU_xywh_torch(boxes1,boxes2):\n",
        "    '''\n",
        "    cal CIOU of two boxes or batch boxes\n",
        "    :param boxes1:[xmin,ymin,xmax,ymax] or\n",
        "                [[xmin,ymin,xmax,ymax],[xmin,ymin,xmax,ymax],...]\n",
        "    :param boxes2:[xmin,ymin,xmax,ymax]\n",
        "    :return:\n",
        "    '''\n",
        "    # cx cy w h->xyxy\n",
        "    boxes1 = torch.cat([boxes1[..., :2] - boxes1[..., 2:] * 0.5,\n",
        "                        boxes1[..., :2] + boxes1[..., 2:] * 0.5], dim=-1)\n",
        "    boxes2 = torch.cat([boxes2[..., :2] - boxes2[..., 2:] * 0.5,\n",
        "                        boxes2[..., :2] + boxes2[..., 2:] * 0.5], dim=-1)\n",
        "\n",
        "    boxes1 = torch.cat([torch.min(boxes1[..., :2], boxes1[..., 2:]),\n",
        "                        torch.max(boxes1[..., :2], boxes1[..., 2:])], dim=-1)\n",
        "    boxes2 = torch.cat([torch.min(boxes2[..., :2], boxes2[..., 2:]),\n",
        "                        torch.max(boxes2[..., :2], boxes2[..., 2:])], dim=-1)\n",
        "\n",
        "    # (x2 minus x1 = width)  * (y2 - y1 = height)\n",
        "    boxes1_area = (boxes1[..., 2] - boxes1[..., 0]) * (boxes1[..., 3] - boxes1[..., 1])\n",
        "    boxes2_area = (boxes2[..., 2] - boxes2[..., 0]) * (boxes2[..., 3] - boxes2[..., 1])\n",
        "\n",
        "    # upper left of the intersection region (x,y)\n",
        "    inter_left_up = torch.max(boxes1[..., :2], boxes2[..., :2])\n",
        "\n",
        "    # bottom right of the intersection region (x,y)\n",
        "    inter_right_down = torch.min(boxes1[..., 2:], boxes2[..., 2:])\n",
        "\n",
        "    # if there is overlapping we will get (w,h) else set to (0,0) because it could be negative if no overlapping\n",
        "    inter_section = torch.max(inter_right_down - inter_left_up, torch.zeros_like(inter_right_down))\n",
        "    inter_area = inter_section[..., 0] * inter_section[..., 1]\n",
        "    union_area = boxes1_area + boxes2_area - inter_area\n",
        "    ious = 1.0 * inter_area / union_area\n",
        "\n",
        "    # cal outer boxes\n",
        "    outer_left_up = torch.min(boxes1[..., :2], boxes2[..., :2])\n",
        "    outer_right_down = torch.max(boxes1[..., 2:], boxes2[..., 2:])\n",
        "    outer = torch.max(outer_right_down - outer_left_up, torch.zeros_like(inter_right_down))\n",
        "    outer_diagonal_line = torch.pow(outer[..., 0], 2) + torch.pow(outer[..., 1], 2)\n",
        "\n",
        "    # cal center distance\n",
        "    # center x center y\n",
        "    boxes1_center = (boxes1[..., :2] +  boxes1[...,2:]) * 0.5\n",
        "    boxes2_center = (boxes2[..., :2] +  boxes2[...,2:]) * 0.5\n",
        "\n",
        "    # euclidean distance\n",
        "    # x1-x2 square\n",
        "    center_dis = torch.pow(boxes1_center[...,0]-boxes2_center[...,0], 2) +\\\n",
        "                 torch.pow(boxes1_center[...,1]-boxes2_center[...,1], 2)\n",
        "\n",
        "    # cal penalty term\n",
        "    # cal width,height\n",
        "    boxes1_size = torch.max(boxes1[..., 2:] - boxes1[..., :2], torch.zeros_like(inter_right_down))\n",
        "    boxes2_size = torch.max(boxes2[..., 2:] - boxes2[..., :2], torch.zeros_like(inter_right_down))\n",
        "    v = (4 / (math.pi ** 2)) * torch.pow(\n",
        "            torch.atan((boxes1_size[...,0]/torch.clamp(boxes1_size[...,1],min = 1e-6))) -\n",
        "            torch.atan((boxes2_size[..., 0] / torch.clamp(boxes2_size[..., 1],min = 1e-6))), 2)\n",
        "\n",
        "    alpha = v / (1-ious+v)\n",
        "\n",
        "    #cal ciou\n",
        "    cious = ious - (center_dis / outer_diagonal_line + alpha*v)\n",
        "\n",
        "    return cious"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztSg0Eprxcar"
      },
      "source": [
        "#### Training\n",
        "\n",
        "After finished your custom dataset code, let's train your data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ERZltjAxcar",
        "outputId": "e6607293-c320-4839-cf6c-3cf7bb618405"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.18 (you have 1.4.15). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n"
          ]
        }
      ],
      "source": [
        "from __future__ import division\n",
        "import time\n",
        "import os\n",
        "import os.path as osp\n",
        "import numpy as np\n",
        "import cv2\n",
        "import pickle as pkl\n",
        "import pandas as pd\n",
        "import random\n",
        "from copy import copy, deepcopy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Subset\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "from util import *\n",
        "\n",
        "import albumentations as A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ohsvi0zxcas"
      },
      "source": [
        "Select the device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2bMe4mNxcas"
      },
      "outputs": [],
      "source": [
        "# Set device to GPU or CPU\n",
        "gpu = \"0\"\n",
        "device = torch.device(\"cuda:{}\".format(gpu) if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b9mGfS8xcas"
      },
      "source": [
        "#### Load dataset\n",
        "Make image transform and load train dataset. We have used the albumentations class which can transform box parameters automatically, but because we want to test the training coco in yolov3, the transform of training is only resize to $416 \\times 416$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yF3i7dHpxcas"
      },
      "outputs": [],
      "source": [
        "train_transform = A.Compose([\n",
        "    #A.SmallestMaxSize(256),\n",
        "    A.Resize(img_size, img_size),\n",
        "    # A.RandomCrop(width=224, height=224),\n",
        "    # A.HorizontalFlip(p=0.5),\n",
        "    # A.RandomBrightnessContrast(p=0.2),\n",
        "], bbox_params=A.BboxParams(format='coco', label_fields=['category_ids']),\n",
        ")\n",
        "\n",
        "eval_transform = A.Compose([\n",
        "    A.Resize(img_size, img_size),\n",
        "    #A.SmallestMaxSize(256),\n",
        "    #A.CenterCrop(width=224, height=224),\n",
        "], bbox_params=A.BboxParams(format='coco', label_fields=['category_ids']),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMcP7cSWxcas"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 10\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "train_dataset = Subset(CustomCoco(root = path2data_train,\n",
        "                                annFile = path2json_train, transform=train_transform), list(range(0,20)))\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                                               shuffle=True, num_workers=0, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDIqBtIaxcas"
      },
      "source": [
        "#### Load Yolov3 model\n",
        "\n",
        "If you want to train the model with pretrained network, load weights from the example above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ye_emnvxcas"
      },
      "outputs": [],
      "source": [
        "print(\"Loading network.....\")\n",
        "model = MyDarknet(\"cfg/yolov3.cfg\")\n",
        "# load pretrained\n",
        "# model.load_weights(\"yolov3.weights\")\n",
        "print(\"Network successfully loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tjd5v7kxcas"
      },
      "source": [
        "#### Training code\n",
        "\n",
        "Create the training code. The important part of yolo training is you must convert and calculate the iou correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "85x7sUFKxcat"
      },
      "outputs": [],
      "source": [
        "def run_training(model, optimizer, dataloader, device, img_size, n_epoch, every_n_batch, every_n_epoch, ckpt_dir):\n",
        "    for epoch_i in range(n_epoch):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels, bboxes in dataloader:\n",
        "            inputs = torch.from_numpy(np.array(inputs)).squeeze(1).float()\n",
        "            inputs = inputs.to(device)\n",
        "\n",
        "            # Initialize lists to hold the tensor labels and bounding boxes for the current batch\n",
        "            label_tensors = []\n",
        "            box_tensors = []\n",
        "\n",
        "            for label in labels:\n",
        "                # Convert each label into a tensor and move to device\n",
        "                label_tensor = torch.tensor(label).to(device)\n",
        "                label_tensors.append(label_tensor)\n",
        "\n",
        "            for bbox in bboxes:\n",
        "                # Convert each bounding box into a tensor and move to device\n",
        "                bbox_tensor = torch.tensor(bbox).to(device)\n",
        "                box_tensors.append(bbox_tensor)\n",
        "\n",
        "            # At this point, label_tensors is a list of tensors\n",
        "            # You can process these lists as needed\n",
        "\n",
        "            # Perform the model forward pass and loss calculation\n",
        "            optimizer.zero_grad()\n",
        "            with torch.set_grad_enabled(True):\n",
        "                outputs = model(inputs, True)\n",
        "\n",
        "                pred_xywh = outputs[..., 0:4] / img_size\n",
        "                pred_conf = outputs[..., 4:5]\n",
        "                pred_cls = outputs[..., 5:]\n",
        "\n",
        "                # Prepare the target values\n",
        "                label_xywh = [label[:, :4] / img_size for label in label_tensors]  # Normalize the xywh\n",
        "                label_obj_mask = [label[:, 4:5] for label in label_tensors]\n",
        "                label_cls = [label[:, 5:] for label in label_tensors]\n",
        "\n",
        "                # Compute the losses for each image in the batch\n",
        "                loss = 0\n",
        "                for i in range(len(label_tensors)):\n",
        "                    lambda_coord = 0.001\n",
        "                    lambda_noobj = 0.05\n",
        "                    # Calculate losses for the i-th image\n",
        "                    loss_coord = lambda_coord * label_obj_mask[i] * nn.MSELoss()(pred_xywh[i], label_xywh[i])\n",
        "                    loss_conf = (label_obj_mask[i] * nn.BCELoss()(pred_conf[i], label_obj_mask[i])) + \\\n",
        "                                (lambda_noobj * (1.0 - label_obj_mask[i]) * nn.BCELoss()(pred_conf[i], label_obj_mask[i]))\n",
        "                    loss_cls = label_obj_mask[i] * nn.BCELoss()(pred_cls[i], label_cls[i])\n",
        "\n",
        "                    loss += loss_coord + loss_conf + loss_cls\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # Statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(dataloader.dataset)\n",
        "        print(epoch_loss)\n",
        "        print('End Epoch')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVxs8Gwjxcat"
      },
      "source": [
        "Let's train it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrSFCa50xcat"
      },
      "outputs": [],
      "source": [
        "model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "n_epoch = 5\n",
        "img_size = 416\n",
        "save_every_batch = False\n",
        "save_every_epoch = True\n",
        "ckpt_dir = \"../checkpoints\"\n",
        "\n",
        "\n",
        "run_training(model, optimizer, train_dataloader, device,\n",
        "                 img_size,\n",
        "                 n_epoch,\n",
        "                 save_every_batch,\n",
        "                 save_every_epoch,\n",
        "                 ckpt_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYxGuCpHxcat"
      },
      "source": [
        "## Exercises\n",
        "\n",
        "1. Train on [Pascal VOC dataset](http://host.robots.ox.ac.uk/pascal/VOC/#bestpractice) (2007 and above) with YOLOv3 model. Show your results into your own test images.\n",
        "2. Please see [YOLOv8](https://docs.ultralytics.com/models/yolov8/). This is the latest version of YOLO. Follow the instruction on the github and perform the training on any dataset.\n",
        "3. Please write a summary report on your training process and the result. Submit the .pdf file of your work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pj8YTmklxcat",
        "outputId": "ec5af63f-d48d-4242-919c-a9d5623ff1df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-19 09:32:33--  http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar\n",
            "Resolving pjreddie.com (pjreddie.com)... 162.0.215.52\n",
            "Connecting to pjreddie.com (pjreddie.com)|162.0.215.52|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar [following]\n",
            "--2024-10-19 09:32:34--  https://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar\n",
            "Connecting to pjreddie.com (pjreddie.com)|162.0.215.52|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 460032000 (439M) [application/x-tar]\n",
            "Saving to: ‘/content/VOC/VOCtrainval_06-Nov-2007.tar’\n",
            "\n",
            "VOCtrainval_06-Nov- 100%[===================>] 438.72M  13.2MB/s    in 34s     \n",
            "\n",
            "2024-10-19 09:33:08 (13.0 MB/s) - ‘/content/VOC/VOCtrainval_06-Nov-2007.tar’ saved [460032000/460032000]\n",
            "\n",
            "--2024-10-19 09:33:08--  http://pjreddie.com/media/files/VOCtest_06-Nov-2007.tar\n",
            "Resolving pjreddie.com (pjreddie.com)... 162.0.215.52\n",
            "Connecting to pjreddie.com (pjreddie.com)|162.0.215.52|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://pjreddie.com/media/files/VOCtest_06-Nov-2007.tar [following]\n",
            "--2024-10-19 09:33:09--  https://pjreddie.com/media/files/VOCtest_06-Nov-2007.tar\n",
            "Connecting to pjreddie.com (pjreddie.com)|162.0.215.52|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 451020800 (430M) [application/x-tar]\n",
            "Saving to: ‘/content/VOC/VOCtest_06-Nov-2007.tar’\n",
            "\n",
            "VOCtest_06-Nov-2007 100%[===================>] 430.13M  10.7MB/s    in 42s     \n",
            "\n",
            "2024-10-19 09:33:51 (10.3 MB/s) - ‘/content/VOC/VOCtest_06-Nov-2007.tar’ saved [451020800/451020800]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p /content/VOC\n",
        "!wget http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar -P /content/VOC/\n",
        "!wget http://pjreddie.com/media/files/VOCtest_06-Nov-2007.tar -P /content/VOC/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xf /content/VOC/VOCtrainval_06-Nov-2007.tar -C /content/VOC/\n",
        "!tar -xf /content/VOC/VOCtest_06-Nov-2007.tar -C /content/VOC/"
      ],
      "metadata": {
        "id": "FNopPPdM0-6a"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "voc_dataset_path = '/content/VOC/VOCdevkit/VOC2007'\n",
        "voc_labels_path = '/content/VOC/VOCdevkit/VOC2007/labels/'"
      ],
      "metadata": {
        "id": "dsXfXHkD1AKY"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import os\n",
        "\n",
        "def convert_to_yolo_format(xml_file):\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    size = root.find('size')\n",
        "    width = int(size.find('width').text)\n",
        "    height = int(size.find('height').text)\n",
        "\n",
        "    yolo_annotation = []\n",
        "\n",
        "    for obj in root.iter('object'):\n",
        "        difficult = obj.find('difficult').text\n",
        "        if int(difficult) == 1:\n",
        "            continue\n",
        "        class_name = obj.find('name').text\n",
        "        if class_name not in class_names:\n",
        "            continue\n",
        "        class_id = class_names.index(class_name)\n",
        "\n",
        "        xmlbox = obj.find('bndbox')\n",
        "        b = (float(xmlbox.find('xmin').text), float(xmlbox.find('ymin').text),\n",
        "             float(xmlbox.find('xmax').text), float(xmlbox.find('ymax').text))\n",
        "        bb = convert_bbox((width, height), b)\n",
        "        yolo_annotation.append(f\"{class_id} \" + \" \".join(map(str, bb)) + \"\\n\")\n",
        "\n",
        "    return yolo_annotation\n",
        "\n",
        "def convert_bbox(size, box):\n",
        "    dw = 1. / size[0]\n",
        "    dh = 1. / size[1]\n",
        "    x = (box[0] + box[2]) / 2.0 - 1\n",
        "    y = (box[1] + box[3]) / 2.0 - 1\n",
        "    w = box[2] - box[0]\n",
        "    h = box[3] - box[1]\n",
        "    return x * dw, y * dh, w * dw, h * dh\n",
        "\n",
        "# Replace with your Pascal VOC classes (e.g., 'person', 'car', 'dog', etc.)\n",
        "class_names = [\"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\",\n",
        "               \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\",\n",
        "               \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]\n",
        "\n",
        "# Parse and convert the annotations\n",
        "for xml_file in os.listdir(voc_dataset_path + '/Annotations'):\n",
        "    if not xml_file.endswith(\".xml\"):\n",
        "        continue\n",
        "    annotations = convert_to_yolo_format(os.path.join(voc_dataset_path, 'Annotations', xml_file))\n",
        "\n",
        "    # Write converted annotations to txt file in YOLO format\n",
        "    with open(os.path.join(voc_labels_path, xml_file.replace('.xml', '.txt')), 'w') as f:\n",
        "        f.writelines(annotations)"
      ],
      "metadata": {
        "id": "oD-xcwHr1AM_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "\n",
        "class VOCDataset(Dataset):\n",
        "    def __init__(self, voc_dataset_path, transform=None):\n",
        "        self.voc_dataset_path = voc_dataset_path\n",
        "        self.transform = transform\n",
        "        self.image_folder = os.path.join(voc_dataset_path, 'JPEGImages')\n",
        "        self.label_folder = os.path.join(voc_dataset_path, 'labels')\n",
        "\n",
        "        self.image_files = [f for f in os.listdir(self.image_folder) if f.endswith('.jpg')]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.image_folder, self.image_files[idx])\n",
        "        image = Image.open(img_name).convert(\"RGB\")\n",
        "\n",
        "        # Load YOLO annotations\n",
        "        label_name = self.image_files[idx].replace('.jpg', '.txt')\n",
        "        labels_path = os.path.join(self.label_folder, label_name)\n",
        "\n",
        "        boxes = []\n",
        "        class_ids = []\n",
        "        with open(labels_path, 'r') as f:\n",
        "            for line in f.readlines():\n",
        "                values = line.strip().split()\n",
        "                class_id = int(values[0])\n",
        "                x_center, y_center, width, height = map(float, values[1:5])\n",
        "\n",
        "                # Convert YOLO format back to bounding box coordinates\n",
        "                x1 = (x_center - width / 2) * image.size[0]\n",
        "                y1 = (y_center - height / 2) * image.size[1]\n",
        "                x2 = (x_center + width / 2) * image.size[0]\n",
        "                y2 = (y_center + height / 2) * image.size[1]\n",
        "\n",
        "                boxes.append([x1, y1, x2, y2])\n",
        "                class_ids.append(class_id)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, torch.tensor(class_ids), np.array(boxes)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            bboxes = list(obj['bbox'] for obj in target)\n",
        "            category_ids = list(obj['category_id'] for obj in target)\n",
        "            transformed = self.transform(image=img, bboxes=bboxes, category_ids=category_ids)\n",
        "            img = transformed['image'],\n",
        "            bboxes = torch.Tensor(transformed['bboxes'])\n",
        "            cat_ids = torch.Tensor(transformed['category_ids'])\n",
        "            labels, bboxes = self.__create_label(bboxes, cat_ids.type(torch.IntTensor))\n",
        "\n",
        "        return img, labels, bboxes\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __create_label(self, bboxes, class_inds):\n",
        "        \"\"\"\n",
        "        Label assignment. For a single picture all GT box bboxes are assigned anchor.\n",
        "        1、Select a bbox in order, convert its coordinates(\"xyxy\") to \"xywh\"; and scale bbox'\n",
        "           xywh by the strides.\n",
        "        2、Calculate the iou between the each detection layer'anchors and the bbox in turn, and select the largest\n",
        "            anchor to predict the bbox.If the ious of all detection layers are smaller than 0.3, select the largest\n",
        "            of all detection layers' anchors to predict the bbox.\n",
        "        Note :\n",
        "        1、The same GT may be assigned to multiple anchors. And the anchors may be on the same or different layer.\n",
        "        2、The total number of bboxes may be more than it is, because the same GT may be assigned to multiple layers\n",
        "        of detection.\n",
        "        \"\"\"\n",
        "        # print(\"Class indices: \", class_inds)\n",
        "        bboxes = np.array(bboxes)\n",
        "        class_inds = np.array(class_inds)\n",
        "        anchors = ANCHORS # all the anchors\n",
        "        strides = np.array(STRIDES) # list of strides\n",
        "        train_output_size = IP_SIZE / strides # image with different scales\n",
        "        anchors_per_scale = NUM_ANCHORS # anchor per scale\n",
        "\n",
        "        label = [\n",
        "            np.zeros(\n",
        "                (\n",
        "                    int(train_output_size[i]),\n",
        "                    int(train_output_size[i]),\n",
        "                    anchors_per_scale,\n",
        "                    5 + NUM_CLASSES,\n",
        "                )\n",
        "            )\n",
        "            for i in range(3)\n",
        "        ]\n",
        "        # 150 bounding box ground truths per scale\n",
        "        bboxes_xywh = [\n",
        "            np.zeros((150, 4)) for _ in range(3)\n",
        "        ]  # Darknet the max_num is 30\n",
        "        bbox_count = np.zeros((3,))\n",
        "\n",
        "        for i in range(len(bboxes)):\n",
        "            bbox_coor = bboxes[i][:4]\n",
        "            bbox_class_ind = cats_dict[str(class_inds[i])]\n",
        "\n",
        "            # onehot\n",
        "            one_hot = np.zeros(NUM_CLASSES, dtype=np.float32)\n",
        "            one_hot[bbox_class_ind] = 1.0\n",
        "            # one_hot_smooth = dataAug.LabelSmooth()(one_hot, self.num_classes)\n",
        "\n",
        "            # convert \"xyxy\" to \"xywh\"\n",
        "            bbox_xywh = np.concatenate(\n",
        "                [\n",
        "                    (0.5 * bbox_coor[2:] + bbox_coor[:2]) ,\n",
        "                    bbox_coor[2:],\n",
        "                ],\n",
        "                axis=-1,\n",
        "            )\n",
        "\n",
        "            bbox_xywh_scaled = (\n",
        "                1.0 * bbox_xywh[np.newaxis, :] / strides[:, np.newaxis]\n",
        "            )\n",
        "\n",
        "            iou = []\n",
        "            exist_positive = False\n",
        "            for i in range(3):\n",
        "                anchors_xywh = np.zeros((anchors_per_scale, 4))\n",
        "                anchors_xywh[:, 0:2] = (\n",
        "                    np.floor(bbox_xywh_scaled[i, 0:2]).astype(np.int32) + 0.5\n",
        "                )  # 0.5 for compensation\n",
        "\n",
        "                # assign all anchors\n",
        "                anchors_xywh[:, 2:4] = anchors[i]\n",
        "\n",
        "                iou_scale = iou_xywh_numpy(\n",
        "                    bbox_xywh_scaled[i][np.newaxis, :], anchors_xywh\n",
        "                )\n",
        "                iou.append(iou_scale)\n",
        "                iou_mask = iou_scale > 0.3\n",
        "\n",
        "                if np.any(iou_mask):\n",
        "                    xind, yind = np.floor(bbox_xywh_scaled[i, 0:2]).astype(\n",
        "                        np.int32\n",
        "                    )\n",
        "\n",
        "                    label[i][yind, xind, iou_mask, 0:4] = bbox_xywh * strides[i]\n",
        "                    label[i][yind, xind, iou_mask, 4:5] = 1.0\n",
        "                    label[i][yind, xind, iou_mask, 5:] = one_hot\n",
        "\n",
        "                    bbox_ind = int(bbox_count[i] % 150)  # BUG : 150为一个先验值,内存消耗大\n",
        "                    bboxes_xywh[i][bbox_ind, :4] = bbox_xywh * strides[i]\n",
        "                    bbox_count[i] += 1\n",
        "\n",
        "                    exist_positive = True\n",
        "\n",
        "            if not exist_positive:\n",
        "                # check if a ground truth bb have the best anchor with any scale\n",
        "                best_anchor_ind = np.argmax(np.array(iou).reshape(-1), axis=-1)\n",
        "                best_detect = int(best_anchor_ind / anchors_per_scale)\n",
        "                best_anchor = int(best_anchor_ind % anchors_per_scale)\n",
        "\n",
        "                xind, yind = np.floor(\n",
        "                    bbox_xywh_scaled[best_detect, 0:2]\n",
        "                ).astype(np.int32)\n",
        "\n",
        "                label[best_detect][yind, xind, best_anchor, 0:4] = bbox_xywh * strides[best_detect]\n",
        "                label[best_detect][yind, xind, best_anchor, 4:5] = 1.0\n",
        "                # label[best_detect][yind, xind, best_anchor, 5:6] = bbox_mix\n",
        "                label[best_detect][yind, xind, best_anchor, 5:] = one_hot\n",
        "\n",
        "                bbox_ind = int(bbox_count[best_detect] % 150)\n",
        "                bboxes_xywh[best_detect][bbox_ind, :4] = bbox_xywh * strides[best_detect]\n",
        "                bbox_count[best_detect] += 1\n",
        "\n",
        "        flatten_size_s = int(train_output_size[2]) * int(train_output_size[2]) * anchors_per_scale\n",
        "        flatten_size_m = int(train_output_size[1]) * int(train_output_size[1]) * anchors_per_scale\n",
        "        flatten_size_l = int(train_output_size[0]) * int(train_output_size[0]) * anchors_per_scale\n",
        "\n",
        "        label_s = torch.Tensor(label[2]).view(1, flatten_size_s, 5 + NUM_CLASSES).squeeze(0)\n",
        "        label_m = torch.Tensor(label[1]).view(1, flatten_size_m, 5 + NUM_CLASSES).squeeze(0)\n",
        "        label_l = torch.Tensor(label[0]).view(1, flatten_size_l, 5 + NUM_CLASSES).squeeze(0)\n",
        "\n",
        "        bboxes_s = torch.Tensor(bboxes_xywh[2])\n",
        "        bboxes_m = torch.Tensor(bboxes_xywh[1])\n",
        "        bboxes_l = torch.Tensor(bboxes_xywh[0])\n",
        "\n",
        "        # label_sbbox, label_mbbox, label_lbbox = label\n",
        "        sbboxes, mbboxes, lbboxes = bboxes_xywh\n",
        "        # print(\"label\")\n",
        "        labels = torch.cat([label_l, label_m, label_s], 0)\n",
        "        bboxes = torch.cat([bboxes_l, bboxes_m, bboxes_s], 0)\n",
        "        return labels, bboxes\n"
      ],
      "metadata": {
        "id": "8FoH0kG21AWt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import division\n",
        "import time\n",
        "import os\n",
        "import os.path as osp\n",
        "import numpy as np\n",
        "import cv2\n",
        "import pickle as pkl\n",
        "import pandas as pd\n",
        "import random\n",
        "from copy import copy, deepcopy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Subset\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "from util import *\n",
        "\n",
        "import albumentations as A"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XrJMhlyIuto",
        "outputId": "7dafc5e0-b92f-4926-ae07-4d1a940a4351"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.18 (you have 1.4.15). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from util import *\n",
        "import darknet\n",
        "\n",
        "blocks = darknet.parse_cfg(\"cfg/yolov3.cfg\")\n",
        "print(darknet.create_modules(blocks))\n",
        "\n",
        "class MyDarknet(nn.Module):\n",
        "    def __init__(self, cfgfile):\n",
        "        super(MyDarknet, self).__init__()\n",
        "        # load the config file and create our model\n",
        "        self.blocks = darknet.parse_cfg(cfgfile)\n",
        "        self.net_info, self.module_list = darknet.create_modules(self.blocks)\n",
        "\n",
        "    def forward(self, x, CUDA:bool):\n",
        "        modules = self.blocks[1:]\n",
        "        outputs = {}   #We cache the outputs for the route layer\n",
        "\n",
        "        write = 0\n",
        "        # run forward propagation. Follow the instruction from dictionary modules\n",
        "        for i, module in enumerate(modules):\n",
        "            module_type = (module[\"type\"])\n",
        "\n",
        "            if module_type == \"convolutional\" or module_type == \"upsample\":\n",
        "                # do convolutional network\n",
        "                x = self.module_list[i](x)\n",
        "\n",
        "            elif module_type == \"route\":\n",
        "                # concat layers\n",
        "                layers = module[\"layers\"]\n",
        "                layers = [int(a) for a in layers]\n",
        "\n",
        "                if (layers[0]) > 0:\n",
        "                    layers[0] = layers[0] - i\n",
        "\n",
        "                if len(layers) == 1:\n",
        "                    x = outputs[i + (layers[0])]\n",
        "\n",
        "                else:\n",
        "                    if (layers[1]) > 0:\n",
        "                        layers[1] = layers[1] - i\n",
        "\n",
        "                    map1 = outputs[i + layers[0]]\n",
        "                    map2 = outputs[i + layers[1]]\n",
        "                    x = torch.cat((map1, map2), 1)\n",
        "\n",
        "\n",
        "            elif  module_type == \"shortcut\":\n",
        "                from_ = int(module[\"from\"])\n",
        "                # residual network\n",
        "                x = outputs[i-1] + outputs[i+from_]\n",
        "\n",
        "            elif module_type == 'yolo':\n",
        "                anchors = self.module_list[i][0].anchors\n",
        "                #Get the input dimensions\n",
        "                inp_dim = int (self.net_info[\"height\"])\n",
        "\n",
        "                #Get the number of classes\n",
        "                num_classes = int (module[\"classes\"])\n",
        "\n",
        "                #Transform\n",
        "                # predict_transform is in util.py\n",
        "                batch_size = x.size(0)\n",
        "                stride =  inp_dim // x.size(2)\n",
        "                grid_size = inp_dim // stride\n",
        "                bbox_attrs = 5 + num_classes\n",
        "                num_anchors = len(anchors)\n",
        "\n",
        "                x = predict_transform(x, inp_dim, anchors, num_classes, CUDA)\n",
        "                if not write:              #if no collector has been intialised.\n",
        "                    detections = x\n",
        "                    write = 1\n",
        "\n",
        "                else:\n",
        "                    detections = torch.cat((detections, x), 1)\n",
        "\n",
        "            outputs[i] = x\n",
        "\n",
        "        return detections\n",
        "\n",
        "\n",
        "    def load_weights(self, weightfile):\n",
        "        '''\n",
        "        Load pretrained weight\n",
        "        '''\n",
        "        #Open the weights file\n",
        "        fp = open(weightfile, \"rb\")\n",
        "\n",
        "        #The first 5 values are header information\n",
        "        # 1. Major version number\n",
        "        # 2. Minor Version Number\n",
        "        # 3. Subversion number\n",
        "        # 4,5. Images seen by the network (during training)\n",
        "        header = np.fromfile(fp, dtype = np.int32, count = 5)\n",
        "        self.header = torch.from_numpy(header)\n",
        "        self.seen = self.header[3]\n",
        "\n",
        "        weights = np.fromfile(fp, dtype = np.float32)\n",
        "\n",
        "        ptr = 0\n",
        "        for i in range(len(self.module_list)):\n",
        "            module_type = self.blocks[i + 1][\"type\"]\n",
        "\n",
        "            #If module_type is convolutional load weights\n",
        "            #Otherwise ignore.\n",
        "\n",
        "            if module_type == \"convolutional\":\n",
        "                model = self.module_list[i]\n",
        "                try:\n",
        "                    batch_normalize = int(self.blocks[i+1][\"batch_normalize\"])\n",
        "                except:\n",
        "                    batch_normalize = 0\n",
        "\n",
        "                conv = model[0]\n",
        "\n",
        "\n",
        "                if (batch_normalize):\n",
        "                    bn = model[1]\n",
        "\n",
        "                    #Get the number of weights of Batch Norm Layer\n",
        "                    num_bn_biases = bn.bias.numel()\n",
        "\n",
        "                    #Load the weights\n",
        "                    bn_biases = torch.from_numpy(weights[ptr:ptr + num_bn_biases])\n",
        "                    ptr += num_bn_biases\n",
        "\n",
        "                    bn_weights = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n",
        "                    ptr  += num_bn_biases\n",
        "\n",
        "                    bn_running_mean = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n",
        "                    ptr  += num_bn_biases\n",
        "\n",
        "                    bn_running_var = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n",
        "                    ptr  += num_bn_biases\n",
        "\n",
        "                    #Cast the loaded weights into dims of model weights.\n",
        "                    bn_biases = bn_biases.view_as(bn.bias.data)\n",
        "                    bn_weights = bn_weights.view_as(bn.weight.data)\n",
        "                    bn_running_mean = bn_running_mean.view_as(bn.running_mean)\n",
        "                    bn_running_var = bn_running_var.view_as(bn.running_var)\n",
        "\n",
        "                    #Copy the data to model\n",
        "                    bn.bias.data.copy_(bn_biases)\n",
        "                    bn.weight.data.copy_(bn_weights)\n",
        "                    bn.running_mean.copy_(bn_running_mean)\n",
        "                    bn.running_var.copy_(bn_running_var)\n",
        "\n",
        "                else:\n",
        "                    #Number of biases\n",
        "                    num_biases = conv.bias.numel()\n",
        "\n",
        "                    #Load the weights\n",
        "                    conv_biases = torch.from_numpy(weights[ptr: ptr + num_biases])\n",
        "                    ptr = ptr + num_biases\n",
        "\n",
        "                    #reshape the loaded weights according to the dims of the model weights\n",
        "                    conv_biases = conv_biases.view_as(conv.bias.data)\n",
        "\n",
        "                    #Finally copy the data\n",
        "                    conv.bias.data.copy_(conv_biases)\n",
        "\n",
        "                #Let us load the weights for the Convolutional layers\n",
        "                num_weights = conv.weight.numel()\n",
        "\n",
        "                #Do the same as above for weights\n",
        "                conv_weights = torch.from_numpy(weights[ptr:ptr+num_weights])\n",
        "                ptr = ptr + num_weights\n",
        "\n",
        "                conv_weights = conv_weights.view_as(conv.weight.data)\n",
        "                conv.weight.data.copy_(conv_weights)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nq6mgODwMKoY",
        "outputId": "8cb5be83-eaa0-4fc5-fbe6-41615b98efe1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "({'type': 'net', 'batch': '1', 'subdivisions': '1', 'width': '416', 'height': '416', 'channels': '3', 'momentum': '0.9', 'decay': '0.0005', 'angle': '0', 'saturation': '1.5', 'exposure': '1.5', 'hue': '.1', 'learning_rate': '0.001', 'burn_in': '1000', 'max_batches': '500200', 'policy': 'steps', 'steps': '400000,450000', 'scales': '.1,.1'}, ModuleList(\n",
            "  (0): Sequential(\n",
            "    (conv_0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_0): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (1): Sequential(\n",
            "    (conv_1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (batch_norm_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (2): Sequential(\n",
            "    (conv_2): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (3): Sequential(\n",
            "    (conv_3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_3): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (4): Sequential(\n",
            "    (shortcut_4): EmptyLayer()\n",
            "  )\n",
            "  (5): Sequential(\n",
            "    (conv_5): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (batch_norm_5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (6): Sequential(\n",
            "    (conv_6): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_6): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (7): Sequential(\n",
            "    (conv_7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_7): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (8): Sequential(\n",
            "    (shortcut_8): EmptyLayer()\n",
            "  )\n",
            "  (9): Sequential(\n",
            "    (conv_9): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_9): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (10): Sequential(\n",
            "    (conv_10): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_10): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (11): Sequential(\n",
            "    (shortcut_11): EmptyLayer()\n",
            "  )\n",
            "  (12): Sequential(\n",
            "    (conv_12): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (batch_norm_12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_12): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (13): Sequential(\n",
            "    (conv_13): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_13): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (14): Sequential(\n",
            "    (conv_14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_14): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_14): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (15): Sequential(\n",
            "    (shortcut_15): EmptyLayer()\n",
            "  )\n",
            "  (16): Sequential(\n",
            "    (conv_16): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_16): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_16): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (17): Sequential(\n",
            "    (conv_17): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_17): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_17): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (18): Sequential(\n",
            "    (shortcut_18): EmptyLayer()\n",
            "  )\n",
            "  (19): Sequential(\n",
            "    (conv_19): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_19): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_19): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (20): Sequential(\n",
            "    (conv_20): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_20): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_20): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (21): Sequential(\n",
            "    (shortcut_21): EmptyLayer()\n",
            "  )\n",
            "  (22): Sequential(\n",
            "    (conv_22): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_22): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_22): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (23): Sequential(\n",
            "    (conv_23): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_23): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_23): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (24): Sequential(\n",
            "    (shortcut_24): EmptyLayer()\n",
            "  )\n",
            "  (25): Sequential(\n",
            "    (conv_25): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_25): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_25): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (26): Sequential(\n",
            "    (conv_26): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_26): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_26): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (27): Sequential(\n",
            "    (shortcut_27): EmptyLayer()\n",
            "  )\n",
            "  (28): Sequential(\n",
            "    (conv_28): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_28): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_28): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (29): Sequential(\n",
            "    (conv_29): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_29): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_29): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (30): Sequential(\n",
            "    (shortcut_30): EmptyLayer()\n",
            "  )\n",
            "  (31): Sequential(\n",
            "    (conv_31): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_31): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_31): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (32): Sequential(\n",
            "    (conv_32): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_32): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_32): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (33): Sequential(\n",
            "    (shortcut_33): EmptyLayer()\n",
            "  )\n",
            "  (34): Sequential(\n",
            "    (conv_34): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_34): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_34): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (35): Sequential(\n",
            "    (conv_35): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_35): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_35): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (36): Sequential(\n",
            "    (shortcut_36): EmptyLayer()\n",
            "  )\n",
            "  (37): Sequential(\n",
            "    (conv_37): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (batch_norm_37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_37): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (38): Sequential(\n",
            "    (conv_38): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_38): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_38): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (39): Sequential(\n",
            "    (conv_39): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_39): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_39): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (40): Sequential(\n",
            "    (shortcut_40): EmptyLayer()\n",
            "  )\n",
            "  (41): Sequential(\n",
            "    (conv_41): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_41): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_41): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (42): Sequential(\n",
            "    (conv_42): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_42): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_42): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (43): Sequential(\n",
            "    (shortcut_43): EmptyLayer()\n",
            "  )\n",
            "  (44): Sequential(\n",
            "    (conv_44): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_44): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_44): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (45): Sequential(\n",
            "    (conv_45): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_45): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_45): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (46): Sequential(\n",
            "    (shortcut_46): EmptyLayer()\n",
            "  )\n",
            "  (47): Sequential(\n",
            "    (conv_47): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_47): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_47): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (48): Sequential(\n",
            "    (conv_48): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_48): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_48): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (49): Sequential(\n",
            "    (shortcut_49): EmptyLayer()\n",
            "  )\n",
            "  (50): Sequential(\n",
            "    (conv_50): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_50): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_50): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (51): Sequential(\n",
            "    (conv_51): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_51): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_51): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (52): Sequential(\n",
            "    (shortcut_52): EmptyLayer()\n",
            "  )\n",
            "  (53): Sequential(\n",
            "    (conv_53): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_53): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_53): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (54): Sequential(\n",
            "    (conv_54): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_54): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_54): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (55): Sequential(\n",
            "    (shortcut_55): EmptyLayer()\n",
            "  )\n",
            "  (56): Sequential(\n",
            "    (conv_56): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_56): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_56): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (57): Sequential(\n",
            "    (conv_57): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_57): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_57): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (58): Sequential(\n",
            "    (shortcut_58): EmptyLayer()\n",
            "  )\n",
            "  (59): Sequential(\n",
            "    (conv_59): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_59): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_59): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (60): Sequential(\n",
            "    (conv_60): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_60): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_60): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (61): Sequential(\n",
            "    (shortcut_61): EmptyLayer()\n",
            "  )\n",
            "  (62): Sequential(\n",
            "    (conv_62): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (batch_norm_62): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_62): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (63): Sequential(\n",
            "    (conv_63): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_63): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_63): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (64): Sequential(\n",
            "    (conv_64): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_64): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_64): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (65): Sequential(\n",
            "    (shortcut_65): EmptyLayer()\n",
            "  )\n",
            "  (66): Sequential(\n",
            "    (conv_66): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_66): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_66): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (67): Sequential(\n",
            "    (conv_67): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_67): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_67): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (68): Sequential(\n",
            "    (shortcut_68): EmptyLayer()\n",
            "  )\n",
            "  (69): Sequential(\n",
            "    (conv_69): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_69): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_69): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (70): Sequential(\n",
            "    (conv_70): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_70): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_70): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (71): Sequential(\n",
            "    (shortcut_71): EmptyLayer()\n",
            "  )\n",
            "  (72): Sequential(\n",
            "    (conv_72): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_72): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_72): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (73): Sequential(\n",
            "    (conv_73): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_73): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_73): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (74): Sequential(\n",
            "    (shortcut_74): EmptyLayer()\n",
            "  )\n",
            "  (75): Sequential(\n",
            "    (conv_75): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_75): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_75): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (76): Sequential(\n",
            "    (conv_76): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_76): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_76): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (77): Sequential(\n",
            "    (conv_77): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_77): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_77): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (78): Sequential(\n",
            "    (conv_78): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_78): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_78): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (79): Sequential(\n",
            "    (conv_79): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_79): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_79): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (80): Sequential(\n",
            "    (conv_80): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_80): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_80): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (81): Sequential(\n",
            "    (conv_81): Conv2d(1024, 255, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (82): Sequential(\n",
            "    (Detection_82): DetectionLayer()\n",
            "  )\n",
            "  (83): Sequential(\n",
            "    (route_83): EmptyLayer()\n",
            "  )\n",
            "  (84): Sequential(\n",
            "    (conv_84): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_84): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_84): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (85): Sequential(\n",
            "    (upsample_85): Upsample(scale_factor=2.0, mode='nearest')\n",
            "  )\n",
            "  (86): Sequential(\n",
            "    (route_86): EmptyLayer()\n",
            "  )\n",
            "  (87): Sequential(\n",
            "    (conv_87): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_87): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_87): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (88): Sequential(\n",
            "    (conv_88): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_88): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_88): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (89): Sequential(\n",
            "    (conv_89): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_89): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_89): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (90): Sequential(\n",
            "    (conv_90): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_90): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_90): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (91): Sequential(\n",
            "    (conv_91): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_91): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_91): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (92): Sequential(\n",
            "    (conv_92): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_92): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_92): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (93): Sequential(\n",
            "    (conv_93): Conv2d(512, 255, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (94): Sequential(\n",
            "    (Detection_94): DetectionLayer()\n",
            "  )\n",
            "  (95): Sequential(\n",
            "    (route_95): EmptyLayer()\n",
            "  )\n",
            "  (96): Sequential(\n",
            "    (conv_96): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_96): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_96): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (97): Sequential(\n",
            "    (upsample_97): Upsample(scale_factor=2.0, mode='nearest')\n",
            "  )\n",
            "  (98): Sequential(\n",
            "    (route_98): EmptyLayer()\n",
            "  )\n",
            "  (99): Sequential(\n",
            "    (conv_99): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_99): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_99): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (100): Sequential(\n",
            "    (conv_100): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_100): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_100): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (101): Sequential(\n",
            "    (conv_101): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_101): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_101): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (102): Sequential(\n",
            "    (conv_102): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_102): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_102): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (103): Sequential(\n",
            "    (conv_103): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (batch_norm_103): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_103): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (104): Sequential(\n",
            "    (conv_104): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (batch_norm_104): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (leaky_104): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  )\n",
            "  (105): Sequential(\n",
            "    (conv_105): Conv2d(256, 255, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (106): Sequential(\n",
            "    (Detection_106): DetectionLayer()\n",
            "  )\n",
            "))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((416, 416)),  # Resize for YOLO\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "def collate_fn(batch):\n",
        "    images, labels, boxes = zip(*batch)\n",
        "\n",
        "    # Stack images\n",
        "    images = torch.stack(images)\n",
        "\n",
        "    # Pad labels and boxes\n",
        "    labels = [torch.tensor(label) for label in labels]\n",
        "    boxes = [torch.tensor(box) for box in boxes]\n",
        "\n",
        "    # Pad boxes to have a fixed size\n",
        "    max_boxes = 50  # Adjust as needed\n",
        "    padded_boxes = []\n",
        "    for box in boxes:\n",
        "        padded_box = torch.zeros((max_boxes, 4))  # 4 coordinates\n",
        "        padded_box[:box.size(0)] = box\n",
        "        padded_boxes.append(padded_box)\n",
        "\n",
        "    padded_boxes = torch.stack(padded_boxes)\n",
        "\n",
        "    return images, labels, padded_boxes\n",
        "\n",
        "BATCH_SIZE = 10\n",
        "# def collate_fn(batch):\n",
        "#     return zip(*batch)\n",
        "\n",
        "voc_dataset_path = '/content/VOC/VOCdevkit/VOC2007'  # Replace with your dataset path\n",
        "\n",
        "train_dataset = Subset(VOCDataset(voc_dataset_path, transform=transform), list(range(0,20)))\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                                               shuffle=True, num_workers=4, collate_fn=collate_fn)\n",
        "\n",
        "# dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=4, collate_fn=collate_fn)  # Adjust batch_size and num_workers as needed\n",
        "\n",
        "# Example usage\n",
        "for images, labels, boxes in train_dataloader:\n",
        "    print(images.shape)  # Shape of the image batch\n",
        "    print(labels)       # Bounding boxes for the images\n",
        "    print(boxes)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5IG14n51Adn",
        "outputId": "3189d7ca-d2f9-4358-d38f-ca045b338d55"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "<ipython-input-6-cc9cf5029238>:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = [torch.tensor(label) for label in labels]\n",
            "<ipython-input-6-cc9cf5029238>:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = [torch.tensor(label) for label in labels]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 3, 416, 416])\n",
            "[tensor([13]), tensor([12, 12, 14, 14, 14]), tensor([12, 12]), tensor([8]), tensor([ 4, 14, 14, 14, 14]), tensor([19, 14]), tensor([3, 3]), tensor([1]), tensor([ 1, 14]), tensor([14, 14])]\n",
            "tensor([[[ 69.,  90., 457., 265.],\n",
            "         [  0.,   0.,   0.,   0.],\n",
            "         [  0.,   0.,   0.,   0.],\n",
            "         ...,\n",
            "         [  0.,   0.,   0.,   0.],\n",
            "         [  0.,   0.,   0.,   0.],\n",
            "         [  0.,   0.,   0.,   0.]],\n",
            "\n",
            "        [[  0.,  62., 324., 317.],\n",
            "         [286.,  74., 361., 237.],\n",
            "         [395.,   0., 468., 315.],\n",
            "         ...,\n",
            "         [  0.,   0.,   0.,   0.],\n",
            "         [  0.,   0.,   0.,   0.],\n",
            "         [  0.,   0.,   0.,   0.]],\n",
            "\n",
            "        [[ 71., 131., 256., 308.],\n",
            "         [256., 132., 448., 267.],\n",
            "         [  0.,   0.,   0.,   0.],\n",
            "         ...,\n",
            "         [  0.,   0.,   0.,   0.],\n",
            "         [  0.,   0.,   0.,   0.],\n",
            "         [  0.,   0.,   0.,   0.]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 14.,  50., 453., 317.],\n",
            "         [  0.,   0.,   0.,   0.],\n",
            "         [  0.,   0.,   0.,   0.],\n",
            "         ...,\n",
            "         [  0.,   0.,   0.,   0.],\n",
            "         [  0.,   0.,   0.,   0.],\n",
            "         [  0.,   0.,   0.,   0.]],\n",
            "\n",
            "        [[ 31., 180., 215., 495.],\n",
            "         [ 24.,  26., 109., 355.],\n",
            "         [  0.,   0.,   0.,   0.],\n",
            "         ...,\n",
            "         [  0.,   0.,   0.,   0.],\n",
            "         [  0.,   0.,   0.,   0.],\n",
            "         [  0.,   0.,   0.,   0.]],\n",
            "\n",
            "        [[ 64.,   1., 499., 374.],\n",
            "         [  0.,   0., 118., 246.],\n",
            "         [  0.,   0.,   0.,   0.],\n",
            "         ...,\n",
            "         [  0.,   0.,   0.,   0.],\n",
            "         [  0.,   0.,   0.,   0.],\n",
            "         [  0.,   0.,   0.,   0.]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def iou_xywh_numpy(boxes1, boxes2):\n",
        "    boxes1 = np.array(boxes1)\n",
        "    boxes2 = np.array(boxes2)\n",
        "    # print(boxes1, boxes2)\n",
        "\n",
        "    boxes1_area = boxes1[..., 2] * boxes1[..., 3]\n",
        "    boxes2_area = boxes2[..., 2] * boxes2[..., 3]\n",
        "\n",
        "    boxes1 = np.concatenate([boxes1[..., :2] - boxes1[..., 2:] * 0.5,\n",
        "                                boxes1[..., :2] + boxes1[..., 2:] * 0.5], axis=-1)\n",
        "    boxes2 = np.concatenate([boxes2[..., :2] - boxes2[..., 2:] * 0.5,\n",
        "                                boxes2[..., :2] + boxes2[..., 2:] * 0.5], axis=-1)\n",
        "\n",
        "    left_up = np.maximum(boxes1[..., :2], boxes2[..., :2])\n",
        "    right_down = np.minimum(boxes1[..., 2:], boxes2[..., 2:])\n",
        "\n",
        "    inter_section = np.maximum(right_down - left_up, 0.0)\n",
        "    inter_area = inter_section[..., 0] * inter_section[..., 1]\n",
        "    union_area = boxes1_area + boxes2_area - inter_area\n",
        "    IOU = 1.0 * inter_area / union_area\n",
        "    return IOU\n",
        "\n",
        "\n",
        "def CIOU_xywh_torch(boxes1,boxes2):\n",
        "    '''\n",
        "    cal CIOU of two boxes or batch boxes\n",
        "    :param boxes1:[xmin,ymin,xmax,ymax] or\n",
        "                [[xmin,ymin,xmax,ymax],[xmin,ymin,xmax,ymax],...]\n",
        "    :param boxes2:[xmin,ymin,xmax,ymax]\n",
        "    :return:\n",
        "    '''\n",
        "    # cx cy w h->xyxy\n",
        "    boxes1 = torch.cat([boxes1[..., :2] - boxes1[..., 2:] * 0.5,\n",
        "                        boxes1[..., :2] + boxes1[..., 2:] * 0.5], dim=-1)\n",
        "    boxes2 = torch.cat([boxes2[..., :2] - boxes2[..., 2:] * 0.5,\n",
        "                        boxes2[..., :2] + boxes2[..., 2:] * 0.5], dim=-1)\n",
        "\n",
        "    boxes1 = torch.cat([torch.min(boxes1[..., :2], boxes1[..., 2:]),\n",
        "                        torch.max(boxes1[..., :2], boxes1[..., 2:])], dim=-1)\n",
        "    boxes2 = torch.cat([torch.min(boxes2[..., :2], boxes2[..., 2:]),\n",
        "                        torch.max(boxes2[..., :2], boxes2[..., 2:])], dim=-1)\n",
        "\n",
        "    # (x2 minus x1 = width)  * (y2 - y1 = height)\n",
        "    boxes1_area = (boxes1[..., 2] - boxes1[..., 0]) * (boxes1[..., 3] - boxes1[..., 1])\n",
        "    boxes2_area = (boxes2[..., 2] - boxes2[..., 0]) * (boxes2[..., 3] - boxes2[..., 1])\n",
        "\n",
        "    # upper left of the intersection region (x,y)\n",
        "    inter_left_up = torch.max(boxes1[..., :2], boxes2[..., :2])\n",
        "\n",
        "    # bottom right of the intersection region (x,y)\n",
        "    inter_right_down = torch.min(boxes1[..., 2:], boxes2[..., 2:])\n",
        "\n",
        "    # if there is overlapping we will get (w,h) else set to (0,0) because it could be negative if no overlapping\n",
        "    inter_section = torch.max(inter_right_down - inter_left_up, torch.zeros_like(inter_right_down))\n",
        "    inter_area = inter_section[..., 0] * inter_section[..., 1]\n",
        "    union_area = boxes1_area + boxes2_area - inter_area\n",
        "    ious = 1.0 * inter_area / union_area\n",
        "\n",
        "    # cal outer boxes\n",
        "    outer_left_up = torch.min(boxes1[..., :2], boxes2[..., :2])\n",
        "    outer_right_down = torch.max(boxes1[..., 2:], boxes2[..., 2:])\n",
        "    outer = torch.max(outer_right_down - outer_left_up, torch.zeros_like(inter_right_down))\n",
        "    outer_diagonal_line = torch.pow(outer[..., 0], 2) + torch.pow(outer[..., 1], 2)\n",
        "\n",
        "    # cal center distance\n",
        "    # center x center y\n",
        "    boxes1_center = (boxes1[..., :2] +  boxes1[...,2:]) * 0.5\n",
        "    boxes2_center = (boxes2[..., :2] +  boxes2[...,2:]) * 0.5\n",
        "\n",
        "    # euclidean distance\n",
        "    # x1-x2 square\n",
        "    center_dis = torch.pow(boxes1_center[...,0]-boxes2_center[...,0], 2) +\\\n",
        "                 torch.pow(boxes1_center[...,1]-boxes2_center[...,1], 2)\n",
        "\n",
        "    # cal penalty term\n",
        "    # cal width,height\n",
        "    boxes1_size = torch.max(boxes1[..., 2:] - boxes1[..., :2], torch.zeros_like(inter_right_down))\n",
        "    boxes2_size = torch.max(boxes2[..., 2:] - boxes2[..., :2], torch.zeros_like(inter_right_down))\n",
        "    v = (4 / (math.pi ** 2)) * torch.pow(\n",
        "            torch.atan((boxes1_size[...,0]/torch.clamp(boxes1_size[...,1],min = 1e-6))) -\n",
        "            torch.atan((boxes2_size[..., 0] / torch.clamp(boxes2_size[..., 1],min = 1e-6))), 2)\n",
        "\n",
        "    alpha = v / (1-ious+v)\n",
        "\n",
        "    #cal ciou\n",
        "    cious = ious - (center_dis / outer_diagonal_line + alpha*v)\n",
        "\n",
        "    return cious\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def run_training(model, optimizer, dataloader, device, img_size, n_epoch, every_n_batch, every_n_epoch, ckpt_dir):\n",
        "#     for epoch_i in range(n_epoch):\n",
        "#         running_loss = 0.0\n",
        "#         for inputs, labels, bboxes in dataloader:\n",
        "#             inputs = torch.from_numpy(np.array(inputs)).squeeze(1).float()\n",
        "#             inputs = inputs.to(device)\n",
        "\n",
        "#             # Initialize lists to hold the tensor labels and bounding boxes for the current batch\n",
        "#             label_tensors = []\n",
        "#             box_tensors = []\n",
        "\n",
        "#             for label in labels:\n",
        "#                 # Convert each label into a tensor and move to device\n",
        "#                 label_tensor = torch.tensor(label).to(device)\n",
        "#                 label_tensors.append(label_tensor)\n",
        "\n",
        "#             print(label_tensors)\n",
        "\n",
        "#             for bbox in bboxes:\n",
        "#                 # Convert each bounding box into a tensor and move to device\n",
        "#                 bbox_tensor = torch.tensor(bbox).to(device)\n",
        "#                 box_tensors.append(bbox_tensor)\n",
        "\n",
        "#             # At this point, label_tensors is a list of tensors\n",
        "#             # You can process these lists as needed\n",
        "\n",
        "#             # Perform the model forward pass and loss calculation\n",
        "#             optimizer.zero_grad()\n",
        "#             with torch.set_grad_enabled(True):\n",
        "#                 outputs = model(inputs, True)\n",
        "\n",
        "#                 pred_xywh = outputs[..., 0:4] / img_size\n",
        "#                 pred_conf = outputs[..., 4:5]\n",
        "#                 pred_cls = outputs[..., 5:]\n",
        "\n",
        "#                 # Prepare the target values\n",
        "#                 # label_xywh = [label[:, :4] for label in label_tensors]  # Normalize the xywh\n",
        "#                 label_obj_mask = [label[:, 4:5] for label in label_tensors]\n",
        "#                 label_cls = [label[:, 5:] for label in label_tensors]\n",
        "\n",
        "#                 # Compute the losses for each image in the batch\n",
        "#                 loss = 0\n",
        "#                 for i in range(len(label_tensors)):\n",
        "#                     lambda_coord = 0.001\n",
        "#                     lambda_noobj = 0.05\n",
        "#                     # Calculate losses for the i-th image\n",
        "#                     loss_coord = lambda_coord * label_obj_mask[i] * nn.MSELoss()(pred_xywh[i], label_xywh[i])\n",
        "#                     loss_conf = (label_obj_mask[i] * nn.BCELoss()(pred_conf[i], label_obj_mask[i])) + \\\n",
        "#                                 (lambda_noobj * (1.0 - label_obj_mask[i]) * nn.BCELoss()(pred_conf[i], label_obj_mask[i]))\n",
        "#                     loss_cls = label_obj_mask[i] * nn.BCELoss()(pred_cls[i], label_cls[i])\n",
        "\n",
        "#                     loss += loss_coord + loss_conf + loss_cls\n",
        "\n",
        "#                 loss.backward()\n",
        "#                 optimizer.step()\n",
        "\n",
        "#                 # Statistics\n",
        "#                 running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "#         epoch_loss = running_loss / len(dataloader.dataset)\n",
        "#         print(epoch_loss)\n",
        "#         print('End Epoch')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "def run_training(model, optimizer, dataloader, device, img_size, n_epoch, every_n_batch, every_n_epoch, ckpt_dir):\n",
        "    for epoch_i in range(n_epoch):\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for inputs, labels, bboxes in dataloader:\n",
        "            # Assume inputs, labels, and bboxes are provided as batches of size 1\n",
        "            inputs = torch.from_numpy(np.array(inputs)).squeeze(1).float().to(device)  # Ensure inputs are on device\n",
        "\n",
        "            # Extract single label and bounding box\n",
        "            label = torch.tensor(labels[0], dtype=torch.float32, requires_grad=False).to(device)\n",
        "            bbox = torch.tensor(bboxes[0], dtype=torch.float32, requires_grad=False).to(device)\n",
        "\n",
        "            # Ensure label tensor is 2D: shape (1, 6)\n",
        "            if label.dim() == 1:  # If it's 1D, we can add a dimension\n",
        "                label = label.unsqueeze(0)  # Add a new dimension\n",
        "\n",
        "            # Ensure bounding box tensor is 2D: shape (1, 4)\n",
        "            if bbox.dim() == 1:\n",
        "                bbox = bbox.unsqueeze(0)  # Add a new dimension\n",
        "\n",
        "            # Forward pass\n",
        "            optimizer.zero_grad()\n",
        "            with torch.set_grad_enabled(True):\n",
        "                outputs = model(inputs, device)  # Ensure model outputs require grad\n",
        "\n",
        "                pred_xywh = outputs[..., :4]  # Assuming the first four are x, y, width, height\n",
        "                pred_conf = outputs[..., 4:5]\n",
        "                pred_cls = outputs[..., 5:]\n",
        "\n",
        "                # Initialize loss variables\n",
        "                loss = torch.tensor(0.0, device=device, requires_grad=True)  # Initialize loss as a tensor with requires_grad=True\n",
        "\n",
        "                lambda_coord = 0.001\n",
        "                lambda_noobj = 0.05\n",
        "\n",
        "                # Compute losses\n",
        "                if label.size(1) == 6:  # Ensure label has the correct shape\n",
        "                    label_obj_mask = label[:, 4:5]  # Object mask\n",
        "                    label_cls = label[:, 5:]  # Class labels\n",
        "\n",
        "                    # Compute losses for the current image\n",
        "                    loss_coord = lambda_coord * label_obj_mask * nn.MSELoss()(pred_xywh, label[:, :4])\n",
        "                    loss_conf = (label_obj_mask * nn.BCELoss()(pred_conf, label_obj_mask)) + \\\n",
        "                                (lambda_noobj * (1.0 - label_obj_mask) * nn.BCELoss()(pred_conf, label_obj_mask))\n",
        "                    loss_cls = label_obj_mask * nn.BCELoss()(pred_cls, label_cls)\n",
        "\n",
        "                    # Accumulate the loss\n",
        "                    loss += loss_coord.sum() + loss_conf.sum() + loss_cls.sum()  # Ensure loss is accumulated correctly\n",
        "\n",
        "                # Check if loss requires grad\n",
        "                print(f\"Loss requires_grad: {loss.requires_grad}, Loss value: {loss.item()}\")\n",
        "\n",
        "                loss.backward()  # Ensure loss is a tensor\n",
        "                optimizer.step()\n",
        "\n",
        "                # Statistics\n",
        "                running_loss += loss.item()\n",
        "\n",
        "        epoch_loss = running_loss / len(dataloader)\n",
        "        print(f'Epoch [{epoch_i + 1}/{n_epoch}], Loss: {epoch_loss:.4f}')\n",
        "        print('End Epoch')\n",
        "\n",
        "        # Optional: Save checkpoint every n epoch\n",
        "        if (epoch_i + 1) % every_n_epoch == 0:\n",
        "            torch.save(model.state_dict(), f'{ckpt_dir}/model_epoch_{epoch_i + 1}.pth')\n"
      ],
      "metadata": {
        "id": "NiYFrRMKIoSS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device to GPU or CPU\n",
        "gpu = \"0\"\n",
        "device = torch.device(\"cuda:{}\".format(gpu) if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(\"Loading network.....\")\n",
        "model = MyDarknet(\"cfg/yolov3.cfg\")\n",
        "# load pretrained\n",
        "# model.load_weights(\"yolov3.weights\")\n",
        "print(\"Network successfully loaded\")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "n_epoch = 5\n",
        "img_size = 416\n",
        "save_every_batch = False\n",
        "save_every_epoch = True\n",
        "ckpt_dir = \"./checkpoints\"\n",
        "\n",
        "\n",
        "run_training(model, optimizer, train_dataloader, device,\n",
        "                 img_size,\n",
        "                 n_epoch,\n",
        "                 save_every_batch,\n",
        "                 save_every_epoch,\n",
        "                 ckpt_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eql1O5sZ1Ah0",
        "outputId": "9d330f2d-734e-4780-f19c-a07621fd4281"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading network.....\n",
            "Network successfully loaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-cc9cf5029238>:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = [torch.tensor(label) for label in labels]\n",
            "<ipython-input-6-cc9cf5029238>:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = [torch.tensor(label) for label in labels]\n",
            "<ipython-input-7-2251ccd1d1c8>:169: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  label = torch.tensor(labels[0], dtype=torch.float32, requires_grad=False).to(device)\n",
            "<ipython-input-7-2251ccd1d1c8>:170: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  bbox = torch.tensor(bboxes[0], dtype=torch.float32, requires_grad=False).to(device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss requires_grad: True, Loss value: 0.0\n",
            "Loss requires_grad: True, Loss value: 0.0\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "End Epoch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-cc9cf5029238>:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = [torch.tensor(label) for label in labels]\n",
            "<ipython-input-6-cc9cf5029238>:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = [torch.tensor(label) for label in labels]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss requires_grad: True, Loss value: 0.0\n",
            "Loss requires_grad: True, Loss value: 0.0\n",
            "Epoch [2/5], Loss: 0.0000\n",
            "End Epoch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-cc9cf5029238>:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = [torch.tensor(label) for label in labels]\n",
            "<ipython-input-6-cc9cf5029238>:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = [torch.tensor(label) for label in labels]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss requires_grad: True, Loss value: 0.0\n",
            "Loss requires_grad: True, Loss value: 0.0\n",
            "Epoch [3/5], Loss: 0.0000\n",
            "End Epoch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-cc9cf5029238>:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = [torch.tensor(label) for label in labels]\n",
            "<ipython-input-6-cc9cf5029238>:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = [torch.tensor(label) for label in labels]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss requires_grad: True, Loss value: 0.0\n",
            "Loss requires_grad: True, Loss value: 0.0\n",
            "Epoch [4/5], Loss: 0.0000\n",
            "End Epoch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-cc9cf5029238>:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = [torch.tensor(label) for label in labels]\n",
            "<ipython-input-6-cc9cf5029238>:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = [torch.tensor(label) for label in labels]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss requires_grad: True, Loss value: 0.0\n",
            "Loss requires_grad: True, Loss value: 0.0\n",
            "Epoch [5/5], Loss: 0.0000\n",
            "End Epoch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Please see YOLOv8. This is the latest version of YOLO. Follow the instruction on the github and perform the training on any dataset."
      ],
      "metadata": {
        "id": "jXIkzdGZWnkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSfQpcF0XD1O",
        "outputId": "bcd7c9a7-0f03-4028-abd1-e0dc012e1603"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.17-py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (10.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.19.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.9-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Downloading ultralytics-8.3.17-py3-none-any.whl (876 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m876.6/876.6 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.9-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.17 ultralytics-thop-2.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/openimages/v6/oidv6-train-annotations-bbox.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCSxd7JKYPa0",
        "outputId": "65ec9063-8e34-4706-a32e-7f2a6327cab5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-19 10:44:36--  https://storage.googleapis.com/openimages/v6/oidv6-train-annotations-bbox.csv\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.170.207, 142.251.175.207, 74.125.24.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.170.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2258447590 (2.1G) [text/csv]\n",
            "Saving to: ‘oidv6-train-annotations-bbox.csv’\n",
            "\n",
            "oidv6-train-annotat 100%[===================>]   2.10G  22.9MB/s    in 97s     \n",
            "\n",
            "2024-10-19 10:46:14 (22.2 MB/s) - ‘oidv6-train-annotations-bbox.csv’ saved [2258447590/2258447590]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/openimages/v5/validation-annotations-bbox.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQem-ifnYQbU",
        "outputId": "a9dfab33-8458-424e-93a1-503c99c075f4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-19 10:46:14--  https://storage.googleapis.com/openimages/v5/validation-annotations-bbox.csv\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.200.207, 74.125.130.207, 74.125.68.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.200.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 25105048 (24M) [text/csv]\n",
            "Saving to: ‘validation-annotations-bbox.csv’\n",
            "\n",
            "validation-annotati 100%[===================>]  23.94M  9.96MB/s    in 2.4s    \n",
            "\n",
            "2024-10-19 10:46:17 (9.96 MB/s) - ‘validation-annotations-bbox.csv’ saved [25105048/25105048]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/openimages/v5/test-annotations-bbox.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00p07iCmYQdq",
        "outputId": "60c5b2f1-b4ff-4104-afff-01d66e9ea939"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-19 10:47:46--  https://storage.googleapis.com/openimages/v5/test-annotations-bbox.csv\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.68.207, 64.233.170.207, 142.251.175.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.68.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 77484237 (74M) [text/csv]\n",
            "Saving to: ‘test-annotations-bbox.csv’\n",
            "\n",
            "test-annotations-bb 100%[===================>]  73.89M  19.0MB/s    in 5.3s    \n",
            "\n",
            "2024-10-19 10:47:51 (14.0 MB/s) - ‘test-annotations-bbox.csv’ saved [77484237/77484237]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/openimages/dataset/master/downloader.py\n",
        "!pip install boto3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAgsauxcYQg4",
        "outputId": "7f7b8a56-461c-435d-dda5-b22f619a30ce"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-19 10:47:51--  https://raw.githubusercontent.com/openimages/dataset/master/downloader.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4244 (4.1K) [text/plain]\n",
            "Saving to: ‘downloader.py’\n",
            "\n",
            "downloader.py       100%[===================>]   4.14K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-10-19 10:47:52 (50.1 MB/s) - ‘downloader.py’ saved [4244/4244]\n",
            "\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.35.44-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting botocore<1.36.0,>=1.35.44 (from boto3)\n",
            "  Downloading botocore-1.35.44-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3)\n",
            "  Downloading s3transfer-0.10.3-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.36.0,>=1.35.44->boto3) (2.8.2)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.36.0,>=1.35.44->boto3) (2.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.44->boto3) (1.16.0)\n",
            "Downloading boto3-1.35.44-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.35.44-py3-none-any.whl (12.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.10.3-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.6/82.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jmespath, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.35.44 botocore-1.35.44 jmespath-1.0.1 s3transfer-0.10.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# install and import pyyaml used to create custom config files for training\n",
        "%pip install pyyaml\n",
        "import yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSLSkuXtYfCl",
        "outputId": "631a76da-d329-4ae4-9eca-d76d57dfe45f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (6.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_yolo_yaml_config(yaml_filepath, dataset_path, dataset_labels):\n",
        "\n",
        "    data = {'path':dataset_path,\n",
        "            'train': os.path.join('images', 'train'),\n",
        "            'val': os.path.join('images', 'validation'),\n",
        "            'names':{i:label for i, label in enumerate(dataset_labels)}\n",
        "            }\n",
        "\n",
        "    # Save the changes to the file\n",
        "    with open(yaml_filepath, 'w') as fp:\n",
        "    # set sort_keys = False to preserve the order of keys\n",
        "        yaml.dump(data, fp, sort_keys=False)"
      ],
      "metadata": {
        "id": "RFoU6VF5Ye_7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_class_id(classes_file, class_names):\n",
        "    id_name_dict = {}\n",
        "    with open(classes_file, 'r') as f:\n",
        "        for line in f:\n",
        "            print(line)\n",
        "            id, label = line.split(',')\n",
        "            label = label.strip()\n",
        "            #print(label)\n",
        "            if label in class_names:\n",
        "                print(label)\n",
        "                id_name_dict[label] = id\n",
        "\n",
        "    return id_name_dict"
      ],
      "metadata": {
        "id": "Rt4JtxW1Ye9C"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "names = ['Ant', 'Insect']\n",
        "\n",
        "# path to the dataset directory (recommended to use absolute path)\n",
        "dataset_path = os.path.abspath(os.path.join('.', 'data'))\n",
        "\n",
        "# path to the YAML file that contains training configuration\n",
        "yaml_filepath = os.path.join('.', 'config.yaml')\n",
        "\n",
        "# Create a custom YAML config file based on the above selected target objects and dataset path\n",
        "create_yolo_yaml_config(yaml_filepath, dataset_path, names)"
      ],
      "metadata": {
        "id": "DP27Gg3xYe6q"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/mohamedamine99/YOLOv8-custom-object-detection/blob/main/Custom-object-detection-with-YOLOv8/class-descriptions-boxable.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "185Q6HZ5ZnGf",
        "outputId": "cd1c8d70-7456-4050-d0d4-3f28e840b457"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-19 10:50:32--  https://github.com/mohamedamine99/YOLOv8-custom-object-detection/blob/main/Custom-object-detection-with-YOLOv8/class-descriptions-boxable.csv\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘class-descriptions-boxable.csv’\n",
            "\n",
            "class-descriptions-     [ <=>                ] 182.18K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-10-19 10:50:32 (12.7 MB/s) - ‘class-descriptions-boxable.csv’ saved [186554]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_ids = get_class_id('./class-descriptions-boxable.csv', names)\n",
        "print(class_ids)\n",
        "print(names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "du5VCkGBYnM4",
        "outputId": "af39716e-0170-4761-8080-d98a22f4f57c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/m/011k07,Tortoise\n",
            "\n",
            "/m/011q46kg,Container\n",
            "\n",
            "/m/012074,Magpie\n",
            "\n",
            "/m/0120dh,Sea turtle\n",
            "\n",
            "/m/01226z,Football\n",
            "\n",
            "/m/012n7d,Ambulance\n",
            "\n",
            "/m/012w5l,Ladder\n",
            "\n",
            "/m/012xff,Toothbrush\n",
            "\n",
            "/m/012ysf,Syringe\n",
            "\n",
            "/m/0130jx,Sink\n",
            "\n",
            "/m/0138tl,Toy\n",
            "\n",
            "/m/013y1f,Organ (Musical Instrument)\n",
            "\n",
            "/m/01432t,Cassette deck\n",
            "\n",
            "/m/014j1m,Apple\n",
            "\n",
            "/m/014sv8,Human eye\n",
            "\n",
            "/m/014trl,Cosmetics\n",
            "\n",
            "/m/014y4n,Paddle\n",
            "\n",
            "/m/0152hh,Snowman\n",
            "\n",
            "/m/01599,Beer\n",
            "\n",
            "/m/01_5g,Chopsticks\n",
            "\n",
            "/m/015h_t,Human beard\n",
            "\n",
            "/m/015p6,Bird\n",
            "\n",
            "/m/015qbp,Parking meter\n",
            "\n",
            "/m/015qff,Traffic light\n",
            "\n",
            "/m/015wgc,Croissant\n",
            "\n",
            "/m/015x4r,Cucumber\n",
            "\n",
            "/m/015x5n,Radish\n",
            "\n",
            "/m/0162_1,Towel\n",
            "\n",
            "/m/0167gd,Doll\n",
            "\n",
            "/m/016m2d,Skull\n",
            "\n",
            "/m/0174k2,Washing machine\n",
            "\n",
            "/m/0174n1,Glove\n",
            "\n",
            "/m/0175cv,Tick\n",
            "\n",
            "/m/0176mf,Belt\n",
            "\n",
            "/m/017ftj,Sunglasses\n",
            "\n",
            "/m/018j2,Banjo\n",
            "\n",
            "/m/018p4k,Cart\n",
            "\n",
            "/m/018xm,Ball\n",
            "\n",
            "/m/01940j,Backpack\n",
            "\n",
            "/m/0199g,Bicycle\n",
            "\n",
            "/m/019dx1,Home appliance\n",
            "\n",
            "/m/019h78,Centipede\n",
            "\n",
            "/m/019jd,Boat\n",
            "\n",
            "/m/019w40,Surfboard\n",
            "\n",
            "/m/01b638,Boot\n",
            "\n",
            "/m/01b7fy,Headphones\n",
            "\n",
            "/m/01b9xk,Hot dog\n",
            "\n",
            "/m/01bfm9,Shorts\n",
            "\n",
            "/m/01_bhs,Fast food\n",
            "\n",
            "/m/01bjv,Bus\n",
            "\n",
            "/m/01bl7v,Boy\n",
            "\n",
            "/m/01bms0,Screwdriver\n",
            "\n",
            "/m/01bqk0,Bicycle wheel\n",
            "\n",
            "/m/01btn,Barge\n",
            "\n",
            "/m/01c648,Laptop\n",
            "\n",
            "/m/01cmb2,Miniskirt\n",
            "\n",
            "/m/01d380,Drill (Tool)\n",
            "\n",
            "/m/01d40f,Dress\n",
            "\n",
            "/m/01dws,Bear\n",
            "\n",
            "/m/01dwsz,Waffle\n",
            "\n",
            "/m/01dwwc,Pancake\n",
            "\n",
            "/m/01dxs,Brown bear\n",
            "\n",
            "/m/01dy8n,Woodpecker\n",
            "\n",
            "/m/01f8m5,Blue jay\n",
            "\n",
            "/m/01f91_,Pretzel\n",
            "\n",
            "/m/01fb_0,Bagel\n",
            "\n",
            "/m/01fdzj,Tower\n",
            "\n",
            "/m/01fh4r,Teapot\n",
            "\n",
            "/m/01g317,Person\n",
            "\n",
            "/m/01g3x7,Bow and arrow\n",
            "\n",
            "/m/01gkx_,Swimwear\n",
            "\n",
            "/m/01gllr,Beehive\n",
            "\n",
            "/m/01gmv2,Brassiere\n",
            "\n",
            "/m/01h3n,Bee\n",
            "\n",
            "/m/01h44,Bat (Animal)\n",
            "\n",
            "/m/01h8tj,Starfish\n",
            "\n",
            "/m/01hrv5,Popcorn\n",
            "\n",
            "/m/01j3zr,Burrito\n",
            "\n",
            "/m/01j4z9,Chainsaw\n",
            "\n",
            "/m/01j51,Balloon\n",
            "\n",
            "/m/01j5ks,Wrench\n",
            "\n",
            "/m/01j61q,Tent\n",
            "\n",
            "/m/01jfm_,Vehicle registration plate\n",
            "\n",
            "/m/01jfsr,Lantern\n",
            "\n",
            "/m/01k6s3,Toaster\n",
            "\n",
            "/m/01kb5b,Flashlight\n",
            "\n",
            "/m/01knjb,Billboard\n",
            "\n",
            "/m/01krhy,Tiara\n",
            "\n",
            "/m/01lcw4,Limousine\n",
            "\n",
            "/m/01llwg,Necklace\n",
            "\n",
            "/m/01lrl,Carnivore\n",
            "\n",
            "/m/01lsmm,Scissors\n",
            "\n",
            "/m/01lynh,Stairs\n",
            "\n",
            "/m/01m2v,Computer keyboard\n",
            "\n",
            "/m/01m4t,Printer\n",
            "\n",
            "/m/01mqdt,Traffic sign\n",
            "\n",
            "/m/01mzpv,Chair\n",
            "\n",
            "/m/01n4qj,Shirt\n",
            "\n",
            "/m/01n5jq,Poster\n",
            "\n",
            "/m/01nkt,Cheese\n",
            "\n",
            "/m/01nq26,Sock\n",
            "\n",
            "/m/01pns0,Fire hydrant\n",
            "\n",
            "/m/01prls,Land vehicle\n",
            "\n",
            "/m/01r546,Earrings\n",
            "\n",
            "/m/01rkbr,Tie\n",
            "\n",
            "/m/01rzcn,Watercraft\n",
            "\n",
            "/m/01s105,Cabinetry\n",
            "\n",
            "/m/01s55n,Suitcase\n",
            "\n",
            "/m/01tcjp,Muffin\n",
            "\n",
            "/m/01vbnl,Bidet\n",
            "\n",
            "/m/01ww8y,Snack\n",
            "\n",
            "/m/01x3jk,Snowmobile\n",
            "\n",
            "/m/01x3z,Clock\n",
            "\n",
            "/m/01xgg_,Medical equipment\n",
            "\n",
            "/m/01xq0k1,Cattle\n",
            "\n",
            "/m/01xqw,Cello\n",
            "\n",
            "/m/01xs3r,Jet ski\n",
            "\n",
            "/m/01x_v,Camel\n",
            "\n",
            "/m/01xygc,Coat\n",
            "\n",
            "/m/01xyhv,Suit\n",
            "\n",
            "/m/01y9k5,Desk\n",
            "\n",
            "/m/01yrx,Cat\n",
            "\n",
            "/m/01yx86,Bronze sculpture\n",
            "\n",
            "/m/01z1kdw,Juice\n",
            "\n",
            "/m/02068x,Gondola\n",
            "\n",
            "/m/020jm,Beetle\n",
            "\n",
            "/m/020kz,Cannon\n",
            "\n",
            "/m/020lf,Computer mouse\n",
            "\n",
            "/m/021mn,Cookie\n",
            "\n",
            "/m/021sj1,Office building\n",
            "\n",
            "/m/0220r2,Fountain\n",
            "\n",
            "/m/0242l,Coin\n",
            "\n",
            "/m/024d2,Calculator\n",
            "\n",
            "/m/024g6,Cocktail\n",
            "\n",
            "/m/02522,Computer monitor\n",
            "\n",
            "/m/025dyy,Box\n",
            "\n",
            "/m/025fsf,Stapler\n",
            "\n",
            "/m/025nd,Christmas tree\n",
            "\n",
            "/m/025rp__,Cowboy hat\n",
            "\n",
            "/m/0268lbt,Hiking equipment\n",
            "\n",
            "/m/026qbn5,Studio couch\n",
            "\n",
            "/m/026t6,Drum\n",
            "\n",
            "/m/0270h,Dessert\n",
            "\n",
            "/m/0271qf7,Wine rack\n",
            "\n",
            "/m/0271t,Drink\n",
            "\n",
            "/m/027pcv,Zucchini\n",
            "\n",
            "/m/027rl48,Ladle\n",
            "\n",
            "/m/0283dt1,Human mouth\n",
            "\n",
            "/m/0284d,Dairy Product\n",
            "\n",
            "/m/029b3,Dice\n",
            "\n",
            "/m/029bxz,Oven\n",
            "\n",
            "/m/029tx,Dinosaur\n",
            "\n",
            "/m/02bm9n,Ratchet (Device)\n",
            "\n",
            "/m/02crq1,Couch\n",
            "\n",
            "/m/02ctlc,Cricket ball\n",
            "\n",
            "/m/02cvgx,Winter melon\n",
            "\n",
            "/m/02d1br,Spatula\n",
            "\n",
            "/m/02d9qx,Whiteboard\n",
            "\n",
            "/m/02ddwp,Pencil sharpener\n",
            "\n",
            "/m/02dgv,Door\n",
            "\n",
            "/m/02dl1y,Hat\n",
            "\n",
            "/m/02f9f_,Shower\n",
            "\n",
            "/m/02fh7f,Eraser\n",
            "\n",
            "/m/02fq_6,Fedora\n",
            "\n",
            "/m/02g30s,Guacamole\n",
            "\n",
            "/m/02gzp,Dagger\n",
            "\n",
            "/m/02h19r,Scarf\n",
            "\n",
            "/m/02hj4,Dolphin\n",
            "\n",
            "/m/02jfl0,Sombrero\n",
            "\n",
            "/m/02jnhm,Tin can\n",
            "\n",
            "/m/02jvh9,Mug\n",
            "\n",
            "/m/02jz0l,Tap\n",
            "\n",
            "/m/02l8p9,Harbor seal\n",
            "\n",
            "/m/02lbcq,Stretcher\n",
            "\n",
            "/m/02mqfb,Can opener\n",
            "\n",
            "/m/02_n6y,Goggles\n",
            "\n",
            "/m/02p0tk3,Human body\n",
            "\n",
            "/m/02p3w7d,Roller skates\n",
            "\n",
            "/m/02p5f1q,Coffee cup\n",
            "\n",
            "/m/02pdsw,Cutting board\n",
            "\n",
            "/m/02pjr4,Blender\n",
            "\n",
            "/m/02pkr5,Plumbing fixture\n",
            "\n",
            "/m/02pv19,Stop sign\n",
            "\n",
            "/m/02rdsp,Office supplies\n",
            "\n",
            "/m/02rgn06,Volleyball (Ball)\n",
            "\n",
            "/m/02s195,Vase\n",
            "\n",
            "/m/02tsc9,Slow cooker\n",
            "\n",
            "/m/02vkqh8,Wardrobe\n",
            "\n",
            "/m/02vqfm,Coffee\n",
            "\n",
            "/m/02vwcm,Whisk\n",
            "\n",
            "/m/02w3r3,Paper towel\n",
            "\n",
            "/m/02w3_ws,Personal care\n",
            "\n",
            "/m/02wbm,Food\n",
            "\n",
            "/m/02wbtzl,Sun hat\n",
            "\n",
            "/m/02wg_p,Tree house\n",
            "\n",
            "/m/02wmf,Flying disc\n",
            "\n",
            "/m/02wv6h6,Skirt\n",
            "\n",
            "/m/02wv84t,Gas stove\n",
            "\n",
            "/m/02x8cch,Salt and pepper shakers\n",
            "\n",
            "/m/02x984l,Mechanical fan\n",
            "\n",
            "/m/02xb7qb,Face powder\n",
            "\n",
            "/m/02xqq,Fax\n",
            "\n",
            "/m/02xwb,Fruit\n",
            "\n",
            "/m/02y6n,French fries\n",
            "\n",
            "/m/02z51p,Nightstand\n",
            "\n",
            "/m/02zn6n,Barrel\n",
            "\n",
            "/m/02zt3,Kite\n",
            "\n",
            "/m/02zvsm,Tart\n",
            "\n",
            "/m/030610,Treadmill\n",
            "\n",
            "/m/0306r,Fox\n",
            "\n",
            "/m/03120,Flag\n",
            "\n",
            "/m/0319l,French horn\n",
            "\n",
            "/m/031b6r,Window blind\n",
            "\n",
            "/m/031n1,Human foot\n",
            "\n",
            "/m/0323sq,Golf cart\n",
            "\n",
            "/m/032b3c,Jacket\n",
            "\n",
            "/m/033cnk,Egg (Food)\n",
            "\n",
            "/m/033rq4,Street light\n",
            "\n",
            "/m/0342h,Guitar\n",
            "\n",
            "/m/034c16,Pillow\n",
            "\n",
            "/m/035r7c,Human leg\n",
            "\n",
            "/m/035vxb,Isopod\n",
            "\n",
            "/m/0388q,Grape\n",
            "\n",
            "/m/039xj_,Human ear\n",
            "\n",
            "/m/03bbps,Power plugs and sockets\n",
            "\n",
            "/m/03bj1,Panda\n",
            "\n",
            "/m/03bk1,Giraffe\n",
            "\n",
            "/m/03bt1vf,Woman\n",
            "\n",
            "/m/03c7gz,Door handle\n",
            "\n",
            "/m/03d443,Rhinoceros\n",
            "\n",
            "/m/03dnzn,Bathtub\n",
            "\n",
            "/m/03fj2,Goldfish\n",
            "\n",
            "/m/03fp41,Houseplant\n",
            "\n",
            "/m/03fwl,Goat\n",
            "\n",
            "/m/03g8mr,Baseball bat\n",
            "\n",
            "/m/03grzl,Baseball glove\n",
            "\n",
            "/m/03hj559,Mixing bowl\n",
            "\n",
            "/m/03hl4l9,Marine invertebrates\n",
            "\n",
            "/m/03hlz0c,Kitchen utensil\n",
            "\n",
            "/m/03jbxj,Light switch\n",
            "\n",
            "/m/03jm5,House\n",
            "\n",
            "/m/03k3r,Horse\n",
            "\n",
            "/m/03kt2w,Stationary bicycle\n",
            "\n",
            "/m/03l9g,Hammer\n",
            "\n",
            "/m/03ldnb,Ceiling fan\n",
            "\n",
            "/m/03m3pdh,Sofa bed\n",
            "\n",
            "/m/03m3vtv,Adhesive tape\n",
            "\n",
            "/m/03m5k,Harp\n",
            "\n",
            "/m/03nfch,Sandal\n",
            "\n",
            "/m/03p3bw,Bicycle helmet\n",
            "\n",
            "/m/03q5c7,Saucer\n",
            "\n",
            "/m/03q5t,Harpsichord\n",
            "\n",
            "/m/03q69,Human hair\n",
            "\n",
            "/m/03qhv5,Heater\n",
            "\n",
            "/m/03qjg,Harmonica\n",
            "\n",
            "/m/03qrc,Hamster\n",
            "\n",
            "/m/03rszm,Curtain\n",
            "\n",
            "/m/03ssj5,Bed\n",
            "\n",
            "/m/03s_tn,Kettle\n",
            "\n",
            "/m/03tw93,Fireplace\n",
            "\n",
            "/m/03txqz,Scale\n",
            "\n",
            "/m/03v5tg,Drinking straw\n",
            "\n",
            "/m/03vt0,Insect\n",
            "\n",
            "Insect\n",
            "/m/03wvsk,Hair dryer\n",
            "\n",
            "/m/03_wxk,Kitchenware\n",
            "\n",
            "/m/03wym,Indoor rower\n",
            "\n",
            "/m/03xxp,Invertebrate\n",
            "\n",
            "/m/03y6mg,Food processor\n",
            "\n",
            "/m/03__z0,Bookcase\n",
            "\n",
            "/m/040b_t,Refrigerator\n",
            "\n",
            "/m/04169hn,Wood-burning stove\n",
            "\n",
            "/m/0420v5,Punching bag\n",
            "\n",
            "/m/043nyj,Common fig\n",
            "\n",
            "/m/0440zs,Cocktail shaker\n",
            "\n",
            "/m/0449p,Jaguar (Animal)\n",
            "\n",
            "/m/044r5d,Golf ball\n",
            "\n",
            "/m/0463sg,Fashion accessory\n",
            "\n",
            "/m/046dlr,Alarm clock\n",
            "\n",
            "/m/047j0r,Filing cabinet\n",
            "\n",
            "/m/047v4b,Artichoke\n",
            "\n",
            "/m/04bcr3,Table\n",
            "\n",
            "/m/04brg2,Tableware\n",
            "\n",
            "/m/04c0y,Kangaroo\n",
            "\n",
            "/m/04cp_,Koala\n",
            "\n",
            "/m/04ctx,Knife\n",
            "\n",
            "/m/04dr76w,Bottle\n",
            "\n",
            "/m/04f5ws,Bottle opener\n",
            "\n",
            "/m/04g2r,Lynx\n",
            "\n",
            "/m/04gth,Lavender (Plant)\n",
            "\n",
            "/m/04h7h,Lighthouse\n",
            "\n",
            "/m/04h8sr,Dumbbell\n",
            "\n",
            "/m/04hgtk,Human head\n",
            "\n",
            "/m/04kkgm,Bowl\n",
            "\n",
            "/m/04lvq_,Humidifier\n",
            "\n",
            "/m/04m6gz,Porch\n",
            "\n",
            "/m/04m9y,Lizard\n",
            "\n",
            "/m/04p0qw,Billiard table\n",
            "\n",
            "/m/04rky,Mammal\n",
            "\n",
            "/m/04rmv,Mouse\n",
            "\n",
            "/m/04_sv,Motorcycle\n",
            "\n",
            "/m/04szw,Musical instrument\n",
            "\n",
            "/m/04tn4x,Swim cap\n",
            "\n",
            "/m/04v6l4,Frying pan\n",
            "\n",
            "/m/04vv5k,Snowplow\n",
            "\n",
            "/m/04y4h8h,Bathroom cabinet\n",
            "\n",
            "/m/04ylt,Missile\n",
            "\n",
            "/m/04yqq2,Bust\n",
            "\n",
            "/m/04yx4,Man\n",
            "\n",
            "/m/04z4wx,Waffle iron\n",
            "\n",
            "/m/04zpv,Milk\n",
            "\n",
            "/m/04zwwv,Ring binder\n",
            "\n",
            "/m/050gv4,Plate\n",
            "\n",
            "/m/050k8,Mobile phone\n",
            "\n",
            "/m/052lwg6,Baked goods\n",
            "\n",
            "/m/052sf,Mushroom\n",
            "\n",
            "/m/05441v,Crutch\n",
            "\n",
            "/m/054fyh,Pitcher (Container)\n",
            "\n",
            "/m/054_l,Mirror\n",
            "\n",
            "/m/054xkw,Personal flotation device\n",
            "\n",
            "/m/05_5p_0,Table tennis racket\n",
            "\n",
            "/m/05676x,Pencil case\n",
            "\n",
            "/m/057cc,Musical keyboard\n",
            "\n",
            "/m/057p5t,Scoreboard\n",
            "\n",
            "/m/0584n8,Briefcase\n",
            "\n",
            "/m/058qzx,Kitchen knife\n",
            "\n",
            "/m/05bm6,Nail (Construction)\n",
            "\n",
            "/m/05ctyq,Tennis ball\n",
            "\n",
            "/m/05gqfk,Plastic bag\n",
            "\n",
            "/m/05kms,Oboe\n",
            "\n",
            "/m/05kyg_,Chest of drawers\n",
            "\n",
            "/m/05n4y,Ostrich\n",
            "\n",
            "/m/05r5c,Piano\n",
            "\n",
            "/m/05r655,Girl\n",
            "\n",
            "/m/05s2s,Plant\n",
            "\n",
            "/m/05vtc,Potato\n",
            "\n",
            "/m/05w9t9,Hair spray\n",
            "\n",
            "/m/05y5lj,Sports equipment\n",
            "\n",
            "/m/05z55,Pasta\n",
            "\n",
            "/m/05z6w,Penguin\n",
            "\n",
            "/m/05zsy,Pumpkin\n",
            "\n",
            "/m/061_f,Pear\n",
            "\n",
            "/m/061hd_,Infant bed\n",
            "\n",
            "/m/0633h,Polar bear\n",
            "\n",
            "/m/063rgb,Mixer\n",
            "\n",
            "/m/0642b4,Cupboard\n",
            "\n",
            "/m/065h6l,Jacuzzi\n",
            "\n",
            "/m/0663v,Pizza\n",
            "\n",
            "/m/06_72j,Digital clock\n",
            "\n",
            "/m/068zj,Pig\n",
            "\n",
            "/m/06bt6,Reptile\n",
            "\n",
            "/m/06c54,Rifle\n",
            "\n",
            "/m/06c7f7,Lipstick\n",
            "\n",
            "/m/06_fw,Skateboard\n",
            "\n",
            "/m/06j2d,Raven\n",
            "\n",
            "/m/06k2mb,High heels\n",
            "\n",
            "/m/06l9r,Red panda\n",
            "\n",
            "/m/06m11,Rose\n",
            "\n",
            "/m/06mf6,Rabbit\n",
            "\n",
            "/m/06msq,Sculpture\n",
            "\n",
            "/m/06ncr,Saxophone\n",
            "\n",
            "/m/06nrc,Shotgun\n",
            "\n",
            "/m/06nwz,Seafood\n",
            "\n",
            "/m/06pcq,Submarine sandwich\n",
            "\n",
            "/m/06__v,Snowboard\n",
            "\n",
            "/m/06y5r,Sword\n",
            "\n",
            "/m/06z37_,Picture frame\n",
            "\n",
            "/m/07030,Sushi\n",
            "\n",
            "/m/0703r8,Loveseat\n",
            "\n",
            "/m/071p9,Ski\n",
            "\n",
            "/m/071qp,Squirrel\n",
            "\n",
            "/m/073bxn,Tripod\n",
            "\n",
            "/m/073g6,Stethoscope\n",
            "\n",
            "/m/074d1,Submarine\n",
            "\n",
            "/m/0755b,Scorpion\n",
            "\n",
            "/m/076bq,Segway\n",
            "\n",
            "/m/076lb9,Training bench\n",
            "\n",
            "/m/078jl,Snake\n",
            "\n",
            "/m/078n6m,Coffee table\n",
            "\n",
            "/m/079cl,Skyscraper\n",
            "\n",
            "/m/07bgp,Sheep\n",
            "\n",
            "/m/07c52,Television\n",
            "\n",
            "/m/07c6l,Trombone\n",
            "\n",
            "/m/07clx,Tea\n",
            "\n",
            "/m/07cmd,Tank\n",
            "\n",
            "/m/07crc,Taco\n",
            "\n",
            "/m/07cx4,Telephone\n",
            "\n",
            "/m/07dd4,Torch\n",
            "\n",
            "/m/07dm6,Tiger\n",
            "\n",
            "/m/07fbm7,Strawberry\n",
            "\n",
            "/m/07gql,Trumpet\n",
            "\n",
            "/m/07j7r,Tree\n",
            "\n",
            "/m/07j87,Tomato\n",
            "\n",
            "/m/07jdr,Train\n",
            "\n",
            "/m/07k1x,Tool\n",
            "\n",
            "/m/07kng9,Picnic basket\n",
            "\n",
            "/m/07mcwg,Cooking spray\n",
            "\n",
            "/m/07mhn,Trousers\n",
            "\n",
            "/m/07pj7bq,Bowling equipment\n",
            "\n",
            "/m/07qxg_,Football helmet\n",
            "\n",
            "/m/07r04,Truck\n",
            "\n",
            "/m/07v9_z,Measuring cup\n",
            "\n",
            "/m/07xyvk,Coffeemaker\n",
            "\n",
            "/m/07y_7,Violin\n",
            "\n",
            "/m/07yv9,Vehicle\n",
            "\n",
            "/m/080hkjn,Handbag\n",
            "\n",
            "/m/080n7g,Paper cutter\n",
            "\n",
            "/m/081qc,Wine\n",
            "\n",
            "/m/083kb,Weapon\n",
            "\n",
            "/m/083wq,Wheel\n",
            "\n",
            "/m/084hf,Worm\n",
            "\n",
            "/m/084rd,Wok\n",
            "\n",
            "/m/084zz,Whale\n",
            "\n",
            "/m/0898b,Zebra\n",
            "\n",
            "/m/08dz3q,Auto part\n",
            "\n",
            "/m/08hvt4,Jug\n",
            "\n",
            "/m/08ks85,Pizza cutter\n",
            "\n",
            "/m/08p92x,Cream\n",
            "\n",
            "/m/08pbxl,Monkey\n",
            "\n",
            "/m/096mb,Lion\n",
            "\n",
            "/m/09728,Bread\n",
            "\n",
            "/m/099ssp,Platter\n",
            "\n",
            "/m/09b5t,Chicken\n",
            "\n",
            "/m/09csl,Eagle\n",
            "\n",
            "/m/09ct_,Helicopter\n",
            "\n",
            "/m/09d5_,Owl\n",
            "\n",
            "/m/09ddx,Duck\n",
            "\n",
            "/m/09dzg,Turtle\n",
            "\n",
            "/m/09f20,Hippopotamus\n",
            "\n",
            "/m/09f_2,Crocodile\n",
            "\n",
            "/m/09g1w,Toilet\n",
            "\n",
            "/m/09gtd,Toilet paper\n",
            "\n",
            "/m/09gys,Squid\n",
            "\n",
            "/m/09j2d,Clothing\n",
            "\n",
            "/m/09j5n,Footwear\n",
            "\n",
            "/m/09k_b,Lemon\n",
            "\n",
            "/m/09kmb,Spider\n",
            "\n",
            "/m/09kx5,Deer\n",
            "\n",
            "/m/09ld4,Frog\n",
            "\n",
            "/m/09qck,Banana\n",
            "\n",
            "/m/09rvcxw,Rocket\n",
            "\n",
            "/m/09tvcd,Wine glass\n",
            "\n",
            "/m/0b3fp9,Countertop\n",
            "\n",
            "/m/0bh9flk,Tablet computer\n",
            "\n",
            "/m/0bjyj5,Waste container\n",
            "\n",
            "/m/0b_rs,Swimming pool\n",
            "\n",
            "/m/0bt9lr,Dog\n",
            "\n",
            "/m/0bt_c3,Book\n",
            "\n",
            "/m/0bwd_0j,Elephant\n",
            "\n",
            "/m/0by6g,Shark\n",
            "\n",
            "/m/0c06p,Candle\n",
            "\n",
            "/m/0c29q,Leopard\n",
            "\n",
            "/m/0c2jj,Axe\n",
            "\n",
            "/m/0c3m8g,Hand dryer\n",
            "\n",
            "/m/0c3mkw,Soap dispenser\n",
            "\n",
            "/m/0c568,Porcupine\n",
            "\n",
            "/m/0c9ph5,Flower\n",
            "\n",
            "/m/0ccs93,Canary\n",
            "\n",
            "/m/0cd4d,Cheetah\n",
            "\n",
            "/m/0cdl1,Palm tree\n",
            "\n",
            "/m/0cdn1,Hamburger\n",
            "\n",
            "/m/0cffdh,Maple\n",
            "\n",
            "/m/0cgh4,Building\n",
            "\n",
            "/m/0ch_cf,Fish\n",
            "\n",
            "/m/0cjq5,Lobster\n",
            "\n",
            "/m/0cjs7,Garden Asparagus\n",
            "\n",
            "/m/0c_jw,Furniture\n",
            "\n",
            "/m/0cl4p,Hedgehog\n",
            "\n",
            "/m/0cmf2,Airplane\n",
            "\n",
            "/m/0cmx8,Spoon\n",
            "\n",
            "/m/0cn6p,Otter\n",
            "\n",
            "/m/0cnyhnx,Bull\n",
            "\n",
            "/m/0_cp5,Oyster\n",
            "\n",
            "/m/0cqn2,Horizontal bar\n",
            "\n",
            "/m/0crjs,Convenience store\n",
            "\n",
            "/m/0ct4f,Bomb\n",
            "\n",
            "/m/0cvnqh,Bench\n",
            "\n",
            "/m/0cxn2,Ice cream\n",
            "\n",
            "/m/0cydv,Caterpillar\n",
            "\n",
            "/m/0cyf8,Butterfly\n",
            "\n",
            "/m/0cyfs,Parachute\n",
            "\n",
            "/m/0cyhj_,Orange\n",
            "\n",
            "/m/0czz2,Antelope\n",
            "\n",
            "/m/0d20w4,Beaker\n",
            "\n",
            "/m/0d_2m,Moths and butterflies\n",
            "\n",
            "/m/0d4v4,Window\n",
            "\n",
            "/m/0d4w1,Closet\n",
            "\n",
            "/m/0d5gx,Castle\n",
            "\n",
            "/m/0d8zb,Jellyfish\n",
            "\n",
            "/m/0dbvp,Goose\n",
            "\n",
            "/m/0dbzx,Mule\n",
            "\n",
            "/m/0dftk,Swan\n",
            "\n",
            "/m/0dj6p,Peach\n",
            "\n",
            "/m/0djtd,Coconut\n",
            "\n",
            "/m/0dkzw,Seat belt\n",
            "\n",
            "/m/0dq75,Raccoon\n",
            "\n",
            "/m/0_dqb,Chisel\n",
            "\n",
            "/m/0dt3t,Fork\n",
            "\n",
            "/m/0dtln,Lamp\n",
            "\n",
            "/m/0dv5r,Camera\n",
            "\n",
            "/m/0dv77,Squash (Plant)\n",
            "\n",
            "/m/0dv9c,Racket\n",
            "\n",
            "/m/0dzct,Human face\n",
            "\n",
            "/m/0dzf4,Human arm\n",
            "\n",
            "/m/0f4s2w,Vegetable\n",
            "\n",
            "/m/0f571,Diaper\n",
            "\n",
            "/m/0f6nr,Unicycle\n",
            "\n",
            "/m/0f6wt,Falcon\n",
            "\n",
            "/m/0f8s22,Chime\n",
            "\n",
            "/m/0f9_l,Snail\n",
            "\n",
            "/m/0fbdv,Shellfish\n",
            "\n",
            "/m/0fbw6,Cabbage\n",
            "\n",
            "/m/0fj52s,Carrot\n",
            "\n",
            "/m/0fldg,Mango\n",
            "\n",
            "/m/0fly7,Jeans\n",
            "\n",
            "/m/0fm3zh,Flowerpot\n",
            "\n",
            "/m/0fp6w,Pineapple\n",
            "\n",
            "/m/0fqfqc,Drawer\n",
            "\n",
            "/m/0fqt361,Stool\n",
            "\n",
            "/m/0frqm,Envelope\n",
            "\n",
            "/m/0fszt,Cake\n",
            "\n",
            "/m/0ft9s,Dragonfly\n",
            "\n",
            "/m/0ftb8,Common sunflower\n",
            "\n",
            "/m/0fx9l,Microwave oven\n",
            "\n",
            "/m/0fz0h,Honeycomb\n",
            "\n",
            "/m/0gd2v,Marine mammal\n",
            "\n",
            "/m/0gd36,Sea lion\n",
            "\n",
            "/m/0gj37,Ladybug\n",
            "\n",
            "/m/0gjbg72,Shelf\n",
            "\n",
            "/m/0gjkl,Watch\n",
            "\n",
            "/m/0gm28,Candy\n",
            "\n",
            "/m/0grw1,Salad\n",
            "\n",
            "/m/0gv1x,Parrot\n",
            "\n",
            "/m/0gxl3,Handgun\n",
            "\n",
            "/m/0h23m,Sparrow\n",
            "\n",
            "/m/0h2r6,Van\n",
            "\n",
            "/m/0h8jyh6,Grinder\n",
            "\n",
            "/m/0h8kx63,Spice rack\n",
            "\n",
            "/m/0h8l4fh,Light bulb\n",
            "\n",
            "/m/0h8lkj8,Corded phone\n",
            "\n",
            "/m/0h8mhzd,Sports uniform\n",
            "\n",
            "/m/0h8my_4,Tennis racket\n",
            "\n",
            "/m/0h8mzrc,Wall clock\n",
            "\n",
            "/m/0h8n27j,Serving tray\n",
            "\n",
            "/m/0h8n5zk,Kitchen & dining room table\n",
            "\n",
            "/m/0h8n6f9,Dog bed\n",
            "\n",
            "/m/0h8n6ft,Cake stand\n",
            "\n",
            "/m/0h8nm9j,Cat furniture\n",
            "\n",
            "/m/0h8nr_l,Bathroom accessory\n",
            "\n",
            "/m/0h8nsvg,Facial tissue holder\n",
            "\n",
            "/m/0h8ntjv,Pressure cooker\n",
            "\n",
            "/m/0h99cwc,Kitchen appliance\n",
            "\n",
            "/m/0h9mv,Tire\n",
            "\n",
            "/m/0hdln,Ruler\n",
            "\n",
            "/m/0hf58v5,Luggage and bags\n",
            "\n",
            "/m/0hg7b,Microphone\n",
            "\n",
            "/m/0hkxq,Broccoli\n",
            "\n",
            "/m/0hnnb,Umbrella\n",
            "\n",
            "/m/0hnyx,Pastry\n",
            "\n",
            "/m/0hqkz,Grapefruit\n",
            "\n",
            "/m/0j496,Band-aid\n",
            "\n",
            "/m/0jbk,Animal\n",
            "\n",
            "/m/0jg57,Bell pepper\n",
            "\n",
            "/m/0jly1,Turkey\n",
            "\n",
            "/m/0jqgx,Lily\n",
            "\n",
            "/m/0jwn_,Pomegranate\n",
            "\n",
            "/m/0jy4k,Doughnut\n",
            "\n",
            "/m/0jyfg,Glasses\n",
            "\n",
            "/m/0k0pj,Human nose\n",
            "\n",
            "/m/0k1tl,Pen\n",
            "\n",
            "/m/0_k2,Ant\n",
            "\n",
            "Ant\n",
            "/m/0k4j,Car\n",
            "\n",
            "/m/0k5j,Aircraft\n",
            "\n",
            "/m/0k65p,Human hand\n",
            "\n",
            "/m/0km7z,Skunk\n",
            "\n",
            "/m/0kmg4,Teddy bear\n",
            "\n",
            "/m/0kpqd,Watermelon\n",
            "\n",
            "/m/0kpt_,Cantaloupe\n",
            "\n",
            "/m/0ky7b,Dishwasher\n",
            "\n",
            "/m/0l14j_,Flute\n",
            "\n",
            "/m/0l3ms,Balance beam\n",
            "\n",
            "/m/0l515,Sandwich\n",
            "\n",
            "/m/0ll1f78,Shrimp\n",
            "\n",
            "/m/0llzx,Sewing machine\n",
            "\n",
            "/m/0lt4_,Binoculars\n",
            "\n",
            "/m/0m53l,Rays and skates\n",
            "\n",
            "/m/0mcx2,Ipod\n",
            "\n",
            "/m/0mkg,Accordion\n",
            "\n",
            "/m/0mw_6,Willow\n",
            "\n",
            "/m/0n28_,Crab\n",
            "\n",
            "/m/0nl46,Crown\n",
            "\n",
            "/m/0nybt,Seahorse\n",
            "\n",
            "/m/0p833,Perfume\n",
            "\n",
            "/m/0pcr,Alpaca\n",
            "\n",
            "/m/0pg52,Taxi\n",
            "\n",
            "/m/0ph39,Canoe\n",
            "\n",
            "/m/0qjjc,Remote control\n",
            "\n",
            "/m/0qmmr,Wheelchair\n",
            "\n",
            "/m/0wdt60w,Rugby ball\n",
            "\n",
            "/m/0xfy,Armadillo\n",
            "\n",
            "/m/0xzly,Maracas\n",
            "\n",
            "/m/0zvk5,Helmet\n",
            "\n",
            "{'Insect': '/m/03vt0', 'Ant': '/m/0_k2'}\n",
            "['Ant', 'Insect']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_bboxes_filename = os.path.join('.', 'oidv6-train-annotations-bbox.csv')\n",
        "validation_bboxes_filename = os.path.join('.', 'validation-annotations-bbox.csv')\n",
        "test_bboxes_filename = os.path.join('.', 'test-annotations-bbox.csv')\n",
        "\n",
        "image_list_file_path = os.path.join('.', 'image_list_file.txt')\n",
        "\n",
        "image_list_file_list = []\n",
        "for j, filename in enumerate([train_bboxes_filename, validation_bboxes_filename, test_bboxes_filename]):\n",
        "    print(filename)\n",
        "    with open(filename, 'r') as f:\n",
        "        line = f.readline()\n",
        "        while len(line) != 0:\n",
        "            id, _, class_name, _, x1, x2, y1, y2, _, _, _, _, _ = line.split(',')[:13]\n",
        "            if class_name in list(class_ids.values()) and id not in image_list_file_list:\n",
        "                image_list_file_list.append(id)\n",
        "                with open(image_list_file_path, 'a') as fw:\n",
        "                    fw.write('{}/{}\\n'.format(['train', 'validation', 'test'][j], id))\n",
        "            line = f.readline()\n",
        "\n",
        "        f.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKoOF9nOYnQA",
        "outputId": "28008641-a7fd-4657-f2b3-af28da19ac14"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./oidv6-train-annotations-bbox.csv\n",
            "./validation-annotations-bbox.csv\n",
            "./test-annotations-bbox.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_ALL_DIR = os.path.join('.', 'data_all') # directory that contains all downloaded data selected from the list\n",
        "DATA_OUT_DIR = os.path.join('.', 'data_out') # directory that contains data reorganized in YOLO format\n",
        "os.makedirs(DATA_ALL_DIR)\n",
        "os.makedirs(DATA_OUT_DIR)"
      ],
      "metadata": {
        "id": "cnRBV3BeYrtf"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python downloader.py ./image_list_file.txt --download_folder=./data_all --num_processes=5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glV9KUTcYtbk",
        "outputId": "930085a1-486a-4774-e22b-f2ed1d2fda86"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading images: 100% 7430/7430 [15:10<00:00,  8.16it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for dir_ in ['images', 'labels']:\n",
        "    for set_ in ['train', 'validation', 'test']:\n",
        "        new_dir = os.path.join(DATA_OUT_DIR, dir_, set_)\n",
        "        if os.path.exists(new_dir):\n",
        "            shutil.rmtree(new_dir)\n",
        "        os.makedirs(new_dir)\n",
        "\n",
        "for j, filename in enumerate([train_bboxes_filename, validation_bboxes_filename, test_bboxes_filename]):\n",
        "    set_ = ['train', 'validation', 'test'][j]\n",
        "    print(filename)\n",
        "    with open(filename, 'r') as f:\n",
        "        line = f.readline()\n",
        "        while len(line) != 0:\n",
        "            id, _, class_name, _, x1, x2, y1, y2, _, _, _, _, _ = line.split(',')[:13]\n",
        "            if class_name in list(class_ids.values()):\n",
        "\n",
        "                if not os.path.exists(os.path.join(DATA_OUT_DIR, 'images', set_, '{}.jpg'.format(id))):\n",
        "\n",
        "                    shutil.copy(os.path.join(DATA_ALL_DIR, '{}.jpg'.format(id)),\n",
        "                                os.path.join(DATA_OUT_DIR, 'images', set_, '{}.jpg'.format(id)))\n",
        "\n",
        "                with open(os.path.join(DATA_OUT_DIR, 'labels', set_, '{}.txt'.format(id)), 'a') as f_ann:\n",
        "                    # class_id, xc, yx, w, h\n",
        "                    #\n",
        "                    x1, x2, y1, y2 = [float(j) for j in [x1, x2, y1, y2]]\n",
        "                    xc = (x1 + x2) / 2\n",
        "                    yc = (y1 + y2) / 2\n",
        "                    w = x2 - x1\n",
        "                    h = y2 - y1\n",
        "\n",
        "                    # class id = 0 if 'Ant' and 1 if 'Insect'\n",
        "                    name = [k for k, v in class_ids.items() if v == class_name][0]\n",
        "                    class_id = names.index(name)\n",
        "\n",
        "                    #*****\n",
        "                    f_ann.write('{} {} {} {} {}\\n'.format(class_id, xc, yc, w, h))\n",
        "                    f_ann.close()\n",
        "\n",
        "            line = f.readline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5doLArtbYteY",
        "outputId": "b6a56a26-cbe1-4794-bcf1-9b5922c0c012"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./oidv6-train-annotations-bbox.csv\n",
            "./validation-annotations-bbox.csv\n",
            "./test-annotations-bbox.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "os.environ[\"WANDB_MODE\"] = \"dryrun\"\n",
        "\n",
        "model = YOLO('yolov8n.pt')\n",
        "\n",
        "results = model.train(data='config.yaml', epochs=5)\n",
        "\n",
        "model.save('yolov8_trained.pt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "85c774bed4de46f0bc59b877ebabba6f",
            "2f14bc6198ab4ce5863dbf5529b737f3",
            "1460716c566b43e79b86d2a3e93d7719",
            "8276b0bc39e2472896131d2e8dd55a33",
            "55f05982baae43b3b45006ca53290bf2",
            "0d269e58700049ba85ac4af7aa93c9e8",
            "d8cb00d63da84c6bba90b01ef0a4000b",
            "8c9d85ae412b4cf0bd7ee29b4e306c75"
          ]
        },
        "id": "nOWtroAvWt--",
        "outputId": "78d2e11c-fefe-4675-c340-470551f43d12"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.17 🚀 Python-3.10.12 torch-2.4.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=config.yaml, epochs=5, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train4, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train4\n",
            "Overriding model.yaml nc=80 with nc=2\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    751702  ultralytics.nn.modules.head.Detect           [2, [64, 128, 256]]           \n",
            "Model summary: 225 layers, 3,011,238 parameters, 3,011,222 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train4', view at http://localhost:6006/\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLO11n...\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.35M/5.35M [00:00<00:00, 212MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/data_out/labels/train... 6461 images, 0 backgrounds, 0 corrupt: 100%|██████████| 6461/6461 [00:07<00:00, 857.75it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/data_out/labels/train.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/data_out/labels/validation... 210 images, 0 backgrounds, 0 corrupt: 100%|██████████| 210/210 [00:00<00:00, 462.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/data_out/labels/validation.cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to runs/detect/train4/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train4\u001b[0m\n",
            "Starting training for 5 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        1/5      8.55G      1.326      2.077      1.566         50        640: 100%|██████████| 404/404 [03:28<00:00,  1.94it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:04<00:00,  1.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        210        280      0.781       0.21      0.273      0.148\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        2/5      2.82G      1.385      1.756      1.606         43        640: 100%|██████████| 404/404 [03:21<00:00,  2.00it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        210        280       0.34      0.322      0.282      0.137\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        3/5      3.95G      1.347      1.615      1.585         49        640: 100%|██████████| 404/404 [03:09<00:00,  2.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:03<00:00,  1.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        210        280       0.47      0.397       0.36      0.214\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        4/5      2.96G       1.28      1.496       1.53         34        640: 100%|██████████| 404/404 [03:15<00:00,  2.07it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:05<00:00,  1.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        210        280      0.479      0.441      0.411      0.269\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        5/5      2.55G       1.19      1.342       1.46         37        640: 100%|██████████| 404/404 [03:08<00:00,  2.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        210        280      0.579      0.404      0.436      0.284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "5 epochs completed in 0.281 hours.\n",
            "Optimizer stripped from runs/detect/train4/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from runs/detect/train4/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating runs/detect/train4/weights/best.pt...\n",
            "Ultralytics 8.3.17 🚀 Python-3.10.12 torch-2.4.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Model summary (fused): 168 layers, 3,006,038 parameters, 0 gradients, 8.1 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:07<00:00,  1.00s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        210        280      0.579      0.404      0.436      0.286\n",
            "                   Ant         12         30      0.406        0.2      0.164      0.105\n",
            "                Insect        198        250      0.752      0.608      0.709      0.466\n",
            "Speed: 0.3ms preprocess, 3.9ms inference, 0.0ms loss, 7.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train4\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "85c774bed4de46f0bc59b877ebabba6f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr/pg0</td><td>▃▇█▅▁</td></tr><tr><td>lr/pg1</td><td>▃▇█▅▁</td></tr><tr><td>lr/pg2</td><td>▃▇█▅▁</td></tr><tr><td>metrics/mAP50(B)</td><td>▁▁▅▇█</td></tr><tr><td>metrics/mAP50-95(B)</td><td>▂▁▅▇█</td></tr><tr><td>metrics/precision(B)</td><td>█▁▃▃▅</td></tr><tr><td>metrics/recall(B)</td><td>▁▄▇█▇</td></tr><tr><td>model/GFLOPs</td><td>▁</td></tr><tr><td>model/parameters</td><td>▁</td></tr><tr><td>model/speed_PyTorch(ms)</td><td>▁</td></tr><tr><td>train/box_loss</td><td>▆█▇▄▁</td></tr><tr><td>train/cls_loss</td><td>█▅▄▂▁</td></tr><tr><td>train/dfl_loss</td><td>▆█▇▄▁</td></tr><tr><td>val/box_loss</td><td>▆█▄▃▁</td></tr><tr><td>val/cls_loss</td><td>▆█▄▂▁</td></tr><tr><td>val/dfl_loss</td><td>▅█▄▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr/pg0</td><td>0.00035</td></tr><tr><td>lr/pg1</td><td>0.00035</td></tr><tr><td>lr/pg2</td><td>0.00035</td></tr><tr><td>metrics/mAP50(B)</td><td>0.43619</td></tr><tr><td>metrics/mAP50-95(B)</td><td>0.28551</td></tr><tr><td>metrics/precision(B)</td><td>0.57862</td></tr><tr><td>metrics/recall(B)</td><td>0.404</td></tr><tr><td>model/GFLOPs</td><td>8.195</td></tr><tr><td>model/parameters</td><td>3011238</td></tr><tr><td>model/speed_PyTorch(ms)</td><td>4.945</td></tr><tr><td>train/box_loss</td><td>1.18995</td></tr><tr><td>train/cls_loss</td><td>1.34242</td></tr><tr><td>train/dfl_loss</td><td>1.45976</td></tr><tr><td>val/box_loss</td><td>1.14794</td></tr><tr><td>val/cls_loss</td><td>1.28912</td></tr><tr><td>val/dfl_loss</td><td>1.52627</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "You can sync this run to the cloud by running:<br/><code>wandb sync /content/wandb/offline-run-20241019_111637-wq9y67zm<code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/offline-run-20241019_111637-wq9y67zm/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The process of training was long. Pascal VOC dataset was causing errors. Understanding those errors took a lot of time. Yolov8 is training process is much easier with the help of ultralytics. I needed to only bring the dataset into yolov8 format."
      ],
      "metadata": {
        "id": "hYXvqfRukYgk"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "85c774bed4de46f0bc59b877ebabba6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2f14bc6198ab4ce5863dbf5529b737f3",
              "IPY_MODEL_1460716c566b43e79b86d2a3e93d7719"
            ],
            "layout": "IPY_MODEL_8276b0bc39e2472896131d2e8dd55a33"
          }
        },
        "2f14bc6198ab4ce5863dbf5529b737f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55f05982baae43b3b45006ca53290bf2",
            "placeholder": "​",
            "style": "IPY_MODEL_0d269e58700049ba85ac4af7aa93c9e8",
            "value": "0.000 MB of 0.000 MB uploaded\r"
          }
        },
        "1460716c566b43e79b86d2a3e93d7719": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8cb00d63da84c6bba90b01ef0a4000b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8c9d85ae412b4cf0bd7ee29b4e306c75",
            "value": 1
          }
        },
        "8276b0bc39e2472896131d2e8dd55a33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55f05982baae43b3b45006ca53290bf2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d269e58700049ba85ac4af7aa93c9e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8cb00d63da84c6bba90b01ef0a4000b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c9d85ae412b4cf0bd7ee29b4e306c75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}