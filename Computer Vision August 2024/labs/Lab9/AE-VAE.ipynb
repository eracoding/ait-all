{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d35d8602-2fdd-4305-8cd2-f78bc3ad7ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "from torchvision.utils import make_grid\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e98cda61-53ae-41f0-8d06-2ab45a20e387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torch/cuda/__init__.py:118: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11060). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "# Hyperparameter\n",
    "lr =2e-4\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "logging_interval = 50\n",
    "save_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "807edd24-4768-457f-bbbc-bc1d603f7e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For our puffer surver we need to browse via a proxy!!\n",
    "import os\n",
    "# Set HTTP and HTTPS proxy\n",
    "os.environ['http_proxy'] = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35151210-5d6d-4564-819d-45510ec1232c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1129)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:04<00:00, 2337660.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1129)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 107022.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1129)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:02<00:00, 685680.77it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1129)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 650966.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trf = transforms.Compose([\n",
    "  transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='.',\n",
    "                                train=True,\n",
    "                                transform=trf,\n",
    "                                download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='.',\n",
    "                              train=False,\n",
    "                              transform=trf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9baebf2-7a1f-4171-8850-2d85f55cb601",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          num_workers=2,\n",
    "                          shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                      batch_size=BATCH_SIZE,\n",
    "                      num_workers=2,\n",
    "                      shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c8a12b0-7198-4354-b14d-d83480aaad8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set:\n",
      "\n",
      "Image batch dimensions: torch.Size([256, 1, 28, 28])\n",
      "Image label dimensions: torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "print('Training Set:\\n')\n",
    "for images, labels in train_loader:\n",
    "    print('Image batch dimensions:', images.size())\n",
    "    print('Image label dimensions:', labels.size())\n",
    "    #print(labels[:10])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "349a213d-4bc2-4a58-9ff8-fe025012e7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        self.shape = args\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(self.shape)\n",
    "\n",
    "\n",
    "class Trim(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :28, :28]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601bce0b-5d79-4a6c-948f-17f5b5cf4883",
   "metadata": {},
   "source": [
    "## Define a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4519925-2e69-4785-9721-5666278c94d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "                nn.Conv2d(1, 32, stride=2, kernel_size=3, bias=False, padding=1),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "                nn.Dropout2d(0.25),\n",
    "                #\n",
    "                nn.Conv2d(32, 64, stride=2, kernel_size=3, bias=False, padding=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "                nn.Dropout2d(0.25),\n",
    "                #\n",
    "                nn.Conv2d(64, 64, stride=1, kernel_size=3, bias=False, padding=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "                #\n",
    "                nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        self.z_mean = torch.nn.Linear(3136, 2) # 7x7x64\n",
    "        self.z_log_var = torch.nn.Linear(3136, 2)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "                torch.nn.Linear(2, 3136),\n",
    "                Reshape(-1, 64, 7, 7),\n",
    "                #\n",
    "                nn.ConvTranspose2d(64, 64, stride=2, kernel_size=3),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "                nn.Dropout2d(0.25),\n",
    "                #\n",
    "                nn.ConvTranspose2d(64, 64, stride=2, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "                nn.Dropout2d(0.25),\n",
    "                #\n",
    "                nn.ConvTranspose2d(64, 32, stride=2, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "                nn.Dropout2d(0.25),\n",
    "                #\n",
    "                nn.ConvTranspose2d(32, 1, stride=2, kernel_size=3, padding=1),\n",
    "                #\n",
    "                Trim(),  # 1x29x29 -> 1x28x28\n",
    "                nn.Sigmoid()\n",
    "                )\n",
    "\n",
    "\n",
    "    def encoding_fn(self, x):\n",
    "        x = self.encoder(x)\n",
    "        z_mean, z_log_var = self.z_mean(x), self.z_log_var(x)\n",
    "        encoded = self.reparameterize(z_mean, z_log_var)\n",
    "        return encoded\n",
    "\n",
    "\n",
    "    def reparameterize(self, z_mu, z_log_var):\n",
    "        eps = torch.randn(z_mu.size(0), z_mu.size(1)).to(z_mu.get_device())\n",
    "        z = z_mu + eps * torch.exp(z_log_var/2.)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        z_mean, z_log_var = self.z_mean(x), self.z_log_var(x)\n",
    "        encoded = self.reparameterize(z_mean, z_log_var)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, z_mean, z_log_var, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c6b76fe-1718-4ab5-bb11-2bc449bb9ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    (3): Dropout2d(p=0.25, inplace=False)\n",
       "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    (7): Dropout2d(p=0.25, inplace=False)\n",
       "    (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    (11): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (z_mean): Linear(in_features=3136, out_features=2, bias=True)\n",
       "  (z_log_var): Linear(in_features=3136, out_features=2, bias=True)\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=3136, bias=True)\n",
       "    (1): Reshape()\n",
       "    (2): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "    (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    (5): Dropout2d(p=0.25, inplace=False)\n",
       "    (6): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    (9): Dropout2d(p=0.25, inplace=False)\n",
       "    (10): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    (13): Dropout2d(p=0.25, inplace=False)\n",
       "    (14): ConvTranspose2d(32, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (15): Trim()\n",
       "    (16): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VAE()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56679385-7955-4ece-ab19-896040a10ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num. of parametes: 170789\n",
      "Total num. of Trainable parametes: 170789\n"
     ]
    }
   ],
   "source": [
    "# Total num. of parametes\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "# Total num. of \"trainable\" parameters\n",
    "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Total num. of parametes: {num_params}')\n",
    "print(f'Total num. of Trainable parametes: {num_trainable_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f85e1f75-a14c-4af7-a809-387df43bebb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = F.mse_loss # evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f395b5b0-cfc2-4a8d-bf1e-5b2b6f6e8f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_epoch_loss_autoencoder(model, data_loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    curr_loss, num_examples = 0., 0\n",
    "    with torch.no_grad():\n",
    "        for features, _ in data_loader:\n",
    "            features = features.to(device)\n",
    "            _, _, _, logits = model(features)\n",
    "            loss = loss_fn(logits, features, reduction='sum')\n",
    "            num_examples += features.size(0)\n",
    "            curr_loss += loss\n",
    "\n",
    "        curr_loss = curr_loss / num_examples\n",
    "        return curr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "348ef2a7-7f46-4175-b09b-b7006f735daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82011838-5258-4778-83f5-7fd416bca16d",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b67577-723a-4064-bb53-847b7f6469f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict = {'train_combined_loss_per_batch': [],\n",
    "            'train_combined_loss_per_epoch': [],\n",
    "            'train_reconstruction_loss_per_batch': [],\n",
    "            'train_kl_loss_per_batch': []}\n",
    "\n",
    "skip_epoch_stats = False\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "  model.train()\n",
    "  for batch_idx, (features, _) in enumerate(train_loader):\n",
    "\n",
    "    features = features.to(device)\n",
    "\n",
    "    # FORWARD AND BACK PROP\n",
    "    encoded, z_mean, z_log_var, decoded = model(features)\n",
    "\n",
    "    # total loss = reconstruction loss + KL divergence\n",
    "    # kl_divergence = (0.5 * (z_mean**2 +\n",
    "    #                         torch.exp(z_log_var) - z_log_var - 1)).sum()\n",
    "    kl_div = -0.5 * torch.sum(1 + z_log_var\n",
    "                              - z_mean**2\n",
    "                              - torch.exp(z_log_var),\n",
    "                              axis=1) # sum over latent dimension\n",
    "\n",
    "    batchsize = kl_div.size(0)\n",
    "    kl_div = kl_div.mean() # average over batch dimension\n",
    "\n",
    "    pixelwise = loss_fn(decoded, features, reduction='none')\n",
    "    pixelwise = pixelwise.view(batchsize, -1).sum(axis=1) # sum over pixels\n",
    "    pixelwise = pixelwise.mean() # average over batch dimension\n",
    "\n",
    "    loss = pixelwise + kl_div\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "    # UPDATE MODEL PARAMETERS\n",
    "    optimizer.step()\n",
    "\n",
    "    # LOGGING\n",
    "    log_dict['train_combined_loss_per_batch'].append(loss.item())\n",
    "    log_dict['train_reconstruction_loss_per_batch'].append(pixelwise.item())\n",
    "    log_dict['train_kl_loss_per_batch'].append(kl_div.item())\n",
    "\n",
    "    if not batch_idx % logging_interval:\n",
    "        print('Epoch: %03d/%03d | Batch %04d/%04d | Loss: %.4f'\n",
    "              % (epoch+1, NUM_EPOCHS, batch_idx,\n",
    "                  len(train_loader), loss))\n",
    "\n",
    "  if not skip_epoch_stats:\n",
    "      model.eval()\n",
    "\n",
    "      with torch.set_grad_enabled(False):  # save memory during inference\n",
    "          train_loss = compute_epoch_loss_autoencoder(\n",
    "              model, train_loader, loss_fn, device)\n",
    "          print('***Epoch: %03d/%03d | Loss: %.3f' % (\n",
    "                epoch+1, NUM_EPOCHS, train_loss))\n",
    "          log_dict['train_combined_loss_per_epoch'].append(train_loss.item())\n",
    "\n",
    "  print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n",
    "if save_model is not None:\n",
    "    torch.save({\n",
    "          'model_state_dict': model.state_dict(),\n",
    "          'optimizer_state_dict': optimizer_D.state_dict()\n",
    "          }, 'vae_mnist.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd7193c-084a-4557-86df-6f2c988f1a6d",
   "metadata": {},
   "source": [
    "## Plot loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5741146-695c-4f34-865f-00caeefa30d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_loss(minibatch_losses, num_epochs, averaging_iterations=100, custom_label=''):\n",
    "\n",
    "    iter_per_epoch = len(minibatch_losses) // num_epochs\n",
    "\n",
    "    plt.figure()\n",
    "    ax1 = plt.subplot(1, 1, 1)\n",
    "    ax1.plot(range(len(minibatch_losses)),\n",
    "             (minibatch_losses), label=f'Minibatch Loss{custom_label}')\n",
    "    ax1.set_xlabel('Iterations')\n",
    "    ax1.set_ylabel('Loss')\n",
    "\n",
    "    if len(minibatch_losses) < 1000:\n",
    "        num_losses = len(minibatch_losses) // 2\n",
    "    else:\n",
    "        num_losses = 1000\n",
    "\n",
    "    ax1.set_ylim([\n",
    "        0, np.max(minibatch_losses[num_losses:])*1.5\n",
    "        ])\n",
    "\n",
    "    ax1.plot(np.convolve(minibatch_losses,\n",
    "                         np.ones(averaging_iterations,)/averaging_iterations,\n",
    "                         mode='valid'),\n",
    "             label=f'Running Average{custom_label}')\n",
    "    ax1.legend()\n",
    "\n",
    "    ###################\n",
    "    # Set scond x-axis\n",
    "    ax2 = ax1.twiny()\n",
    "    newlabel = list(range(num_epochs+1))\n",
    "\n",
    "    newpos = [e*iter_per_epoch for e in newlabel]\n",
    "\n",
    "    ax2.set_xticks(newpos[::10])\n",
    "    ax2.set_xticklabels(newlabel[::10])\n",
    "\n",
    "    ax2.xaxis.set_ticks_position('bottom')\n",
    "    ax2.xaxis.set_label_position('bottom')\n",
    "    ax2.spines['bottom'].set_position(('outward', 45))\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_xlim(ax1.get_xlim())\n",
    "    ###################\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d874cfc-b7e3-4bfc-82aa-6e9cf54952e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_loss(log_dict['train_reconstruction_loss_per_batch'], NUM_EPOCHS, custom_label=\" (reconstruction)\")\n",
    "plot_training_loss(log_dict['train_kl_loss_per_batch'], NUM_EPOCHS, custom_label=\" (KL)\")\n",
    "plot_training_loss(log_dict['train_combined_loss_per_batch'], NUM_EPOCHS, custom_label=\" (combined)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e899d132-f9d7-449f-bd66-402c2f95ec38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_generated_images(data_loader, model, device,\n",
    "                          unnormalizer=None,\n",
    "                          figsize=(20, 2.5), n_images=15):\n",
    "\n",
    "  fig, axes = plt.subplots(nrows=2, ncols=n_images,\n",
    "                            sharex=True, sharey=True, figsize=figsize)\n",
    "\n",
    "  for batch_idx, (features, _) in enumerate(data_loader):\n",
    "\n",
    "    features = features.to(device)\n",
    "\n",
    "    color_channels = features.shape[1]\n",
    "    image_height = features.shape[2]\n",
    "    image_width = features.shape[3]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoded, z_mean, z_log_var, decoded_images = model(features)[:n_images]\n",
    "\n",
    "    orig_images = features[:n_images]\n",
    "    break\n",
    "\n",
    "  for i in range(n_images):\n",
    "    for ax, img in zip(axes, [orig_images, decoded_images]):\n",
    "      curr_img = img[i].detach().to(torch.device('cpu'))\n",
    "      if unnormalizer is not None:\n",
    "        curr_img = unnormalizer(curr_img)\n",
    "\n",
    "      if color_channels > 1:\n",
    "        curr_img = np.transpose(curr_img, (1, 2, 0))\n",
    "        ax[i].imshow(curr_img)\n",
    "      else:\n",
    "        ax[i].imshow(curr_img.view((image_height, image_width)), cmap='binary')\n",
    "\n",
    "\n",
    "def plot_latent_space_with_labels(num_classes, data_loader, encoding_fn, device):\n",
    "  d = {i:[] for i in range(num_classes)}\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for i, (features, targets) in enumerate(data_loader):\n",
    "      features = features.to(device)\n",
    "      targets = targets.to(device)\n",
    "\n",
    "      embedding = encoding_fn(features)\n",
    "\n",
    "      for i in range(num_classes):\n",
    "        if i in targets:\n",
    "          mask = targets == i\n",
    "          d[i].append(embedding[mask].to('cpu').numpy())\n",
    "\n",
    "  colors = list(mcolors.TABLEAU_COLORS.items())\n",
    "  for i in range(num_classes):\n",
    "    d[i] = np.concatenate(d[i])\n",
    "    plt.scatter(\n",
    "        d[i][:, 0], d[i][:, 1],\n",
    "        color=colors[i][1],\n",
    "        label=f'{i}',\n",
    "        alpha=0.5)\n",
    "\n",
    "  plt.legend()\n",
    "\n",
    "\n",
    "def plot_images_sampled_from_vae(model, device, latent_size, unnormalizer=None, num_images=10):\n",
    "\n",
    "  with torch.no_grad():\n",
    "    ##########################\n",
    "    ### RANDOM SAMPLE\n",
    "    ##########################\n",
    "\n",
    "    rand_features = torch.randn(num_images, latent_size).to(device)\n",
    "    new_images = model.decoder(rand_features)\n",
    "    color_channels = new_images.shape[1]\n",
    "    image_height = new_images.shape[2]\n",
    "    image_width = new_images.shape[3]\n",
    "\n",
    "    ##########################\n",
    "    ### VISUALIZATION\n",
    "    ##########################\n",
    "\n",
    "    image_width = 28\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=num_images, figsize=(10, 2.5), sharey=True)\n",
    "    decoded_images = new_images[:num_images]\n",
    "\n",
    "    for ax, img in zip(axes, decoded_images):\n",
    "      curr_img = img.detach().to(torch.device('cpu'))\n",
    "      if unnormalizer is not None:\n",
    "        curr_img = unnormalizer(curr_img)\n",
    "\n",
    "      if color_channels > 1:\n",
    "        curr_img = np.transpose(curr_img, (1, 2, 0))\n",
    "        ax.imshow(curr_img)\n",
    "      else:\n",
    "        ax.imshow(curr_img.view((image_height, image_width)), cmap='binary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cd3b5f-42c9-4424-944e-930c890c0324",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_generated_images(data_loader=train_loader, model=model, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf4c446-9e21-479c-abd3-33df54ea1203",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latent_space_with_labels(\n",
    "    num_classes=10,\n",
    "    data_loader=train_loader,\n",
    "    encoding_fn=model.encoding_fn,\n",
    "    device=device)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e1aa24-fde8-48fe-b4a0-8e5cf90042d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    plot_images_sampled_from_vae(model=model, device=device, latent_size=2)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
