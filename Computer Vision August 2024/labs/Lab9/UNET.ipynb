{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5639a1a3-2485-4192-adc2-8c5070a049dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For our puffer surver we need to browse via a proxy!!\n",
    "import os\n",
    "# Set HTTP and HTTPS proxy\n",
    "os.environ['http_proxy'] = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c2df8d0-ce9c-40e4-a1e8-0a5089eb951d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch_snippets in /opt/conda/lib/python3.9/site-packages (0.545)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.9/site-packages (from torch_snippets) (0.3.4)\n",
      "Requirement already satisfied: imgaug>=0.4.0 in /opt/conda/lib/python3.9/site-packages (from torch_snippets) (0.4.0)\n",
      "Requirement already satisfied: typer in /opt/conda/lib/python3.9/site-packages (from torch_snippets) (0.12.5)\n",
      "Requirement already satisfied: fastcore in /opt/conda/lib/python3.9/site-packages (from torch_snippets) (1.7.19)\n",
      "Requirement already satisfied: jsonlines in /opt/conda/lib/python3.9/site-packages (from torch_snippets) (4.0.0)\n",
      "Requirement already satisfied: mergedeep in /opt/conda/lib/python3.9/site-packages (from torch_snippets) (1.3.4)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.9/site-packages (from torch_snippets) (3.5.1)\n",
      "Requirement already satisfied: python-Levenshtein in /opt/conda/lib/python3.9/site-packages (from torch_snippets) (0.26.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from torch_snippets) (1.3.5)\n",
      "Requirement already satisfied: xmltodict in /opt/conda/lib/python3.9/site-packages (from torch_snippets) (0.14.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from torch_snippets) (4.62.3)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.9/site-packages (from torch_snippets) (8.4.0)\n",
      "Requirement already satisfied: confection in /opt/conda/lib/python3.9/site-packages (from torch_snippets) (0.1.5)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.9/site-packages (from torch_snippets) (13.9.3)\n",
      "Requirement already satisfied: wasabi in /opt/conda/lib/python3.9/site-packages (from torch_snippets) (1.1.3)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.9/site-packages (from torch_snippets) (6.0)\n",
      "Requirement already satisfied: deepdiff in /opt/conda/lib/python3.9/site-packages (from torch_snippets) (8.0.1)\n",
      "Requirement already satisfied: fuzzywuzzy in /opt/conda/lib/python3.9/site-packages (from torch_snippets) (0.18.0)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.9/site-packages (from torch_snippets) (3.9.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from torch_snippets) (1.21.5)\n",
      "Requirement already satisfied: pre-commit in /opt/conda/lib/python3.9/site-packages (from torch_snippets) (4.0.1)\n",
      "Requirement already satisfied: typing in /opt/conda/lib/python3.9/site-packages (from torch_snippets) (3.7.4.3)\n",
      "Requirement already satisfied: icecream in /opt/conda/lib/python3.9/site-packages (from torch_snippets) (2.1.3)\n",
      "Requirement already satisfied: srsly in /opt/conda/lib/python3.9/site-packages (from torch_snippets) (2.4.8)\n",
      "Requirement already satisfied: pydantic in /opt/conda/lib/python3.9/site-packages (from torch_snippets) (2.9.2)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch_snippets) (4.12.2)\n",
      "Requirement already satisfied: loguru in /opt/conda/lib/python3.9/site-packages (from torch_snippets) (0.7.2)\n",
      "Requirement already satisfied: catalogue in /opt/conda/lib/python3.9/site-packages (from torch_snippets) (2.0.10)\n",
      "Requirement already satisfied: Shapely in /opt/conda/lib/python3.9/site-packages (from imgaug>=0.4.0->torch_snippets) (2.0.6)\n",
      "Requirement already satisfied: scikit-image>=0.14.2 in /opt/conda/lib/python3.9/site-packages (from imgaug>=0.4.0->torch_snippets) (0.19.1)\n",
      "Requirement already satisfied: opencv-python in /opt/conda/lib/python3.9/site-packages (from imgaug>=0.4.0->torch_snippets) (4.10.0.84)\n",
      "Requirement already satisfied: imageio in /opt/conda/lib/python3.9/site-packages (from imgaug>=0.4.0->torch_snippets) (2.13.5)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from imgaug>=0.4.0->torch_snippets) (1.7.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from imgaug>=0.4.0->torch_snippets) (1.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.9/site-packages (from pydantic->torch_snippets) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.9/site-packages (from pydantic->torch_snippets) (2.23.4)\n",
      "Requirement already satisfied: orderly-set==5.2.2 in /opt/conda/lib/python3.9/site-packages (from deepdiff->torch_snippets) (5.2.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from fastcore->torch_snippets) (24.1)\n",
      "Requirement already satisfied: pygments>=2.2.0 in /opt/conda/lib/python3.9/site-packages (from icecream->torch_snippets) (2.18.0)\n",
      "Requirement already satisfied: executing>=0.3.1 in /opt/conda/lib/python3.9/site-packages (from icecream->torch_snippets) (0.8.2)\n",
      "Requirement already satisfied: asttokens>=2.0.1 in /opt/conda/lib/python3.9/site-packages (from icecream->torch_snippets) (2.0.5)\n",
      "Requirement already satisfied: colorama>=0.3.9 in /opt/conda/lib/python3.9/site-packages (from icecream->torch_snippets) (0.4.4)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.9/site-packages (from jsonlines->torch_snippets) (21.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->torch_snippets) (3.0.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib->torch_snippets) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->torch_snippets) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.9/site-packages (from matplotlib->torch_snippets) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib->torch_snippets) (4.28.5)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk->torch_snippets) (2024.9.11)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->torch_snippets) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->torch_snippets) (1.1.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas->torch_snippets) (2021.3)\n",
      "Requirement already satisfied: virtualenv>=20.10.0 in /opt/conda/lib/python3.9/site-packages (from pre-commit->torch_snippets) (20.27.1)\n",
      "Requirement already satisfied: nodeenv>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from pre-commit->torch_snippets) (1.9.1)\n",
      "Requirement already satisfied: identify>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from pre-commit->torch_snippets) (2.6.1)\n",
      "Requirement already satisfied: cfgv>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from pre-commit->torch_snippets) (3.4.0)\n",
      "Requirement already satisfied: Levenshtein==0.26.1 in /opt/conda/lib/python3.9/site-packages (from python-Levenshtein->torch_snippets) (0.26.1)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /opt/conda/lib/python3.9/site-packages (from Levenshtein==0.26.1->python-Levenshtein->torch_snippets) (3.10.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.9/site-packages (from rich->torch_snippets) (3.0.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.9/site-packages (from typer->torch_snippets) (1.5.4)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich->torch_snippets) (0.1.2)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->torch_snippets) (1.2.0)\n",
      "Requirement already satisfied: networkx>=2.2 in /opt/conda/lib/python3.9/site-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->torch_snippets) (2.6.3)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.9/site-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->torch_snippets) (2021.11.2)\n",
      "Requirement already satisfied: distlib<1,>=0.3.7 in /opt/conda/lib/python3.9/site-packages (from virtualenv>=20.10.0->pre-commit->torch_snippets) (0.3.9)\n",
      "Requirement already satisfied: filelock<4,>=3.12.2 in /opt/conda/lib/python3.9/site-packages (from virtualenv>=20.10.0->pre-commit->torch_snippets) (3.15.4)\n",
      "Requirement already satisfied: platformdirs<5,>=3.9.1 in /opt/conda/lib/python3.9/site-packages (from virtualenv>=20.10.0->pre-commit->torch_snippets) (4.3.6)\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('dataset1'):\n",
    "    !wget -q https://www.dropbox.com/s/0pigmmmynbf9xwq/dataset1.zip\n",
    "    !unzip -q dataset1.zip\n",
    "    !rm dataset1.zip\n",
    "    !pip install -q pytorch_model_summary\n",
    "import torch\n",
    "!pip install torch_snippets\n",
    "from torch_snippets import *\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.models import vgg16_bn\n",
    "import cv2 as cv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58e27108-0625-4f4d-b6ee-edf295f532dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torch/cuda/__init__.py:118: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11060). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "# Create Params dictionary\n",
    "class Params(object):\n",
    "    def __init__(self, batch_size, test_batch_size, epochs, lr, seed, cuda, log_interval):\n",
    "        self.batch_size = batch_size\n",
    "        self.test_batch_size = test_batch_size\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.seed = seed\n",
    "        self.cuda = 'cuda' if cuda and torch.cuda.is_available() else 'cpu'\n",
    "        self.log_interval = log_interval\n",
    "\n",
    "# Configure args\n",
    "args = Params(8, 2, 5, 1e-3, 1, True, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b457563-8ac7-4a00-8677-51c521e0492f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms():\n",
    "  return transforms.Compose([\n",
    "                             transforms.ToTensor(),\n",
    "                             transforms.Normalize(\n",
    "                                 [0.485, 0.456, 0.406],\n",
    "                                 [0.229, 0.224, 0.225]\n",
    "                                 )\n",
    "                             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24ca1d00-ebdb-47a1-8e36-f16a674f379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SegmentationData(Dataset):\n",
    "    def __init__(self, split):\n",
    "        self.items = stems(f'dataset1/images_prepped_{split}')\n",
    "        self.split = split                                     # Store the split information\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve an image and its corresponding mask based on the index 'idx'\n",
    "        image = read(f'dataset1/images_prepped_{self.split}/{self.items[idx]}.png', 1)\n",
    "        image = cv.resize(image, (224,224))\n",
    "\n",
    "        mask = read(f'dataset1/annotations_prepped_{self.split}/{self.items[idx]}.png', 0)\n",
    "        mask = cv.resize(mask, (224,224))\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    def choose(self): return self[randint(len(self))]  # Randomly select and return one image and mask pair from the dataset\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        # Custom collate function to combine a batch of images and masks\n",
    "        # Unzip the batch into images and masks\n",
    "        ims, masks = list(zip(*batch))\n",
    "        \n",
    "        # Transform the images: Normalize and convert to tensor, then stack them into a single tensor\n",
    "\n",
    "        ims = torch.cat([get_transforms()(im.copy()/255.)[None] for im in ims]).float().to(args.cuda)\n",
    "\n",
    "        # Convert masks to tensors, stack them into a single tensor and cast to long type\n",
    "\n",
    "        ce_masks = torch.cat([torch.Tensor(mask[None]) for mask in masks]).long().to(args.cuda)\n",
    "\n",
    "        return ims, ce_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79cd5b27-acf7-4778-afec-3ed20c073bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders():\n",
    "    trn_ds = SegmentationData('train')\n",
    "    val_ds = SegmentationData('test')\n",
    "\n",
    "    trn_dl = DataLoader(trn_ds, batch_size=args.batch_size, shuffle=True, collate_fn=trn_ds.collate_fn)\n",
    "    val_dl = DataLoader(val_ds, batch_size=args.test_batch_size, shuffle=True, collate_fn=val_ds.collate_fn)\n",
    "\n",
    "    return trn_dl, val_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d63afa8-2756-4ffe-b755-ffde95b74d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dl, val_dl = get_dataloaders()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d43cbb-432d-4ddc-b889-544fa78fb7bf",
   "metadata": {},
   "source": [
    "# U Net Architecture\n",
    "\n",
    "- U-Net is a convolutional neural network architecture primarily used for image segmentation tasks. It consists of a contracting path (encoder) that captures context and a symmetric expanding path (decoder) that enables precise localization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91ebf39b-ea57-4e4c-a32b-4e715ede5fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bde56da-5e40-4dac-bece-50357e32dccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def up_conv(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73cb9c0b-cb02-40e7-a16c-fa9e75ae1965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, pretrained=True, out_channels=12):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = vgg16_bn(pretrained=pretrained).features\n",
    "        self.block1 = nn.Sequential(*self.encoder[:6])\n",
    "        self.block2 = nn.Sequential(*self.encoder[6:13])\n",
    "        self.block3 = nn.Sequential(*self.encoder[13:20])\n",
    "        self.block4 = nn.Sequential(*self.encoder[20:27])\n",
    "        self.block5 = nn.Sequential(*self.encoder[27:34])\n",
    "\n",
    "        self.bottleneck = nn.Sequential(*self.encoder[34:])\n",
    "        self.conv_bottleneck = conv(512, 1024)\n",
    "\n",
    "        self.up_conv6 = up_conv(1024, 512)\n",
    "        self.conv6 = conv(512 + 512, 512)\n",
    "        self.up_conv7 = up_conv(512, 256)\n",
    "        self.conv7 = conv(256 + 512, 256)\n",
    "        self.up_conv8 = up_conv(256, 128)\n",
    "        self.conv8 = conv(128 + 256, 128)\n",
    "        self.up_conv9 = up_conv(128, 64)\n",
    "        self.conv9 = conv(64 + 128, 64)\n",
    "        self.up_conv10 = up_conv(64, 32)\n",
    "        self.conv10 = conv(32 + 64, 32)\n",
    "        self.conv11 = nn.Conv2d(32, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Contractive Path\n",
    "        block1 = self.block1(x)\n",
    "        block2 = self.block2(block1)\n",
    "        block3 = self.block3(block2)\n",
    "        block4 = self.block4(block3)\n",
    "        block5 = self.block5(block4)\n",
    "\n",
    "        bottleneck = self.bottleneck(block5)\n",
    "        x = self.conv_bottleneck(bottleneck)\n",
    "        # Expansive Path\n",
    "        x = self.up_conv6(x)\n",
    "        x = torch.cat([x, block5], dim=1)\n",
    "        x = self.conv6(x)\n",
    "\n",
    "        x = self.up_conv7(x)\n",
    "        x = torch.cat([x, block4], dim=1)\n",
    "        x = self.conv7(x)\n",
    "\n",
    "        x = self.up_conv8(x)\n",
    "        x = torch.cat([x, block3], dim=1)\n",
    "        x = self.conv8(x)\n",
    "\n",
    "        x = self.up_conv9(x)\n",
    "        x = torch.cat([x, block2], dim=1)\n",
    "        x = self.conv9(x)\n",
    "\n",
    "        x = self.up_conv10(x)\n",
    "        x = torch.cat([x, block1], dim=1)\n",
    "        x = self.conv10(x)\n",
    "\n",
    "        x = self.conv11(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cd3854-084f-40ab-8bb2-94e5e4cdaf3a",
   "metadata": {},
   "source": [
    "## Steps\n",
    "\n",
    "### Initializes the U-Net model.\n",
    "   \n",
    "Takes two parameters:\n",
    "\n",
    "    - pretrained: A boolean indicating whether to use a pretrained VGG16 model.\n",
    "    - out_channels: The number of output channels for the final segmentation map (e.g., 12 classes for segmentation).\n",
    "    \n",
    "### Encoder (Contractive Path)\n",
    "\n",
    "    - The encoder part uses the features from a VGG16 model with batch normalization (vgg16_bn).\n",
    "    - The encoder is divided into five blocks, each consisting of several convolutional layers that progressively reduce the spatial dimensions while increasing the number of feature channels.\n",
    "    \n",
    "## Bottleneck\n",
    "\n",
    "    - The bottleneck section takes the deepest layers of the encoder.\n",
    "    - A convolution layer `conv_bottleneck` is applied to increase the number of feature channels from 512 to 1024, allowing the network to learn more complex features.\n",
    "    \n",
    "\n",
    "## Decoder (Expansive Path)\n",
    "\n",
    "The decoder consists of up-convolution (or transposed convolution) layers followed by concatenation with corresponding encoder features to retain spatial information:\n",
    "\n",
    "    - Each up_conv layer increases the spatial dimensions (upsampling).\n",
    "    - The output of each up-convolution is concatenated with the corresponding feature map from the encoder (skip connections).\n",
    "    - This helps the model learn both high-level features from deeper layers and low-level features from shallower layers.\n",
    "    - Finally, conv11 reduces the number of channels to out_channels (e.g., for multi-class segmentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71c184e7-6c3b-4236-893e-3e134d8b0fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torchvision.models.vgg.vgg16_bn(*, weights: Optional[torchvision.models.vgg.VGG16_BN_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.vgg.VGG>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16_bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fef1a14-c584-4f59-ae6a-0feb71c80b83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ce = nn.CrossEntropyLoss()   # Applies softmax to output logits --> converts into class probabilities --> calculates neg. log likelihood loss between pred and true class label!!\n",
    "\n",
    "def UnetLoss(preds, targets):\n",
    "    ce_loss = ce(preds, targets)\n",
    "    acc = (torch.max(preds, 1)[1] == targets).float().mean()\n",
    "    #  (torch.max(preds, 1)[1] returns the indices of the maximum values along the class dimension (i.e., the predicted class for each pixel). \n",
    "    #  The 1 indicates that we're looking along the columns (the class dimension).\n",
    "    #  if preds class == targets return 1 --> change to float --> take mean to keep score between 0 and 1\n",
    "    return ce_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9aea8bb-8d15-4a8a-a3b7-f136bb65c86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainEngine():\n",
    "    def train_batch(model, data, optimizer, criterion):\n",
    "        model.train()\n",
    "\n",
    "        ims, ce_masks = data\n",
    "        _masks = model(ims)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss, acc = criterion(_masks, ce_masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        return loss.item(), acc.item()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validate_batch(model, data, criterion):\n",
    "        model.eval()\n",
    "\n",
    "        ims, masks = data\n",
    "        _masks = model(ims)\n",
    "\n",
    "        loss, acc = criterion(_masks, masks)\n",
    "\n",
    "        return loss.item(), acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "845967f7-fa7f-461c-abed-1ed748f6c4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "def make_model():\n",
    "    model = UNet().to(args.cuda)\n",
    "    criterion = UnetLoss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    return model, criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f025fdad-495f-4d49-942f-c434ae165bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_BN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num. of parametes: 29311308\n",
      "Total num. of Trainable parametes: 29311308\n"
     ]
    }
   ],
   "source": [
    "model, criterion, optimizer = make_model()\n",
    "# Total num. of parametes\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "# Total num. of \"trainable\" parameters\n",
    "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Total num. of parametes: {num_params}')\n",
    "print(f'Total num. of Trainable parametes: {num_trainable_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68b4372a-41e0-44f1-a516-f218766e47d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model():\n",
    "    for epoch in range(args.epochs):\n",
    "        print(\"####################\")\n",
    "        print(f\"       Epoch: {epoch}   \")\n",
    "        print(\"####################\")\n",
    "\n",
    "        for batch_idx, data in tqdm(enumerate(trn_dl), total=len(trn_dl), leave=False):\n",
    "            train_loss, train_acc = TrainEngine.train_batch(model, data, optimizer, criterion)\n",
    "            if batch_idx % args.log_interval == 0:\n",
    "                # Print training information inline instead of calling a function\n",
    "                step = epoch * len(trn_dl) + batch_idx\n",
    "                print(f'Epoch [{epoch+1}/{args.epochs}], Step [{batch_idx}/{len(trn_dl)}], '\n",
    "                      f'Train Loss: {train_loss:.6f}, Accuracy: {train_acc:.6f}')\n",
    "\n",
    "        avg_val_acc = avg_val_loss = 0.0\n",
    "        for batch_idx, data in tqdm(enumerate(val_dl), total=len(val_dl)):\n",
    "            val_loss, val_acc = TrainEngine.validate_batch(model, data, criterion)\n",
    "\n",
    "            avg_val_loss += val_loss\n",
    "            avg_val_acc += val_acc\n",
    "\n",
    "        step = (epoch + 1) * len(trn_dl)\n",
    "        avg_val_loss /= len(val_dl)\n",
    "        avg_val_acc /= len(val_dl)\n",
    "        print(f'Val: Average loss: {avg_val_loss:.4f}, Accuracy: {avg_val_acc:.4f}')\n",
    "        print()\n",
    "\n",
    "    # Save the model and optimizer states after training is complete\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, 'unet.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0af5080-3d59-496c-b4f7-d6a100c5f0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "       Epoch: 0   \n",
      "####################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/46 [00:09<06:47,  9.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [0/46], Train Loss: 2.339851, Accuracy: 0.232003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 11/46 [01:32<04:51,  8.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [10/46], Train Loss: 1.637340, Accuracy: 0.680151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 21/46 [02:53<03:22,  8.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [20/46], Train Loss: 1.454079, Accuracy: 0.719433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 31/46 [04:15<02:01,  8.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [30/46], Train Loss: 1.238068, Accuracy: 0.769078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 41/46 [05:36<00:41,  8.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [40/46], Train Loss: 1.158401, Accuracy: 0.747935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:53<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val: Average loss: 1.1736, Accuracy: 0.7735\n",
      "\n",
      "####################\n",
      "       Epoch: 1   \n",
      "####################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/46 [00:08<06:00,  8.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [0/46], Train Loss: 0.993044, Accuracy: 0.824734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 11/46 [01:29<04:50,  8.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [10/46], Train Loss: 0.983688, Accuracy: 0.782227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 21/46 [02:49<03:23,  8.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [20/46], Train Loss: 0.811047, Accuracy: 0.844283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 31/46 [04:11<02:04,  8.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [30/46], Train Loss: 0.749451, Accuracy: 0.848154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 41/46 [05:31<00:40,  8.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [40/46], Train Loss: 0.802150, Accuracy: 0.810504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:53<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val: Average loss: 0.7339, Accuracy: 0.8509\n",
      "\n",
      "####################\n",
      "       Epoch: 2   \n",
      "####################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/46 [00:08<06:06,  8.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [0/46], Train Loss: 0.664351, Accuracy: 0.866664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 11/46 [01:27<04:40,  8.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [10/46], Train Loss: 0.643855, Accuracy: 0.852806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 21/46 [02:49<03:31,  8.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [20/46], Train Loss: 0.643532, Accuracy: 0.859283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 31/46 [04:17<02:01,  8.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [30/46], Train Loss: 0.535793, Accuracy: 0.882314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 41/46 [05:41<00:40,  8.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [40/46], Train Loss: 0.489861, Accuracy: 0.896796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:47<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val: Average loss: 0.7112, Accuracy: 0.8244\n",
      "\n",
      "####################\n",
      "       Epoch: 3   \n",
      "####################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/46 [00:08<06:06,  8.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [0/46], Train Loss: 0.587825, Accuracy: 0.854114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 11/46 [01:34<04:49,  8.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [10/46], Train Loss: 0.627285, Accuracy: 0.842118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 21/46 [03:08<04:11, 10.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [20/46], Train Loss: 0.637420, Accuracy: 0.846715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 31/46 [04:35<02:05,  8.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [30/46], Train Loss: 0.532278, Accuracy: 0.867467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 41/46 [05:54<00:39,  8.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [40/46], Train Loss: 0.618013, Accuracy: 0.836618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:52<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val: Average loss: 0.6775, Accuracy: 0.8267\n",
      "\n",
      "####################\n",
      "       Epoch: 4   \n",
      "####################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/46 [00:08<06:10,  8.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [0/46], Train Loss: 0.463302, Accuracy: 0.889277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 11/46 [01:30<04:47,  8.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [10/46], Train Loss: 0.618043, Accuracy: 0.836665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 21/46 [02:56<03:36,  8.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [20/46], Train Loss: 0.488171, Accuracy: 0.868954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 31/46 [04:15<01:59,  7.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [30/46], Train Loss: 0.461211, Accuracy: 0.882486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 41/46 [05:49<00:49,  9.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [40/46], Train Loss: 0.449941, Accuracy: 0.883234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [01:10<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val: Average loss: 0.5263, Accuracy: 0.8690\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "run_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea9b434-3bc7-401a-a175-985c06d91ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize and save results\n",
    "for bx, data in tqdm(enumerate(val_dl), total=len(val_dl)):\n",
    "    im, mask = data\n",
    "    _mask = model(im)\n",
    "    _, _mask = torch.max(_mask, dim=1)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(im[0].permute(1, 2, 0).detach().cpu()[:, :, 0])\n",
    "    plt.savefig(f'original_image_{bx}.jpg')\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(mask.permute(1, 2, 0).detach().cpu()[:, :, 0])\n",
    "    plt.savefig(f'original_mask_{bx}.jpg')\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(_mask.permute(1, 2, 0).detach().cpu()[:, :, 0])\n",
    "    plt.savefig(f'predicted_mask_{bx}.jpg')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80f39aa-f1bd-40ff-a1d6-9f79bef29850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5929d9-1a1d-453a-af0d-a157e64ac674",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
